{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3IRYr3lLUHSW0C1g48GnR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdon021/Comp261_A5/blob/master/Class_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-bkKamI1UeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from mpl_toolkits import mplot3d\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tro7iEX-ew8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = [0, 0, 0]\n",
        "cov = [[1, 0, 0],\n",
        "       [0, 1, 0],\n",
        "       [0, 0, 1]]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b135hxdIfNxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unit_sphere(data):\n",
        "  return np.array([[coord / math.sqrt(sum(coords * coords for coords in line)) for coord in line] for line in data])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTrKt37dfPnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "united = np.random.multivariate_normal(mean, cov, 4000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObgwOT9zfTjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x11, x22, x33 = united.T\n",
        "united_y = unit_sphere(united)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEqaNvahfVfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "46fe8530-8400-4862-c533-5d92232e9f7b"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(x11, x22, x33, 'o')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7fdaefa3f208>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PmRxaopfXGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(united, united_y, test_size = 0.25)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4uQtuxLfcLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.25)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stLTEF21nI8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5915ee93-636e-4e63-dd81-5882768899ae"
      },
      "source": [
        "x11, x22, x33 = Y_test.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(x11, x22, x33, 'o')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7fdaa67c24a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eXgchZUtfqq6etO+L5blRYvlRd53hiHYJhAwGJgAdt4Mw2T5/Zi8JMPAvIAIJjF8EMzLBEIICZmBAFkNJAHbMRiD2Qaw8SZ5t6x9bW0tqfetlvdHu8rV3VXVVa2SLJk636fPVndt3ao6devec88lOI6DAQMGDBiYGJCX+gAMGDBg4IsEg3QNGDBgYAJhkK4BAwYMTCAM0jVgwICBCYRBugYMGDAwgaCSvG9IGwwYMGBAOwi5N4xI14ABAwYmEAbpGjBgwMAEwiBdAwYMGJhAGKRrwIABAxMIg3QNGDBgYAJhkK4BAwYMTCAM0jVgwICBCYRBugYMGDAwgTBI14ABAwYmEAbpGjBgwMAEwiBdAwYMGJhAGKRrwIABAxMIg3QNGDBgYAKRzGXMgAFZcBwHlmURCoVA0zQoigJJkjCZTCBJEiRJgiBkzZYMGPhCgkgymNKwdjSQAI7jwDAMaJqO+T//nphoeRLmfwwyNvAFgewJbpCuAdWIJ1uCIEAQBGiaBk3TIEkyYXn+h6ZpnD9/HvPnzwdgkLGByx6yJ7KRXjCQFDxp9vb2wm63IysrK4FgpcCTMo9wOAyTyQT+Rk/TNCKRSMw6BhkbuNxhkK4BWfBky6cOPB7PmAiQJ1t+/fjtxJOxOFVBEIRAwmJSNsjYwFSDQboGEsCybEyelo9YSZIEy7IJy6shPi3LyJExwzA4cuQIamtrQVEUOI5TjIwNQjYwGWGQrgEBLMuCpmkwDAMgMT1AEASS1ABkMdZ1xf/yBCsmY/4GIV6HJElQFGWQsYFJBYN0v+DgC12RSESIYuWIiSTJlIlTL8TfBOJfAy5GxrycLX59/nOmp6cbZGxgwmGQ7hcUvMaWpmmcOXMGc+fOTUo8YsLSiokkNDkyBqKfe3h4GB6PB7NmzYp5T5ym4KNjg4wN6A2DdL9gEJMty7IgCAKjo6Oq1QiXOtIdK3gS5QtzPPjvhWGYBK2xVM7YUFQYSBUG6X5BIKex1UIccoW0qYj4zy33XSiRsc/nQ2ZmJiwWiyFvM6AaBule5pAiWzVRrRTGkl7QExMZbSuRcXt7OyoqKsAwjNGFZ0A1DNK9TMFrbMURWqpky2MypBf0IC49PgN/HBRFgaIuXkZG44eBZDBI9zIDT7bd3d2wWq3Iy8tTRbbxj85SkFMvTEXS0Iu8pdIUUttXIuPh4WEUFRUJhkE8OU/F79VAchike5kgvqGBl0qpuXD5XK24sCQFufQCwzAIh8OwWq1fKKJQc6PioUTGbW1tyM/Pj0kBGY0fly8M0p3ikGto0FL0Uqu/jU8v0DSNrq4u9PT0wGw2IxKJgKIopKenx/xYLJbUPtw4Qa8UiRbSlYP47xW/bcBo/LgcYZDuFISahgYtpKu2QMaTbiQSQUdHB/r6+jB9+nSsXr0aDMOAJElEIhH4fD74fD4MDg6ivb1dIONgMIju7u4xk7GeOdmxQE/yjkcqjR8DAwMoLS2FyWSKSVUYZDy5YJDuFEK8xhZQ7h6Lzx3KQS1BMwwDp9OJwcFBlJeX44orrhDW5SNts9mMnJwc5OTkxKwbiURw5MgREASBwcFBtLW1CcbnGRkZMZGx2WyWPYbJRJbAxOezlRo/urq6UFxcLKR7xDAaPyYPDNKdAuDJ1u12Y2RkBNOmTUt6wWhNLygtGwwG0d7ejoGBAdjtdixfvlyzEsJsNoOiKJSVlcW8Hg6Hhci4v78fPp8PNE3DbDYnpCmUyFgrxquQdqmOhYfU30VKa8z/azR+TDwM0p3EiNfYRiIRDA8PJxCXFLSmF6Siv0AggLa2NoyOjmLWrFkoKipCX1/fmKVnYlgsFlgsFuTm5sa8LkfGwWAQra2tyMrKEshYLNmaSOhFuuMNrY0fAwMDKCwsFJo+DHmbvjBIdxJCrqHBZDLpFr0qLev3+9Ha2gqPx4OKigrMmzcPBEHA5XLJblPvi1GKjDmOQ319PfLy8hAKheBwOODz+cAwDCwWS0JkLEfGk6mQdimPRY6Me3t7UVBQgEgkgnA4bDR+6AyDdCcRkjU06JkykFrW6/WitbUVgUAAFRUVWLBggW7WjnqAfxzOzs6G1WoVXuc4LiYyliPjjIwMpKWlCdvS65jGApZldXly0DPqZlkWFEVp0hobZKweBulOAsRPaJDrHhsv0qVpGo2NjSAIAhUVFcjLy5O8WJRI91I+ahMEAavVKjSDiI9JTMY9PT3w+/0IBoMwmUwIhUIxkXEynfJ4QK/vTS/y5iH395d6T46MvV4vCIJAdna2pLztiwqDdC8hWJZFMBiE0+lEQUFB0uKYyWQSVALJoIZ0R0dH0draCrfbjdmzZ2PmzJkpb3MyXkRyZNzT04NQKITs7GyBjH0+H1iWhdVqTUhTjCcZT1bS1QI5Mna73TCZTMjIyJBt/BBL274oigqDdC8BxA0NgUAAPT09KCoqSrqelpyu0rIjIyNoaWkBSZKorKxEX18f0tPTk25TLtKdahcJQRCwWCzIz89Hfn6+8DrHcQiFQkJk3N3dDb/fD5ZlYbPZYoiYT1OMFXqRJa+TnkxgGAZWq9Vo/IiDQboTBLmGBrPZrDp61eLyFR+VchwHp9OJ1tZWWCwW1NTUIDMzE0C0Wq0mV6vH5IjJXPEnCAI2mw02my2BjIPBoEDGw8PD8Pv98Pl8OHHiRAIZa4mM9fo++OhxMoFhGMnv4ove+GGQ7jgjWUODlpSBlpOLJ12O44RmBLvdjvnz5yMjIyNhu1o60lKFWB861m2MBVrXJwgCdrsddrsdBQUFwuuHDh1CdXV1AhmzLAu73S6QMF/AkyJFvSJdPQtyekGOdOWgR+MHH9RkZWWN7eDHEQbpjhOkJjRI3ZXHyxicIAgEAgEcPHgQmZmZWLhwoewjsdpjkCNnl8uFgYEBIdqz2+2yxHqprSF56NXUIEXGHMchEAgIZOx0OuH3+8FxnEDG/I+eOV09cs96Rsw0TeuWD5crLou1xgDwzjvv4Pjx4/jxj3+sy37HAwbp6gw+LxgKhWCxWGTJlofewx5ZlkVfXx/a29sRDoexdu1a2Gw2xXXUHkP8cqOjo2hpaQEAFBQUwOPxoK+vD4FAACRJClEeTzBfFBAEgbS0NKSlpaGwsFB4PZ6Mh4aG4PF4EA6HcfLkSUHWxt+4tJBfvJF6qtAzN6w10pUDH7RIIf7acrlcyM7OHvM+xxMG6eoEcUPD0NAQnE4n5s6dO2H7Z1kWvb296OjoQEFBAZYuXYqGhoakhAtoTy+IC3FVVVXIyspKENEzDAO/3w+v14uRkRF0d3fD6/Wivr5eIBb+34l2IbtUeWUpMna73ejp6cHMmTPh8/ng9XoxMDCAQCAAAAmRsRwZ61mQ0ys6ZRhGl25B3qNDDVwuV4Lvx2SDQbpjhFRDg5bi2FjBMAx6enrQ1dWFoqIirFy5EhaLRSjcqYHa9MLo6Cg8Hg/a29sxZ84cIW8mtR+TyYTMzEyhWAcAx44dw7x58wTtrNiFTOy1wJPxeHaUTZbiC58WkIqMWZaNiYyVyHiy5YYB/QhcK+kmkz5eahikmyKUGhq0FMdS3TfDMOju7kZ3dzdKS0uxatWqGEMYrUU3JUey4eFhtLS0gKIoWK1WLF26NKXjJggCFEXBbrcnPAKKmxj6+vrg9XrBMAyODBL48/kwhvwMijLN+GoVherqlHY/KaEUdZMkKZmakSJjt9stpC/iI2Mt54Keke6liL6NSPcyRPyEBql8bSqkq/aRlyRJtLS0oK+vD2VlZVizZs2YH+HkFAE82ZrNZsydOxeZmZn47LPPdN8PALx7fgTPfNCOPncIWbboqJrRQKyGs98Twa8bInjl5GH4aCDTTIAgCXhCLEqyrLhn3SxsrC2OWWfPqX5hu9l2ChzHwRVkQBIAyzWjVGa9iYJSvlIOUmTscDgQiUSQl5cHn88Xk1/n0xpqyFivghygXxpHS6TrdrsN0r1cIDehQQoURWkiXTVSqnA4jI6ODng8HhQVFWHt2rW6XRzi9ALHcQLZWiwWgWz1AEEQeOv0IH68rxUBmrvwGrB6RhYaerwI0tFjcAXlvzuGA3wXuNgT4QBEt+Nwh1C3sxG//6QZ/QFgyM8g02pCgGYRYaLLiEmcvcD9DncID//tPJ54pxnuIIOSLCuuqsrFx80j6HOHZMlcL+ilFuDJMiMjI0ESyLKsoCuOL3byaQo+pUPTtG7pBb1SOFpJ1yikTWGIGxpaWlpQUFCArKyspCeT1kiXX17qZA+FQmhvb8fQ0BBmzJiBvLw8QSSuF0iSBMMwGBoaQmtrK6xWq6Sed6z4rCeMX9W3QBzrchxwsMOt2z5OOS9+7+6Qur9BhOHguvD3crhDePVYn/BePCnzUbgrQCPPRuD79v4xEfJESMZIklQkY6/XC5fLhd7eXni9XsGzQhwZ22w2zceplypHa0433iZ0ssEgXQlINTREIhFEIhHVKYBUSFeckw0Gg2hra8PIyAhmzpyJ6upqkCQJp9Opq66X4zh4PB709vYiGAzqQraPvX0er9f3geUAkgBuX1qCrdfPwWvngpgcKl1tEJOyOAp3Bjls29MEACkTr55NDVq3I0XG/f398Pv9KCgogM/nE8g4GAxKygDlhpHqqRDRovc1crpTDHyBimGYhIYGLSkDrSeb2CchEAjEmNDMnTs3wc9UD9LlOE6IbEmSRH5+PhYuXDimbe451Y9H3jqPQOQitbIc8OqxPvy1oQ8R/XtALjmCNIvt+1qwfV9LTPoix06h7trKpGSsFznpVQDjb/7xyhP+PbEMsKenRyBjKZMgPaVnYitPNcc/mWGQLqRNw+NzthRFJZhz6AWSJOH1etHS0gKfz4eKigrMnz9f8mLUmrqIv6h5sm1paUF6ejpqa2sRDofhcDhUb3P3cQee2t8ChyuI0mwb7t1QAZpm8MhbTQjS0rHs5Ui4POILfvxrD//tPADlKHiySb2UtiMlAwSiRMcrKXhNdjAYRCgUwtmzZ2OkgHzDkBaoTS9Mlm7HZPhCk67chAYpjJcMzOPxCPrXmpoa5OfnK56UqXjqmkwmwYOhtbUVGRkZWLRokdAWzH9+Nfjt6SA+7D4tpAl6XUE8vPscrCZCKIQZiCLCcHjkrfP4wa7GhFQLj8lm7ZhKpGgymZCVlRXjd+D3+9Hc3IyysrIYMg6FQjCZTAmRsRIZayXdyaLBlsMXknT5Vl2n0ykYdic7YSmKEoTpWvYjdwK43W60tLSApmlkZWWhvLw8xtlKDlo9dcUFsszMzBiy5aG2I233cQc+6E5cLhhhEVQ3eFh3pJlJmE2EpPJgMsQ9UqkWAALx8lMaxorJ1pHGk3c8GQNREhX7UnR2diIcDieQcUZGBsxms+qcbjAY1M1yczzxhSJdcUNDOBxGW1tbjFGJEkwmk6b0Au9TEE+6Yr+CiooK5ObmoqmpSVdzcgCC6uLIkSPIzs7GkiVLYLfbVW1z93EHHnv7PEYDUSbNsVPYen0NntrfouoYlVCSacaANyJItsYKf4SFjSPxxM01CY/x1z77ORzukMya2kAAupH46/V9uHN+1Cw9HA5PKtLVaztKRElRFLKzsxOkXXJkzOeNxcNIpVrHR0dHJ7W7GI8vBOlKNTRYLBZNJKpVe8uTNN+Sy/sVUBSF6urqmJNDqzm50nFwHIf+/n60tbWBpmksWrQoqYRGbGSzbfdZ/OlIT8z7owEa/+evp1UdXzLcuSQHP/mfQV22xSNIs3jmg/YE0r2qKjdG/jUWZNlMePC6qoSCWSpgOeCTrhBePNyNQT+DPCtw25x2XFOdE6MM0JL/nIyRrtabiRwZHzlyBCUlJQgEAjGt4xRFCd+V2+3GyMjIpFcuAJc56So1NGj1ZdVaSONJ1+VyobW1FTabTbbRQIvETC7SFZNtdnY2li5dinPnzqkyk+HTC7uPOxIIV2+sKOCQbyfgDOj78N8XF9HuOdWPnScGdNu+O8hgY20xnvmgfcykCwDbP7p4MxgOAf91MoJXm4bxr6sIrC4JClEeTyxiMpbKuU62SFfPdmKO45CbmxszcgmIyjj5yPjdd9/F66+/DofDgfXr12PBggX47ne/i5qaGtnt7t27F/fccw8YhsG3vvUt1NXVxbx/77334oMPPgAQzVEPDAxgdHQUQPT65tU+M2bMwK5du1R/nsuOdOUmNIw1ua4ll8rvv76+HllZWaitrVW0NhzL7DOO49DX14e2tjbk5uZi6dKlgrOY2lQEv9xTH449faCEAjuJ6dOn4/vX5mPbniZdC28lWdaYll+CgG4pDADItkcvlXhy1xOuIIMnPx4AAeCOZSXYev2SGGLp7++Hz+cTnqDEuU+9LBn1jHT1bOCRun7NZjNycnKQk5ODe+65BzU1NThy5Ajuv/9+nD59WjHqZRgG3/nOd/Duu+9i+vTpWLlyJTZt2oT58+cLyzz99NPC/5999lnU19cLv9vtdjQ0NKT0WS4b0k02oWGsUBPp8gTY3t4OmqZRU1OjevZZ/HiSZMtyHAeHw4H29nbk5uZi2bJlCTaOWkiX4zg4XEFVx5AKbBSJ/7UwAxzHYWNtMeq7XLo9+tsoEldV5cYQud7qIW+Qxp5T/SjJsuqWJ5YDh9iCG08swvsSU45dLhcaGhpi3MeUJlbIQS/vBb1sHbWAb4zIz8/HVVddpbjsoUOHUFVVhYqKCgDAli1bsHPnzhjSFeNPf/oTHnnkEV2Oc8qTrrihoaGhAYsWLdJEtmolO0rRKMuycDgc6OjoEKLN9vZ21Sev1pE9o6OjOHDgAPLy8rB8+XJZ4biaXPHu4w78dH8zHC79iSTbZhJUBfesm4Vqiwscx2HPqX68Xq8P4QIAx7K6EbgcaA545oN23LNuFrbuaoSMHFlXvF7fFyMv4yE15djv92Px4sUxkfHw8DB8Pp8wsUKcopDz5dUzYlbb0KAELSlALWY3PT09KC8vF36fPn06Pv/8c8llOzo60NbWhvXr1wuvBYNBrFixAhRFoa6uDrfccovq45yypCulsQ0Gg5okOHz0qkaXKEWMLMuip6cHnZ2dKCgoiCFALUSqZlme2Jubm2GxWBTJlgcf6carEYCoIuH6BcV447gDwXHqXEizUPjK/EK8c3YIdTsbAQB2qh8RhtP10T80QfLgPndIKNbxn2c8oeU74nOxSuODvF5vgi8v7z4mNrzRywN3otMULpcLpaWlY95nPHbs2IHbbrst5jg6OjpQVlaG1tZWrF+/HgsXLkRlZaWq7U050lVqaOA1feNBuvFTEXgv2+LiYsE4XAytpCsXkcZPhKipqcHo6KiqKIIkSew9O4Qn3u8WnLZ4jAbopEUzEwHcsbwMHzU50esKwkREXb7UIt48BoDgLiZ7zDrnYvnt6SH5KsmKfucba4vxs/fb0OcJJ1lj7NhzSr2hjlyEKp5YIYaU4Y3P58OxY8cSindaO8n0Si9o2Y4W34WysjJ0dXUJv3d3d6OsrExy2R07duC5555LWB+Iyj6vvvpq1NfXX76kyzCMYDwTf5KZzWZEIhFVI2oA7YoEjuPQ1taG3t5elJaWYvXq1bInRCpNDGKIybawsFAg9pGREU0dab8+0JtAuGrBcBAIl4A2wk0FNorUvauNIgmwLKdLOuCedbOE///rFaXY9k7H2DeaBHU7G1Hf5ZJMM4wVUoY3hw8fxtKlSyX1smKJlrh5QQp6To0YD7OblStXoqmpCW1tbSgrK8OOHTvwxz/+MWG5c+fOYWRkBGvXrhVeGxkZQVpaGqxWK4aGhvDpp5/i/vvvV/eBMAVJl593LwWtJKp2+Ugkgs7OTvh8PhAEoUi2PFJNL4hTFuLxO1q2K5VOSBW9F4pr453CJAlg28ZqoWVWL4R1ulPcUpuHa2vyYhQSVlJbaoMgUivwvXqsD0vLsxUjXj19B6TaeoFYidbg4KCgBTebzTFz79LS0i7ZqB61pEtRFH7xi1/guuuuA8Mw+MY3voEFCxbghz/8IVasWIFNmzYBiEa5W7ZsiYnyz549i7vvvltI39XV1ckW4CT3rXrJKQC9SZc3Dh8YGEB5eTkyMjIwY8YMVYUGiqI0KRJomkZnZ6cw6yx+/A6PZIqE3ccdeHDnmZSj20sBG0Vi28ZqbKwtTporpcho6687xKZMYlpxY00m/mmuGb/eewwvnQohfOHr5wnXTKoz9OG41KN5qeaPiYZYoiVGOBwW8sV8isLr9eLcuXMxXWRpaWmaiVirgbkWL90bbrgBN9xwQ8xrjz76aMzv27ZtS1jviiuuwMmTJ1XvJx6XFeny6QW1kCNd3jjc6XRixowZWLt2LUiSRF9fn+rqrtpIly+QDQ8PIzs7W5Zsk21Xz+h2vEESQKY1VtnAE0qpgiSrNMuK2+ZYkJefhx+/1znuN5b4UT7f2/e5QLhiqK1DkgRw86KilJQWvD5YHGmLv7tLafJisViQl5cX07xw5MgRzJkzB8FgEF6vF8PDw/D7/WBZNmFahdK4+cttPhowBUlX6eQaa6QrNg6fNWuWYBzOg49IU1U7iCEuxhUWFiIzMxNVVVVJtysV6e4+7sADb54Bo+dz+Tjix5sSfRKAKKG4ZG4aFBHNqVaYhvH/vZVYGATUP76biOh5RCt8X2YTkTCmZ6yNESyHlLvk+OYPsRbZ4Q4JJurJ1eDJoWeKguM4QRkhNnLilRRS4+bjlRQ2m01TpBuJRHSRqY03phzpKsFsNsPn86lenk8B+P1+tLa2wuPxSBqHi5cfqwyMYRh0dXUJU3xXr14Nk8mEAwcOpLTd3ccduvkiTASKMihZwn34b+dlo1deJ/vMNVmyM9Q4LkrOyYpm31udg4z0DLx4eFA2qo4wHLbva4k5Vj0aI1ItFPrDNJ54pzlhfd534vE1k6cbDZDXv4uVFPHj5vk5bm63W5hWEYlEYLfbwbKsoifvVPHSBaYg6eoZ6dI0jd7eXgwMDGD27NlYsGCBbtuPJ8d4sk11iq840pUyp9Eb07JtgnpBfFpbTcCV00z4xMEipEEaMOClcfVP/wfTMk04PUQLPrM2ikyaLlATaaZbTQgzbIylohilWVZ8aWYajg5Ff1eSksV7LNyzbtaE6HOloDSs0+EO4WCfBQ89+/mYhmnqOQlYK+TmuPG6dLPZnODJy0fE4kaMye6lCwD6jP2cYMh9sWpzuh6PBw0NDeju7obdbseqVatQVFSk68BJnqAZhkFbWxsOHDgAlmWxZs0aVFZWpqxh5Ft2J8KcZlq2DR/cdyUaH7kGP/mHBSi9oFPNswEPrC/Hc9+8Go9vmo9p2eokejycQQ4nB2lBpcByUYvGZMiyRQkhyyp/2rqCDCI0h83LSmCjEpdzBSK49dVePLa/Gw6Nnrsba4uxeVmJhjUmDi+eCAufh0877DnVr2kbenWjAfqRH8dxyMzMRGlpKaqqqrB48WKsWrUKixcvRklJCQiCwIEDB7B582Z0dnbiy1/+Mu69914cPHhQcbt79+5FTU0NqqqqsH379oT3X375ZRQWFmLJkiVYsmQJXnjhBeG9V155BdXV1aiursYrr7yi+TNNuUhXCckiUZfLhZaWFrAsi4qKClAUhY6ODtUniNZI2uv14sCBAygrK0s5spXDw7vP6rYtigTin3pJArhvQ1TsHQ6HUW114cdrTbDZ8jBjxgyh4+mmxaW4aXEp5v7ovQkxDWcYBvd+aToef7dTNo1Ac8DeM4OSRSs15M4j25YY9W29fg6Wlmfj6f2t6PdOnqJlfCggZ3epBL0iXT0f9eVyumIbyC1btuDqq6/Gd77zHfzhD3/A6dOnFX111ZjdAMDmzZvxi1/8Iua14eFhPPLIIzhy5AgIgsDy5cuxadMmTaqJLwTp8l62JEmisrJS8Ov0+/2a7RqTRbq89Ku3txccx2Ht2rXj8sgW0Kl1N8dOYV5JBg60jca8znLA4Y5h1Ng9GBoawqxZs1BTU4OmpibJi6ok2zou/g1iuIMMRkdHUWby4FuLbfjD6SA8MrznCjL4uHlkTPv7yvxCydc31hZjdYkJbrcbZ/3peOaD9nE3wUkFfe4QPB6PaqnWZHMqA9RLxnjlQmFhIa6++mrFZbWa3Yjxzjvv4Mtf/rKg1Pjyl7+MvXv34mtf+1ryD3MBU5J05bxwxekFjuMwPDyM1tZWmM1mzJkzJ+Hup6eul6ZpdHR0wOFwYPr06Vi7di0+//xzTSdfMvOd6EDIZvTqTG6H2kclX3/9qAP//7K5WLNmDfac7Mdjv/3oQp6zEzl2M7ZePwc3LY72ut+zbja2/e38uM5JK8myoqAgG9OmTcNymw1vNB+FRyGdNFa1wc4TA7JNCfzfKpkXA3lBUVGSZYU/TCvmZvVGQboJ3d3d8Pl8glRL3MRgt9sT2tsnm62jWtJ1u90J5udyUGt285e//AUff/wx5syZg6effhrl5eWS6/b0aEvzTUnSlQNfZOIHMNrtdsybNy8hOc8jFdKNn5MmJtvy8vKUI1vxEEkp7Dzei4d3ndVUtFIDJUNuFtGTavdxB+rePB2TghgNRPDgzjMAoimGG2uLwbEcfv5h+5jmk/Ha2Hi/XRtFXmjDjUavFosFAwqP95kWAlYTMDQGs3T+ER1AgjZ2ZdFFJzt+GSmw3MXPBEyMUQ6PCEuglcnDxhXzZKVavJogIyMDLMuCZdkxD8vUIvNKBrUEPjo6qpp01eCmm27C1772NVitVvz617/GXXfdhffff1+XbU9J0pU6ITiOw8DAAPx+P/r7+7Fw4cKkQ+rEY2rUQJxeiEQi6OjoQH9/vxDZjuXuzm87fhu8B8OTbzXqTrhJj4m4oAF+47Sk70KE4fDU/hYh2t1YW4QbFxZjz6n+lMiFJKLSqAd3NiLLZoLNTMEVoGOq8Y2NF6NyJQnXQxe8CsZqls4XpeK1sfdeWYS/mx4tLCaLqPl1tm2sRo6d0mXyhBrwY+CfeKc5rhFltrAMw6YkbmsAACAASURBVDCC6c3g4CB8Ph8OHz4MiqJiouL09HTVRKq39ExNykNvsxuxtvhb3/qW4K1QVlaGDz/8MGbdZOmMeExJ0hVDbByenZ0Nu92O2tracdkXRVEIh8Nobm5Gf3+/ENnqkQeLdxrjO9Xa29tRWFiI4WBqhEsRgJkiZCVUSlg1Kwdbd59VNLrhjc/FN0KlyE8JLHdRGuUKMrBR0gMn+RulVEQMAJuXlcSsw0epWTaT5sd7kkjU1gZpFi8eHsKV5dMBqNPv8lFz3bWVuk/NUEKE4eC6ECiImyn478dkMiEzMxOZmZkgCAKRSATl5eWCz4LX60VfXx+8Xq8gzYr3WYg///WeGqEGo6OjuprdOBwOwSZy165dmDdvHgDguuuuww9+8AOMjESfuPbt24cnnnhC07FOSdLlZ3r19vais7MTeXl5WLZsGaxWqyDN0kv6wiMSiaC3txf9/f2orq5WTbZqH9V4pzHxRIj8/HzB8KY0e0Awn1GLHDuF/zXXjIrZFQnpATGy7SasLqHwXnsILBdr6ZjMa7c02yYYofe5QrpOVkhWgedfl2qLFS8j/l3LhGCriUBI5o4z6KPxYZsXv9+pfntiP95LVXwL0ix+sCv6FBL/vYoLaVI+CxzHIRQKCWTsdDrh9/sBICZfHIlEdL/+ksHtdqvq6ATUmd38/Oc/x65du0BRFPLy8vDyyy8DAPLy8vDwww9j5cqVAIAf/vCHCbPbkoFI8ng9Kds8BgYGcOrUKRQWFmLWrFkxLlxHjhzBwoULVbcDfvbZZ1i7dq0sMYbDYbS3t2NwcBAlJSVwu91YunSpqm1//vnnWL58uarHshMnTiAjIwN9fX3Iy8vD7NmzYbVaLxTPWiQbFKTAm5N/1OSEwxVEnp3Ag9dHq7Lff+O0ZJus3Uwi3Qw4/SxKs224b0OlahnY11aUjasROgHgxEMXR680NjaipKQk5fzdY2+fV+V9kGOnUHdtpSw5plNRzwUpLwY5lGZZse97q4XfFz3+8ZgvsLH4D29eVhJjGdnV1QWz2YySEm1aZJZlY/LFTqcToVAIdrtdyBfzhKxmUKp4u8eOHcOKFSuSLvujH/0IGzZswMaNGzUd+zhCNtKakpFuRkaGrDEMXxxTS7pyRuZisp05cybWrl2LcDgsPFaoAZ+nVSJdforv0NAQWJaNmQix+7gDW3efFQhNzbUVb07uDHCoe/M0HtxQLutLEIiw4C0Pel1BbL2gAS690I2mBDXR8FiQJaGVTRVaJgTbzSYhEpQq6pEkh7CGHPvFQuBF6PFUMBa7jXjLyFRH7JAkKTiJFRUVwWazgWVZlJaWCkQ8NDQkjE6Pt4JMT0+XTEdoSVNoGdVzqTElSddms8l2nqUqA+NJNxwOo62tDUNDQwLZ8o9KWrwXki3PF/5aW1uRnZ2N4uJiFBcXx5z0T+1v0YXQaBZ4+sMu1VFRMMLiqf0tuG9DZQzpx8NEQHXKI81MampM4OGPsDHTE8ZSVZfyLpCDwx2K2W98CuNBhUIh75TGf9/5NgK3VpGYzvThN/v78YeTXgz6aGTZTLpPytAKcfpGTw9cs9ks68srtoLs6emB3+8HwzCw2WwxUTFBEJq8dPVUL4wnpiTpKiFVe0exnePMmTMTHMYAbW3AcstzHIfBwUG0tLQgOztbGJne3Nyc4B6m53Rerc1TDldQUCU8vPuMZCFOi7OiEuESiEZ9o4Fwwn4iDJeQ102l42nPqX7NRTRx0Sk+//nUey0Y8CXe3ONTCDw4jsPOhl788kiroEKJHg8HhSfRcYdYeaFXLYRhGNjtdtn3pawgOY4TbCD5OW5erxeRSARnzpyJiYqtVmvCzdflcmnqCruUmJKkq6fpDUEQaGlpgd/vl7RzVLtfKfBWkMBFsm1tbUVmZiaWLFkSc2JKEbSax/vxQqnIT4FLkk0e6/yxPBuBr1ZTeO6o9KP2WJscgNQUFUqFvLuW5uDnB5wxhTapFAIPgiDwy0+6JWR/l9aghZ/7Blza5giCIIShmrz72MjICAYGBlBWVgav14vR0dEEwxuSJNHa2gqv16uadPfu3Yt77rkHDMPgW9/6Furq6mLef+qpp/DCCy+AoigUFhbiN7/5DWbOnAkgep0uXLgQADBjxgzs2rVL0+cEpijpKkFtpMt75w4ODqKsrAyLFy/W3aGIJ10+ss3IyMCiRYsk9cNSc9Lu21B5SWwbbWZS8F1Qk+IY69OxM8jhheN+ZFqjEyHiUZhOCe2sybDnVD+eeKdZiGr5gpgScW9eViJbXJNb7+rZGTCZKLxSP6La2Ws81AoUAVxblY4jjhAGvbSmv0X8TUIv7wU9yZvP/8Y3ONE0DZ/Ph/b2drz22mtoa2vD2rVrMW3aNNx+++34xje+IbvNZL4LS5cuxZEjR5CWloZf/epXuP/++/Hqq68CiKo0GhoaxvS5pqTLmBKSRbrBYBBnz57FsWPHkJOTg5kzZyIrK0t3wuXlNefOnUNfXx8WLVqE2tpaWeKQinRvWlyKqgLpx7QV09M1u3upQY7djMdumiekFrRG2mZTat9jkI5K6+KdwawUgbuW5qCrqwv19fXo7+9HW1sb2tvbMTQ0hFAoJKQb9pzqx9ZdjTFpBL5BQK4gl2OnsPX6OYKDWjxKZF7nOA7XVGdj3/dW44mbawAAD+5sxLXPfp7g7rXnVD+ufTaxzVQP0BzwdpMPC3M57Pnn2SjOSG6wz+PmRUUxN4nJ5r2g1NnGG94sXrwYL7zwAoqLi1FfX48XXngBV155pew2xb4LFotF8F0QY926dcJ1umbNGnR3d4/5s8Qcu65bmyAoEaTZbIbX6014PRgMorW1FS6XK8aovLOzU1M6AlDW3vKeD83NzWAYBtOnT8fs2bMllxXDZDJh7zkn/vCXLjhcQWTbKYQZDv6wdB6ycySEX16fj/eaOLx0Oqxbt1qQju4vGAzipfe1z4Eaywgdd5DBEzfXKOpuGxsbkZmZCZIk4XK58NrnbXjtXADOoLx8KsJwICxEwnwyG0Wi7tpoRK/cepwIjuOwv8WNX/6+OYbkHe4Qtop0sPHTHsYCuckYHID9XQz2/7YNmRb1N71dJ/rxen2f4Gm8rtyM/1szeSJdre3EJEnKjlHnodZ3gceLL76I66+/Xvg9GAxixYoVoCgKdXV1uOWWW1QfH48pSbqAvOlNfKQbCATQ1tYmkO28efNiCFPLAEnx9qXkajzZWq1WLFiwACMjI6qLPvtb3Pj5Z0NCjjBZq+iAj0ZJSQn+vbYWVZV9eGp/Cxyu4Jgf9YMRFk/uPYd8nxUvHZ9Y8X5JllWyaCUGSZKw2+3Izc3FnlP9eOmUA8ELX5WSCmA0QOPRGyrwq0960OcOoTjTjH9fXyHsS06lAEQbKuJvAh+0evHc4RHJmx3NRZUSG2uL8cwH7boQrtqBlp4wp1oRERAdO8sB+zsj2P5uG364sWYsh6qb9wJN06pSSrwMTW/8/ve/x5EjR/DRRx8Jr3V0dKCsrAytra1Yv349Fi5ciMrKSk3bnbKkKweeFAOBAFpbW+F2u1FRUZFAtuLltYz44dMA4j/y8PAwWlpaYDabMX/+fCH/5Ha7VRP6fx3sk+1+kkJROiUUHHhPW0CfaRKDfgZD6bPgCp6RXYZPbehV6JOLKuMHMW6eZ8M/RK18NRPaD99qRb4N+Je5wHVzs5Cby8Dtdgs60XjCV5pJ9t9HRxWfLvjoN1ke12ICbl1cgp0nBmI+C0UAGbZY7wm1fhYsB9gpIoZU1eIvx/txz5UlmrwW4jHRBTmXy6XonyuGGt8FAHjvvffw+OOP46OPPoqRcfLLVlRU4Oqrr0Z9ff0Xh3TlIl2apjE8PIyGhgZUVlZi/vz54zbMcmRkBM3NzTCbzZg7dy4yMzNjltUiMRuQM4aVW95HY/X2j2LsFQFg203RHvHXjvZoknSJYSKAp/e3Ki7DF9qUdLxqwMvFpIpQUqT3qyMhZGaO4I7VuSmpGpxB4HeNQFGRCV/KYtDT0yNYH4q7pzIyMiRJPUiz2L6vBR4VrWjJJjeUZJpxW40Vd18XNUbnu9/IC3Pe7GaTkP7Qqr5IhXCBKGGLvRbitbNpaWlJ6x8sy+pSI9HqpasGanwX6uvrcffdd2Pv3r0oKro48nNkZARpaWmwWq0YGhrCp59+KhjhaMGUJd14iIdLmkwmrFmzRtUfXivpmkwmjI6O4ty5czCZTJJkK15WDenuPu5QPclWjHh7RR7bbponkO/Tb3yCnW2cJoNxhlOOYPlpObyywURE19Fb6C9FeiEG+K+D/bhjdUXKHV00Bzx/yIl/+vu5wmt8K6vX64XL5UJPT4/sttW4hKWZSUWipAjg7iumYdg5LKQvsmwmmE2EkBd3uEN4aFdjyjfPVEASQE1NNL0g1s7ydpB+vx8kSSbcoMTtvQRBTDjpqm2MUOO78P3vfx9erxe33347gIvSsLNnz+Luu+8WbFjr6upUGZ8nHIPmNSYJ+D+q3+9HS0sLfD6fENkePHhwXEbwuFwuOJ1OeL1ezJ8/P+kjjRrS5Vt9UyWreHvFePx9uQ3/+4aFWPzj/9HNSCPEIsZAh+GiMrObFhZhd9xjso0iYTOTskTFz/TaKmHCIkd6/JgcqeKXmmnAQOKgx7fPDCbkc0uzginLvMwmQjESpzng0Xc6QBIXm0ykmjcmknAB4PalF30XpLSzQKwdpNPpREdHR0x7byQS0TSxQg7JWuh5aHEYA4AbbrgBN9xwQ8xrjz76qPD/9957T3K9K664AidPai8ux2PKkq7P58P58+cRCARQUVGBgoKClO6uakjX7XajubkZHMchPz8fhYWFqnJIaratR6uvUucaf1dW22ihttEhPpUajLD4qHEIddfMwK8/cyQUo5JV8MXFJ+HYZSJn8sKfOb74lWUzRTvfNDKVVBqjbmcjzBKnk40iYaWIpN1tvH+tEmlzmHhSTYal5ckjRrEdpBh8e29/fz+6urrg9/vBcVzCxAqbzabqWqVpWnVOd6p0owFTmHS9Xi+mTZuG/Pz8MT3KKBGjmGwrKyuRk5ODtrY21XlaNZGuHq2+HIB1T30iuIOJQZIkRkZGsGk28HwSTXdhugmDvtTHyQz6aNRY3Xh8DYmDDjP+2hzBgzsbUZRhxqJpGTjU6VZc3xVkYvwO5KJ/8et88WvPqX78YFej6ieGLOvFi1muICdlQXzzoiIsLc/Gj/7WiJDCV0UQwNysCAY8k49YlSBn+6gGFosFOTk5sFqtwmO32IHM4/HA4XAgGAzGjFDnUxTxUa3aQprb7VZdSJsMmLKkW1JSohhFavGxjS/IeTweQWdbWVkZcxfVko5QIl2O49Db24s8GwFnigblYojdwXjidbvdGBkZQSQSwfSyMqChRXZ9K0XgzkXZ2HHGn7IioSTLitraWuw51Y/fnr0YOfZ7I6on59btbMT2fS2ou7ZS1iTHfqGBglc2pJIC+I91M4RtaFn/4+YRbL1+DnodDvzisFvxxvCJg7ukZjapgOWif4P6LleM7aNaxKcE4h3IePAdZXxk3NLSEmOSnpGRIfhLJ7uOXS4XZsyYoflYLxWmLOkqQUlLqwSv14vm5mZEIhFUVVVJPrKYTCbVMjA5w5v+/n60trYiPz8f1cWZcHYoR4BqEYyweOztRqyvysJv3j+JP532wRngUJIZgCuorEYI0Rx+d9yFB74yN2VFQiDCCEQ4Fm3qaIBWlEcFaHZMTQfXzDTj+vkFwja0gM/T/n25Dbk5OXjy/W7ZYxhLo4hWlGZZxzSbLh7xto9qoTY6FY9Q58F3cfKFu3A4jKNHjwJAjOENX7jjyXh0dBSLFi3SdJyXElO2DThZV5oWpzGGYXD8+HGcOXMGM2bMwMqVK2VzRKlGuryV48GDBzE8PIxly5bhT+dZHNRIuGYTgcdurJK1SRkN0Pj+nw7hvxuihAsAfZ5E9y4pDPoZ/J+/nk45xzwaoLFtT9OETERIhdhz7BS231yDbyzOSHkbfFswx3G4bm4+tm2slm0hnijk2wjs+95qnHjoKtilEtEpYvs++ScjOYxFo0sQBGw2GwoKCjBz5kzY7XasXLkSy5cvR3l5OcxmM0ZGRnD27FkcPnwY9fX1+I//+A+cP38efX19whQLJezduxc1NTWoqqrC9u3bE94PhULYvHkzqqqqsHr1arS3twvvPfHEE6iqqkJNTQ3eeeedlD4jcJlHusng8/nQ0tKCYDCIefPmoaCgIOk6YuewZOC1xENDQ2hpaUF6enqMu9hrR7U1MfBjz6+Zk4Of7W/FUECaMD7sZi5ZHjFIs+PuEUsS6p3HKAJ4bFPsnLWTJ6NG5lp1vuIGjo87A3j93VMY8ERQkmVFdgrz1/SAiQBuqSAQCARgs9kQTGEWnhxGA3RMjl0N9GqMENtMkiQpW7jbsGEDnn/+eezduxcvv/wyioqK8Oabb8oeWzKzmxdffBG5ublobm7Gjh078MADD+DVV1/FmTNnsGPHDpw+fRq9vb245pprcP78+ZQ+62VLukqRrt/vR3NzMwKBACorKxEIBDTp/NQW0oaHh+H3+9Hb24va2lqkp6fHvK+GGKdl2/DBfbEGHuFwGLfXWPGrhoDkOhNBuNMU1BAsJ9+2yhNyqh1TAPCVqnQc7g2qKvrRHBLGqOfbCfwb7UyqLrBTBKxmU8JE4j2n+vFf9V7wthgOdwhmE6FarqYnNs7NxopCD5qamhAMjr0NPB7xipJkmEjfBYvFghtuuAEvvfQSnnnmGZSXlyu23YvNbgAIZjdi0t25cye2bdsGALjtttvw3e9+N+qFvHMntmzZAqvVitmzZ6OqqgqHDh3C2rVrNX+2KUu6ydILUtFovKaXl5nxpjdqcsBqomiXy4WmpiZQFAWbzTamfBPf+bX7uAOPvd0o6F3TKcBqgmQF3USMH/GWZlvx5N9ZsHr1aqx76hNJ4uUn6EpFvDwh/2hjNYDoI6yWkeSLisy4f8MsHHTQqnO68WPUhwIcfryvHTcvKkpovxWDA4G6aysTSOeZD9oR70MUYTjk2CnYzSZdc6vJ8FGbF7eUW4VzjHzvY12fMrRG72plXnpux+12C+lAJV5QY3YjXobPOzudTvT09GDNmjUx6/b0pNZuP2VzukqQMr05deoUTpw4gZKSEqxevRqFhYXCH0gvRYLH48GxY8fQ3NyMOXPmYMmSJWM6Ab+2ogw3LS7F7uMO1L15OoacfHR0MGK8laLNTOKO5cpOSzzMJgL/+Q8LVFtE2swk7t1QKUy4uG9DJWzmxFOIv+jlLn6xOfj/3HcFTj50FbbfXKMqN+rwRve9sbYY2zZWC5pdJciNUX/79IDk8ccfZzzk0hKuAC3kVicqz8uTIm8feanVEmobGvTcjt/vV5xUMdlw2Ua6kUgkxvSmsrISCxYskDW9UUu6UsuKVQ/V1dUx3TH8uPhUfEo/anJi2+6zsj4KLBdtyS3MsMHhCsZM8k1memMigNuWThMmDScDAeDWxaXYtKgUBw60A7goTeMdzgD1hubxxCU2m9lzql9WvTDkZ4RHyI21xYqzynjIEVHUMF05UpYiWLm0hNh7V6pbbrxwoJfG786p35eWnDsB9fJLYOJtHflzQc31pcbshl9m+vTpoGkaLpcL+fn5qo1y1GBKR7pyJwLLsnA4HGhoaEBBQQHWrFmDoqIi2eVTjXT9fj9OnDiB06dPo7y8HCtXrkxoR1TKAScz/O51BfGnI8rGNYEIB3+Yxk/+YQE+uO9KgQiVolc+Gn6jwaFak8shehOI/w5vWlyKD+67Emd+tF7TIzVBREeQS5l+b6wtlo0UC9JiL2g5k/H4faWKgjQSDocDXq9XiPDvWTcLVNw2zSZCKLKlKpujiKhngxakmUn8tZnWtC8t0TAH4PDhwzh69CgaGxvR09MDl8sle05fKi9dNTcFsdlNOBzGjh07sGnTpphlNm3ahFdeeQUA8Oc//xnr168HQRDYtGkTduzYgVAohLa2NjQ1NWHVqlXaPtQFTNlIVwr8CJ6hoSHY7XYsX75cd9MbPnI9ffo0PB5PTG5YCrzaIT5fvG33Wd10nKMBOqExQm6Sb47djAevq8J/vnteMylIdc/tPu6IiXTVgr/wxXaJ4typVKRoNhEIRlis//UZobClJqLkFAp7SrBRJO5eOw37Gkfw8rEWDAVY5NsILCu1JJgTcezF6RVaItyiDAqDXjrBae3aZz9XJb0zmwg4g+MbTa9atQo0TQv6WfFNiG/x5X9omtblUV9tTldL7leN2c03v/lN3HnnnaiqqkJeXh527NgBAFiwYAHuuOMOzJ8/HxRF4bnnnktdGpfEZHtS99OEw2FBUN3a2oqRkRHMnj0baWlp6OzsFAbIJUNnZycIgohJskuB309XVxcWL16sGD3zOH78OCorK2NmPO0+7hiX2Wc5djM+r/tSzH5+su88BrwRlGbb8O8bKrA4O4yenh58Y5+08kEJvJLis88+wxVXXCGY9STT9ZrJaP5ZCfwUXbF/bpbNBIIg4ArQyLZT8AbplNUB20UTKfLsBGhEVQnxIC+4vWnxjeBRmGYCQRIY8Kq7gRemmfDanTUoKChI8A2+qipXdm6bGASgW1ejHLbfXCOpYOA4TnBm439cLpfQDiy2hNSa5+3q6gJFUSgtlTZy4uF0OvH1r38dH3zwgabtTwBkiWFKR7rhcBgtLS0YHh7GrFmzhBE8fr9fs0euUpdZOBxGW1sbnE4nZs+ejeHhYRQXq5PRSBXentqvXXSuBqOBCFZv/xBbr68RjM1XlZDweDyw2+1ob28DkxEtJJre/VCTwkE8rJKHGrMeigRsZgKRkPLO+tyhhCjRFWRgo0hhhI8WlYMY2bZYg/KGhgZ0oBCPvdOW4Ii2bWN1DMFc++znqqPWIb/6Sr+NIvFPizJBEISk4c7OEwOqtsMBWFJE4dNeZtzyx3ITkQmCQFpaGtLS0oQW38bGRhQWFoIkSSEq9vl8wlh2cVSsZHxD0zRstuQFXi1eupMFU5p0Ozs7kZWVhZqampg/ntaONLnpETRNo729Hf39/Zg5c6Ywnr2trU11cUGKdPUwuZHDaIBG3ZvRKPrGRSXweDzo6upCaWkpVq5cKfieqiVcAogp0PEIh8OqPkea2QS3kjPMBZRkWWVNw/kIMBVQBPDgdVUxr3Ech+sXFIKiqJjJwVJKBi375fPLatIC62fbsaIwOmrmmQ86JD+32oLXh10R3La0BB83X5xMrGdXoJbvgKZpWK1WpKenx5ChOCqWMr6Jj4rHw0t3smBKk+6cOXMkE/pjmQYBRE+czs5OOBwOlJeXY+3atTHVUZ5I1ZwUUoU0tTaLqYJmgUf3nENxqAskSaKwsBBz586NWUapuUG8THxjBsMwCIVCOHz4MDKtyQlVDeFaSOCr1RSeOyo9NomfpqDV5J2U6Ebjwd8wxSN3+DZm4GJ+WS2BmU0E/GFata51f6sfZVYKV4S7ZbevtuDFcFEjnn3fWw1AWf2RCtQUK4VjkSmkSUXFABJyxXxUHIlEwDAMaJpWjIoN0p0k0Gr1yJMuwzDo6upCd3c3pk+fjjVr1kieQFruxPGRbiQSwddq0/HTT1Mn3WnZNrj8QfgUgnl3iMG8efNA0zQcDkfC+3KFNjG+VJ0v/J/jOPT09KCjowMAMJQxG/6I/Aw1tcixU7j/yxVYNzsDr507gQGf9M1Sq/5UKlUQD6XIml9Pbkrw2lIC51wU+twhId+spZEgxHD471MRvHha25gmOTjcIfz9U5/hunkFSVMTWiRjYlWGGmhVL1AUhZycnISo+Pjx48jIyFCMiq1Wq5FemGjoMRIEiGr83G43Dh48iNLSUqxZs0aRULX4L/DLMgyDzs5O9Pb24vp5M/DTT51J182xmxGkmQRiHA1E4FdxrWZmZsLlcglSJzH4VMEDb5yWTTW8drQHy2ZkY+00M5qbm5GXl4dVq1bh8OHD+Nn+1gQj8+SfhxLystk2Ex68riqGFO+7plIXbavUtsXgi8dyj83i1+WmBBcHu7Bs2TKQJIlrn/085Xyzns0MowE6afHNRpFJO/F45NgpyY48JeghGeOv66KiopgxQLwdpMfjQV9fH5588snoE1dmNDe+aNEi3HjjjTHryGF4eBibN29Ge3s7Zs2ahddeey3B5KqhoQHf/va34Xa7YTKZ8NBDD2Hz5s0AgH/5l3/BRx99JETZL7/8MpYsWaLq801p0k2GZHlX3tOWNya/4oorVLcCq/VfIEkSQ0ND6OjowLRp00TRs/Ljn81MYuv1c3C0czSh0cEf34MqgRy7Wdi/FOkCUeL9voKKguGAh3aewb8uy8Q3NizBe+dH8a/Pfp5SaqQ0y4q931mJY8eOYcWKFZLLiAluLDnJNEvyi54giKRNDvGKgidEVfzDhzuFcyvVfPNEo/SCKuLj5pGkhGujSM2EC0SvqVQageIh9SQZbwf5m9/8Bj/72c9gtVpRXV2N48eP48Ybb1S1/e3bt2PDhg2oq6vD9u3bsX37djz55JMxy6SlpeG3v/0tqqur0dvbi+XLl+O6664TIuuf/OQnuO222zR/tindHKGEZAbiDocDBw4cgNfrxYoVK2CxWFT776rJGfOE3tLSgnA4jNWrV2P27NlCFJBjV77f3XpBffBRU/KIOB5mEyEYUCuRLhDNLyshzAB/Ph/Be+dHsXX32aSEm20zwUYlnlYOdwhfee4wPuuJhuh822p8g8TG2mLs+97qMbXROtxh/HB3I3711mG0trZicHAQgUAgwQzlnnWzZI/1yp9+iof/dh6OCz4KvJ5Y3MjBk65czrM0y4qTD12V8ufQE8UZFL59ZRl2nhhQdUOTa4GeKKglb6/Xizlz5uDWW2/Ftm3bVEW5QNTY5q677gIA3HXXXZLOZHPmzEF1ddQjZNq0aSgqKsLg4KCGTyGNKR3pqhmtLr5bchyHwcFBtLS07zp89wAAIABJREFUICcnB8uXLxdm2ifRK8cgGaGL91FdXQ2/3y+RrlBOjbxx3IHlM3JUKx1y7Ga4ApEEpUE86YqbGUqzbfhSdT5eP9ajmCpwuIKq5GE2ihSUAlLRqsMdwitnALflfMzjrVSDhFwuleNYxTE5PMIs8PszYfy5qQ+D3m7k2wnU5gMnhwBnkEPRx4fw7SvL8MMbqvDshx0JxyqVn43P9/KQa9Dwh6PWiKVjUBNsXlaCvWcGx2wbubTYjGc+aENQg9B5KkTwbrc7pUJaf3+/oAEuKSlBf3+/4vKHDh1COBxGZeVF2eRDDz2ERx99FBs2bMD27dsFLkmGyzbSFcvGeE/bzz//HIODg1iyZAnmzZun+kuKh1yk63Q6cejQIQwMDAj7sNvtksu6AspJ2WCExVP7W5JGosLyNIMtK6K94N//62mse+oT7D7uiCFdvpmh1xW1AOx1BfHGcQduWzYNmRb5kZSl2TZF8icQLe796IZqQQ8rF62GWeD1+j7JAlbdzkYsvBD5AhAMwglEo8ZtG6thkYhM5eAOMRjw0uAQdRb7sJsTmggGfDQef7cDj791XlBHqIEUEfHmO/FPL64gg627GjEaCKs+5nh83DwCd5BBaZY16dOREt5pCQim9mqhRbWgN9QGQUqFtGuuuQa1tbUJPzt37oxZLtnIeIfDgTvvvBMvvfSSEH0/8cQTOHfuHA4fPozh4eGE1IQSLvtId3h4GM3NzYLFYlpa2pj3G19IE1s5xvvmykXFamRjDlcQW1aUJTWvAaIkveNIj0Cb/Mw0mp2D9q4gvv+xtA1jMMLi1SO9yLSSAIiEacB8U4ScMc60C5H1U/tb8INdjfj5h+1C3jBVKRQf+W7bWC3IoHioMbhRC5q9ONVYbUFLjog21hZLNnDQHECPwVic/w4d7hAoIpo6SqV9PJUjqMkKCyoC/ictLU3R80SP4rYWgx2lScByo9QBoLi4GA6HA6WlpXA4HDEyNjHcbjc2btyIxx9/PMbakY+SrVYrvv71r+M///M/VR0vMMVJVwkMw+DMmTNIT0/H/PnzY9pw5aD2j813sHm9XjQ1NYFlWcyZM0dyIqkc6d63oRIP7jyjeAFl2Sj8pV69Z2f8loIRFo++1ZS08MaBd9yK3ca0uFRFvMTMZibxper8mNcd7lDSCroayVKQZvGDXY14cGdjTEsukYJeVy9YSODm2UBTUxMikQi8Xi/S0tKE6CeVx3GKAL66tESVmoDmgGwzibR0akJGIh3sI7CBLcCV2VZ4vV4MDg7C7/eDJMkYIuYn+eppdqN2O6mOX+eNberq6vDKK6/g5ptvTlgmHA7j1ltvxT//8z8nFMx4wuY4Dm+++SZqa2tV7/uyI11+bLrf70dpaWlMDkYJWoZZMgyD3t5eDAwMoLq6Gnl5ebLLypHuTYtLY0zJpeAKpiZDEkON0kEK8Y0RYhvHXlcQpVlW/Mc1VapyvWJYSODWJepIRmyKo2a8Ok/mpVlWBCJMyjIuHhQBZNiiMjeSiKZG3mzjUFBgwnSOQ0dHB/x+PwiCQHp6OgrTKVmdsRQsBLDtxmrctKgUS8uzY5QScqTqCjJIs0zMZRukWfzyk27c8r3VMaOsGIYRGhrEk3wtFgtCoRAGBweTtvkqQYuXrtfrVRVQxaOurg533HEHXnzxRcycOROvvfYaAODIkSN4/vnn8cILL+C1117Dxx9/DKfTiZdffhnARWnYP/7jP2JwcBAcx2HJkiV4/vnnVe97ShveABA8E3hPW5qmUVVVBY/HA47jVI9mPnbsmJCDVdpXa2srhoaGkJ6ejqVLlyY9qcLh6CPaypUrE96b+6P3Ju0XTAA498g1ku8dPXoUtbW1sFqtmj5DaZYVN85gUVlZEdN+O1ZsXlaSMC481WnBagxvbBSJO+ea8G83R0e1MAwDn8+HnQ29+NlnAwlTJeSQbyPw/r1/J1mlV+syNt4gAJxQocDgOA7Dw8Nob29Hbm4uvF4vgsEgKIpKaPNNFsV6vV50dnbGjNGR2+dVV12F+vp63TT7OuLyNLwBLs47CwaDgh0bELV5lPJTkIOSDCwSiQiWkbNnz0ZpaSm6u7vHbBuZajvwtGwb/nlJDn55cEBIC+gNcQEvXvFwa6UJ8+ezwnJqPgM/TPK3Z4DQKf3yskC02BSP+KYGNTcGtYY3QZrFX5s5VMbpeO9ZNwu3LiZVuYMBUHQGS9UEXb4cmhrUFtMIghAIlp9BBkBIw3i9XvT09MDn84HjOKSlpQlEnJmZmdAEobeX7mTClCfd9vZ2TJs2Dfn5+TFf/lj9F4BoBNPR0QGHw4EZM2ZgzZo1gnuS1onAUvj3DRW4/6/aWmn5wtbwyEiMb4De4FuA4+0be11B/PdxYIBtxruNTtWP8HxqQOkekWq+Vi6XKnYWSxY5lsb52SbbtjPIJTiDbdvTpDj+Jx651ujjLJ8jzczMFIhIqhNOTeSr9PVtXlaiWOCMh3j6sRpI5XTNZjNyc3Nj8q4sy8Lv98Pr9WJkZASdnZ2IRCKwWCzIyMgAx3FgGCbpxBW9CncTjSlPurW1tZLi/1ScxngiZVkW3d3d6OrqQllZWYIHg5aONKmTgm+cKPC2w2YC1D5l8yPYb1pciqt+ch6hFE3QTSQBJkmClG/KkMrZhhng9QZpXSNBAKtn5aBjOIA+VwiEhj5/qynaovrX4/2aKvRZtuRFl3vWzZI1geG9fKWgRHZSEbDayNRGkfhqNYlVq1bF5EjFBuEz09Lw35tKBEK+8df1SQlTrkhZmmXF1uvnqDbDkbsJKUFtIU1ciOPBcRzC4bDwHfj9fhw9elTIl4tvSHwU7PF4EsayTwVMedKVQyqRbiQSQU9PD9rb21FcXIzVq1dLPuZo3TYPceNEbm4uVq5cidC+j1WvH6QvsnO/V7tRCq9GONo5Kjt3jQefMtBqQ8lxQEO3G4/eWIMbF5ZgwaPqzaWDNIvX6/s0+xGoiXY21hbLko2S6kCJrLWAQPTm4A4yQiqiKNAJIFpsFbe3AtEbfyAQgMfjwcjICHYcaIbTkzxCZblEWZmNInFVVS7+/qnPVD+ZXFWVq7kFeCzqBYIgYLVaYbVaEQwGkZubi7KyMiFfHl+0q6+vx6lTp8CyLNra2jBr1izVUa8a3wUg+nfhByHMmDEDu3btAgC0tbVhy5YtcDqdWL58OX73u9+p7oQDLoPmCD3mnvFen01NTfB4PFi5ciWqqqpk80pKHWlyGBkZweHDh9Hf348lS5Zg7ty5sFgsqpsfgKgE7IE3TmPuj+T1h3IQqxHeOO5Q5ae7+7hD0/HxCEZYPPVeC1pbW5Fn0/b4l4oBjNQECCnItRYr5S21Eo9cG3TWBROeEw9dhX3fW42NtcWKDQAkSSI9PR0lJSVoDGbi5dMRhFV+NxwbHQfPN5XwBjda1Byv16vLS4sxHvPRTCYTsrKyMG3aNNTU1GD58uVYuXIlvvKVr2D27Nnw+/247777sGzZMhw8eFDV9nnfhaamJqGbTAp2ux0NDQ1oaGgQCBcAHnjgAdx7771obm5Gbm4uXnzxRU2f77KNdNWmF5xOJ5qbm0EQBEpLSzFnzpyk62jJI3m9Xvj9frS1tWHevHkJj0P3bajUNLonlYyChQTu3RAtbmiReD329nk8dH01frBT+zy3Pk8I3d3duG2OBS+fDqlq3VVCaZYVDMtIjsIpybIKKSY+B8ib1fDdZiwX9bugCMSM/FGTt1Tbxitug96+ryWG5FxBRnIWnBjxBjv8473WIZc0F70RpZujUXwqTw+p3Px42dhYkUynSxAEZs6ciRUrVmBoaAjPPvuspu3v3LkTH374IYCo78LVV1+tuqOM4zi8//77+OMf/yisv23bNnz7299Wvf8pH+nKgSRJxUhidHQUhw8fRnd3N2prazU9nqhBIBDAyZMncebMGdjtdixatEgy/ySexqAH+AHD/L/Tsm34eq0VNy4sAaAtXTAaiIBhWdApMH1plg1XXXUV/vfGlbhhnryOWQ1sFIlbK0ncMdeWEEXaKBLfvWoGGIYRzK93NvRg254mgSh5AhkNRFuC0ynEtBYni2bljHHEyLaZhG1trC2G3ZxIGkomMrzETcpgJ5WmCw6ANxL9NxUCVdsWLYZW1YEc1Op0U/XSVeu7EAwGsWLFCqxZs0YwxHE6ncjJyRGOb/r06ejpUd/ABFwGka5WovR4PGhubgbLspg7d65AhKFQKKU8bTzC4bAwJLOyshKFhYU4evSoJsF3qrCZSTx207wEIj906JBQCdYqU3vinSbNEiSbmRQi63fOOfF246jkciYieeROEsBd880oLCrCfx3sixljU5ptxb3rK3DjwhIh0mVZFs/9T7dsZMhw0WKfGu0pDzWWk2kWKoa81Xj1iqFkqJ4t8iGeKNy+tETzOhM9fl3J7Oaaa65BX19iiuTxxx+P+V3Jd6GjowNlZWVobW3F+vXrsXDhQl2mVEx50lULsZ63uro6IXGeSnFM3DbMMAza29vR19eHWbNmxcxtS5YDztHpouLtION1tbdUEFiyJHpBq5kYwSPbboIrkDwvsK7chDOjJAY8EZSIiBAAnn6/VXJfBBH9ScboLAdkZGZg+/5OQSLHcheIXbQfPq1AkmTSyNAbgZB64i86giAk5Unxj/xyiN9nls0k2fzBISpfu2fdLBSJLna5Y74UDRJWEriuwIUzZ87ESNmSdWtONOmOjo6isLBQ8j09fBfKyqIGUhUVFbj66qtRX1+Pr371qxgdHRWOsbu7W1hOLaY86SpFugRBIBAIoK2tDS6XC1VVVSgoKJBcRyvp8kRKkmSMvCx+npp4WTlsvb5Gl5HsHzU5JXW1L5wABtGIT1tdcLiCyLZTsFEmuAIRZNvNcAcjko+gagj3prlZ2H77Mtm/Q59LmjQ4DqrGqZdkWvC7464ETXIwwuKn7zZjQ1V2QrtpSbYVDpn98jCbzWBZVvgBoqTx1plBPPdxV8wYHn7XSgQYT8iKrlUXUgd3zjWB71PUe5hkquAbRJbPLxRUA4ODg2hraxMm9IqJWPzdT7T3gtvtRlVVVdLl4qHGd2FkZARpaWmwWq0YGhrCp59+ivvvvx8EQWDdunX485//jC1btsiur4QpT7pyiEQiCIfDOHr0KKqqqjBv3jxVrmRqYTKZ0Nvbi+7ubhQWFsrKy/htK5HuTYtLdSHdXldQcvxOmAFer7+YtxoN0DCbCPzff5iPp/a3YDSQ2v5uW1KERzctwN9O9uHp91vR5wolRLpqCFAONjOJ+66pxANvnJV8f8AbQWNjIwKBACwWCzIzM5GZmYlv/10ZfvhWq+x27WYCb58ZxNPvt8Lhulhoy7aZ4I+wQtFQ7dNHfDFuz6n+pOvyXW3/duH3VDvQ9ARJICbHzX+ffP6T4zgEg8GEib4URSEzMxM+nw/BYBB2u31M0yO05HRTMbtR47tw9uxZ3H333YI1al1dndCW/OSTT2LLli3YunUrli5dim9+85ua9j/lvRdYlo1RKdA0jY6ODvT19Qlzk/4fe9cdHlWVvt9pmUzakN5DeiGVFMBOEV0VLCxg2V0RZWH5CSLuoggrK+4KiB1l1y7KPsIqq+BawEKxQZoiLb2Q3maSmUkm08/vj3iOdyYzkzvJhBLyPk8eZXJncm5y73e/833v9758BDEsFgsKCwtx2WWXDXmsQqHA8ePHERQUhNTU1CF1eSsqKhAYGGglGmKLx/9XxkvC0Z3w8hBCaxj+TR7oKcBtiSK8W2ay0hvwlAgZT/eTk23Y8EnFIHUyqVhgN5OmAZBbt+032BevCZdL8fWqywEM1NI1Gg3UajU0Gg2WfKqAIyqzl4cIFkJcEuqxhY8E6DMOZKgrp0/ETekhEAqFLms+cJ0l/vF5Je8R4tHAllvsOycPBTrqW15eDl9fX/T3DzzFuZoLvr6+vHsaRUVFmDJlypDHLV++HKtXr0ZeXp7Laz4HGLvaCzR7tVgszMmXbvPLysp4Z69DsR0Aa93cwMBAxMbG8hJC58PrXXd9Arq7lfiiph/nKtcZScAFAKWO4NNGEQxm69+xzmjB1v3lSJX1YkqoH/56XRy2f9eENpUefjIRBBDYDaKeEiFuzQ7D3p/bfpWK/CVLtp2iozVdCg8PDwQGBiIwMBA9PT24K02D107YFw8frvIaFyYiwJM3J+GGtCA2tmo2m7H5QDXvgCsEkPnkN1YPmvOJ4QRc4NdRX7FYjEmTJkEoFMJsNkOr1UKj0aCzsxO1tbUwm82QyWRW5QmpVDps1tBwM93zjYs+6FJr8Pr6eoSFhVlt84c7OWaLvr4+VFVVwWQyISkpCXK53KWA7izochtwj1wbhz/mKrFgj+u8yvOBMLnUYc1W0U/g5+cHjUaDOKEa/5giQHGnJ94+qbc7vkyZCI4ab2YLYdoM4XIprkkKxPMHa/HIR2WspHFt0gT2d/rj7Bz8p6qUV116ONCZCF46cha35gw0USwWCz452eaScho9S/q3vhj+5kOBlhVEIhErT1DQIaTe3l6oVCo0NzdDr9dDIpFYBWK+rhHDteo537jogy4wwKcrKCgYRMweadDV6/WoqamBWq1GUlISAgMDrT6b71SavaBLHxaUlkIbcGq1+qK4+WimSeuitgiTSxESEmLVGV73/Hd2A66vhwBmsxmPfFTmtJ5FyK/C6bbZ8GMfl6MqTYw/XJ3Kyjjrf5M8qLThKsRCOPSP4563UCjEi4frh/1znEEoGKBw2dOkEAHwdQP7JTfCsaSpuyAQCODl5QUvLy+r64JqLvT29qK+vh79/f0oKSlxqLlAodFoxoPu+YBQKERiYqLdp6OrojcA2Faxrq4OnZ2diI+Pt9uEcyWgi0QiK7+2zs5OVFdXIzAwEFOmTLGi4ohEIoT6StCucb5uD5EAhmEK3owU4TbNMns1W+7W32g0ora21uE5aQwEGgO/36XOaMH7pS2DHkx6M8EnjUKs5tTN6fo27a/iHZTEQsBHKoaq34RAmQC3JYjx5mn77A7AmjboKOsfCTzFAyJA31R3w2j+NdsHBoqG9FE+UknHF251nQXgLnh4eCAgIAABAQEwGAwwmUzIzMxEX18fNBqNleaCl5cXzGYzKisrIRQKXW7Y8dFdOHToEFavXs3+XV5ejt27d+PWW2/FPffcgyNHjrBgT0XNXcFFH3Sdgdrq8IVIJEJtbS3a2tqspBwdHcs30xWLxdDpdOjp6UFlZSVkMhkmT55sVzBdKBTij1NC8cw3LU4zNC8PEYIEJrRo+Z2bLegjxJUbdYJMjB/WXGX1GpePa8tesFgsaG5uRlNTEyZOnIjwETAZuHAUAB0FPb411nC5FKtmxCHLT4+2tjY23PL6KceiPc9/9B2ujpHB19cXwT5iu2PKruJXIXUPXBE/wcplg5ABRwuBUOAyy8IRHijwHTHVyxU3bWeg/FequcC1wKLlierqanz33XdoaWlBbm4uQkNDsWLFCsydO3fIz6e6C2vXrsWWLVuwZcuWQSPAM2bMwPHjxwEMBOnExERcd9117PtPP/30IPseVzAmgq4jzVqJRILe3t4h30+lFjUaDSZMmDBIytEeXAnoRqMRLS0t6Onpsau/wIVIJMLMBE9MkKexAQe5TAyDmVg1gHr6TbA/58UPrt4iUpEAK68Mh8FgGFTGmZMZxoIvBdW0CAwMREFBAcRiMa5J0mB3ScsIVj0AIWC32WhveMFRjZiCm9lazBbUVNcgc3IkCgoK2DXg7GHx1mkT3jylQYiPDgURnviyppe3c4Q9cJkfFosFs7cdG/TQMBEMT4TDDmQSAa6Iko6I4gW4ZibpDM44urQ8kZWVheeffx4//vgjjh8/jra2Nrvyrvbgqu7Cnj17cMMNN7jF0JZiTARdRxiqBMDd6gcEBCAoKAgRERG8nvq2jsD2YDAYUF1dDaVSCW9vb+Tm5vL6XLPZjLnZkVbjvDOe+84tXffhIMzXA4vzA5EXaMGJEydgNBrh5eUFX19f+Pn5wdfXl7E4+vr6UFlZCZFIhKysLJbNf3KyDXt/5k+H8vIQod9oHiRq7iEEfpPihy+qe62CqVQE3BwLHDt2DF5eXmxdzrb8cpkIWoOFZYrtvUbsLBciKUmGOM41sHpmPB52wBWmWXd7rxFf15kxb3IEjlQpBpgaDqbSKKiQjr2xZoDfdN1IIBYAf7sxGWZzx4gzXXcNRvDl6Pb29rLkJSyM/8gyX90Fit27d+Ohhx6yem39+vV44oknmEIZHwYTF2M66Dqr6XZ3d6OyshJeXl5sq3/mzBnedVpnjTQuIyE+Ph6RkZFoaGjg9bkikchuBu2qrq27wOXCUtBtHtV6PXv2LPR6PZvuiomJQVhYmNXFOFTGyYVEJIDBNDjg+koEWHdDMm7JicCVDgYy/neiDc9/XYN2jQI+EseZV7hcChBA1W/9u9aZLHj+YK1V5j4nM4xXXVhntOBIlQKrZ8bjua9q0KaxT1kDgGAvIbZcIYHZLBz0AONiJMMlziAUAP+4eYCX+/PPbSMOmO4Su+H7OSqVyq77NuAe3QVgwPH35MmTuP7669lrmzdvRlhYGAwGA5YuXYqnnnoKGzZsGHK9XIyJoOuKpq5Go0FV1YDEXnp6utXghCvNMXvHOmIk9PX1ucR0sLdVGq6f2khhL1PkdqGDg4PR3NyMxsZGREVFQSaTQaPR4MyZM9Dr9fD09Bwy4+RCLhu4+e1RvXy8PHBLTgQA+yWNJz6rsCpfaIyOt+DJ3nocaSGwx2G3t9Z1v0nixYSgTIqhXD3WXJ+CgswwEEJYw0ihUKC+vh4GgwEy2UCd+N78IDx7pNWqxCASAEKhwGW5TS6euDEBv0kLgsVigclkGnF54VzrLqhUKofMBXfoLgDA+++/j9tuu82q0U2zZKlUisWLF+OZZ54Zcq22GLPSjoB1YKRSi2VlZYiLi0Nubu6gSbXh0sAIIejo6MDRo0eh1WoxZcoUxMbGWnEW+X4uJZbb4qFZCS75b7kLAgGQ/sQhzHrxB3xy0jp7UCgUKC4uhl6vZ+ccGhqKxMRE5ObmYtq0aUhLS4Ofnx+CvOzfkELBLzKLcim23paGo2uuhtoBt9ZZ4P7kZJtL9eKj7YDc0/7NHeQlQnt7O7RaLesVzMkMwxNzUhAul0IAx4MMQmDIgDtBJmYPDIFAAB8fH5R2CbD88y7c/akafy0iqDUHwNfXF1NDhVicIUXAL5uGIJkQj0yPwGPXxw17mGJh7sADixAClUrFdilGoxEmk8lKj4IvzrXuQk9Pz7BkHanuAoAhdRN27dqFO++80+q11tZWAAP3/N69e5GRkeHyGsZ0pkupWuXl5VZSi84yY74UMxrQuYyE3NxceHoOdlpwlV5mj9N7Y0YIzBYLHt1Xdk55vPRntar02PDJgG3NjHhfVFVVQSgUsrqtI/0FT09PeHp6Ys31FruZYlqAEG19FrSq9Hj6QCVaWlvh7wko7ST1YXLHtbPnDzrWWrAHgxkQCS3wlAgHrUlvJvjkVDvyg9qg1WohkUjg6+uLvCA/7LsvC97e3vj0VPug8/EQYcgmmqdEiHW/SbJ6zXZUulWlxz++qMPGm1IwJdQX+cGdmJeXjMDAQHz0YyNeOdqKzj4zhhPitt6WhjmZYTCbzaipqYFKpUJ2djYkEgkIIYwyCYD9dyglNnqsu2q6fGqkw9XS5aO7AAwY3jY2NuKaa66xev/vfvc7dHZ2ghCCnJwcvPLKKy6vYUwEXXugGgxarRa+vr5WUouO4AojQa/XQ6VSoaamhhcjYbiDFPQmIIRgTmYoBILBvFh3gtKV7BlK6owW/P3TcoTM9LKSx7QXNGiAphndnMwwfHi8FcfqrDkXpxW/nken1ox/FvXg6mgJvmk0gjul7CkW4sEZ8XCE4XBk+40Ed+SHY/9paysbtd6C7UXdAyyCy8Kw96cmvPhpPTp6WxAoE+K2BBGujJZi2WQf/PukBop+glBfCf58baLDYRFgcKOMwl69W2e0YOuBcrxyYzDy8vLwRYUSm94rtlqnq21VGnBp8zg6OhpJSUl27wuLxQJCiNV/AbBrUSgUWgXi81FeGE7QDQwMxNdffz3o9fz8fBZwASA2NtauOPnBgwdd/pm2GHNB19bJ19vbm7fepVgsRl9fn9NjKCNBrVZDLBbzEttwhUpDgy690OnFTi9wesM+ub9yVEZcLQQIlAmg6LefTmsMBO2eMZjCIZQ7ChrchtQnJ9sGBVx7MFiA0z1CzMuNYEMQQgFwVZQYwdp6FBc3soaTn58fvLy8IBQKh91wOlKlgMxDNKhJRtcPAE/s/9XiqKvfgp0VQFCwL9J9uvDydf4QiUTo7e2FsL8BC1KkeLXUYFVi4NLA6O+CuytwtG5lP0F6erpd0SBXMSNKiGhLO44erYNEIkFW1kDG7gjc0hiFs0Dc19cHgWBgstBZRjwURjvoXggYE0GX8nTb2tpQV1dnJbXY0tLCm0PorAxgy0hIS0vD0aNH3X0qEAqF0Ol00Gq18PDwsNthpU2kSS447drCmXC6o4BL8fzBml+ybueTWNzXXdn+t6r02Ptzm5UmwbfNJsyenIDrUwOZmlhdXR36+vogFApxe6on/lVicNmWvk2ld8hZblXpHT5QXj/Wji8fmGa1FTabzUjp7YXMswlvlnShS2tBoKcAv8v0RpbcgJ6eHrzwXSve5yiJOXtQ+MlEmPXiD8N6mEyQDXCPw+RSPDgjHjn+Rpw9e5aNSJ85c4YJ0NCHGJf6Zw/2ArHBYEBNTQ16e3uRmprKgjG33yESidi1MlQwdkVLNzY2dsjjLkSMiaCr1WpRWloKPz8/5OXlWV04EokEJpNpSNV7wH4jzWKxoKWlZRAjwd2gFyu14z516hTjw9LJHD8/P+tO6giEr0cyxdSmNiD974cRKBNgSqTMajSVC24N1pXtv1AAu4Fu7d4yPELAqRnHAhi4UZObPk6FAAAgAElEQVQ0Gkg8mvH0t50unYuzEC0UOF63ot8yKECJRCJ829CPd39WQaG1sAk3nU6Huz84i44+/r9zsXBABc6W0sYHtIwAgEkuarW+mDZtmlUWSQhhSmCU+mcwGBjjhAZjW5F4iq6uLlRXVyMqKgqpqalWin/0v/ayYsBxnXi0/dEuBIyJoEuNH+1NjdDmGN+ga08jISgoaJBGgrvA3a7RrCApKYl9T6vVQq1WQ6FQoK6ujgVii8WCGyINeEvt9iXxhqKf4PNq+3PIUrEA/3dFFNtl8N3+22tsUdhr6s3JDINYLIZYLEamrw7BXkJ0at1T76YDC44EfQDrUoFcJkav3sQEclpVeqzdW8775wk5dfSBzxhex5Q2yurq6qBUKpGammqX0yoQCODt7c2s3oGBa06v1zNd4paWlkEi8TKZDI2NjbBYLMjJyRnUPOZaJ3HhrDxB12M0GnklNWq1ejzonk8IhUKHY3rD4d7yYSRQ8C1dCAQCZg7JfS+t39qrg3FvivDwcEZNq6mpgbe3N+ZkhuFYexvOKEdGZxDaaZqNFLPivJAgVuL5j87io2ozFLqhfwBtNPEdRHj+YC2uTw1k29uUlBSskfa5rdEY5ueBeQlivP6z3oqVIBYC/QbzoPLOSDUQ3PU3UCqVqKysRHh4OPLz813amQkEAsY4sVUCU6vVaGlpgUKhgFgshqenJ+rr61kw9vHxcfqznNWJzWYzGhsbWfIxlIfdxaqlC4yRoOss6LmiNEYvLD6MBODXphef7RDXU81Rk8wZ1Go1qqqqIJPJrEooe1JTsfHTCvyndPiaBqNBQfupw4Rr0mOws7xnkL+ZLcQC4Mlb0jA3a8BpolfPL3i1qfQoKSmxMgKdkznwN3PGIuADDyFwQ6QRczKjEBUlwbYj9UyEnTs6fKHBRzLgYpudnW1XUGm4oEFRKpXiyiuvZGU7jUYDjUaDhoYG1oT28fGxqhM7q9EKhUL09vairKwM/v7+zDHCWUYMDIzzjme6Fyj4ZLpcRoJEIuFt/0H1F/gGXTr540qw1el0qK6uhsFgQHJyst0Hwd9uSkFejBzr9pU51H4912hV6bH1QPmQAZeiqroKJYYmPH1Yy/scgrxETEyHC9poHG7X30cyMLCxo4zgf2frMS9RjCeniuDtHYSVX/RAdZ4kNYeCCMCSXDmSkpKc7s5cASEEjY2NaGlpQXJyMgICAtj3xGIx/P39rTJOs9ls5aFWWVkJi8XCtHFpMKbGoGfPnkVnZ6fdEohtRgwM3A/PPfccewBcjBgzQdeR0pizgQeun9pwGAl8J9gor7G9vR2BgYHw9PTk1cWlF2RCQoJDF2OK0aaSDQdDsSAoTAR445QJnzYY0Knlt3ZPsRBrrk92+sDjyk7yyXrFAmB6jATfNZvZ2G2X1oJ3y0yIj4/HjFgfdPaV8FqfK3DUiHQFQTIhll0WhssjPVBbWwutVsvcG2ig8/b2dqnUQDPQCRMmWKmuOQNtBHNHdC0WC+tNUOsevV4Pg8EAX19fxMbGwsPDw2mpTigU4vjx41i1ahVuvvlm1NXVjUqP5VzgojempDAYDHaDbnNzM4xGoxW9hOq8NjQ0IDIyEjExMexi/OGHH3D55ZcP+hx7OHXqFKKjox3OgNP6lMVigUajQUdHB9RqNdMkoIwEuVzO5BKpzGRDQwOioqIQGRk5IrZExt8PXRROFEPBRwJIhATdegECPQW4O0eOOZlhjKs71I4h/YlDTi9mHwnw4NVReLOk026ApsI/fChcUiGg55lce0qEyIny48VhtgcPIfDo7Im4fergwRGj0ci2/xqNZoBLLBRabf99fHwGBVOLxYK6ujooFAqHTbjhwmKxoLa2FkqlEvHx8TCbzaxpp9frIZVKrR4SXl5eMBqN2Lp1Kw4fPoxXX30VWVlZblvPKGLsGlNSOMt0qTspX0YC3+aYs0kz2yYZV5CZWlmr1Wr09PSgoaEBBoOBrVUulyM7O9stGp4L8yLcomFLnQlCvMX4vysjsfHA2XP6RL5qoje23p4HkUhk5fzb0dHBRnVthya4f0NH7AkfCbBrYQxiY2MhEonw5NdNdn9+q0rPmzPr7+3hVGGMi1uzw3CkSsHrWOAXQSACqHRmBHuL8OfZSbg5K9zusRKJhDkyUJjNZhaEm5qamN409SgTCARobm5GWFiYy024oaBSqVBeXo7Q0FAUFBSwv09o6IAhJmVO0PV9//33eOyxx6DT6ZCQkIClS5detHVcLsZM0HUE2kjjy0igJQM+dVpHSmNDNckEAgFkMhlkMhlCQ0PR19eHiooBClRsbCx0Oh3OnDkDo9EIb29vK56uq/J5f5keDUVnF748yy8IOEKYnxQfLk6HWq2GWq1GgKeAFyOBQoCBwKfqNw1LF/h456+kea7zL/ALbevrWrSpmxHkLcLCFE/kBpqZZoKfnx+WXxGJjZ/XwWzzYNaZgDKtNxJ++Wxn1Da+jTm+ARcAvj7Tji6eJRUAeGtOMLRaLdLS0ob1UBaJRJgwYYJV8LJYLGyknQ7ltLW1QaPRWDXEhrudpzoParUaGRkZDifhuMwJPz8/7Ny5ExEREdiyZQsMBgN++uknlJWVISYmZljruFAwZoKuo8zUaDSybIgPI4EGUlcYCQC/YGsLg8GA2tpaaDQaJCUlDXqKU9k/mtFRnyhvb2/I5XJ2Q9irtZlMJtTV1aG7uxt/m5uG2c2GETXa2tR6Vqszm82YeqIPn1U5H5mm4GryDneKztGQgm2zrLPPjDdP9mPinBRclxLAMuL2ts5BARcYqCev3TsgUD4nMwyrZ8aPqraFLTq1ZqZ3MRR8JIBcLrcaRHAHlEolqqqqEBMTg4iICEZv5NZhuR5l3DqxrYuILbq7u1FRUYHIyEiHOg+2KC0txYMPPogFCxbgyJEj7F6cNWuWW873fGPMBF1bUCffnp4eeHp68mYkuMrrNRqNMJvNLgVbi8XCOsJcupMtqOyfj48PIiIi2HtpIOZ2h7k3At06RkdHIzExcYBK9csOc9iNtl8kHkN8xAiQmFDWze9tIgBzJxKUlZXBz89v2Jxg7nTbE59V2DWnpODqPtCM+L97B4uXUFgI8NjH5eju6cG83GhsvCkFLxwaWhvBXeD7+1hxeZhbuakGgwEVFRWwWCzIzc21YgPQ2i9X/pQ7wWZP+5frImI2m1FdXQ2tVsubvqbT6bB582YcPXoU7777LtLT0912rhcSxlzQtWUkJCYmMpM5PuAr70inxzo7OxkdZqjuLh1uqKurQ2hoKKZMmeKyMpNQKGTbPSrkY7FY0Nvbi9bWVtTU1EAgEEAqlUKtHhhX8/Pzg4+Pj5Xw9ycn2xxa0Nhf+8B/23tNcG5wYg0zgP/VCxAeLsVl3uZhBVyuu7CtULkj0MyY0gHbhnBX1psJNh9qxeZDA3qpPhLg+gQZfmx3zU16JHDGYrh1kj+uiPJARUUFmxDj1rBlMhnv7JfqlNTX1yMhIcGpkLf1+uxPsNH+hEqlQmNjI7RaLQwGA/z9/dk1OlSfpLi4GA899BBuv/12HD582C0uFBcqxsyZUT4hZSRQjQRCCO/MFeCX6dImWWBgIIxGI5qbm9Hb2wuBQMBuAj8/PyuKjkqlQlVVFbMHcifHUK/Xo66uDgAwZcoUZlNNt9YNDQ2sc03XNj3OD7fnRYxoqMIZuOO8rWo9njrYyETAXckchQIwhS5XhMrD5FKmNhcXF4dweY9LP7fXCOyv6ed9vFtAgDMbZuD9ojq8dPgslDqCMD8pVs8aLAdJB3nUajUTXOfWsB2xOvr7+1FeXg6pVIr8/PwR0664/YmAgABUVlZCIBAgMzOTrbG1tXXQKDEN3gaDAZs2bUJxcTH+/e9/Iy0tbUTruRgwZoJuZ2cn+vv7BzESXK19OePe2tZtxWIxoqOj2fe5ga6+vn6Q3F1iYuKQfFtXwK3bJiYmWnWp7TVM6AQRVei6PrAPE7Kk+LDaxJsfyweOBGueP1iLa5ICeQdODyGwOEOKJA8Vdn7bg+e/5Z9jp/mZ0NfXx4YnVs8cXR1idyDMT4qKigrECzX4/P/yBzmbcOHh4YGgoCCmGgbAitXR2dnJuLp0p6PVatHV1YWUlBSra8UdoKyg2NhYhIWFsWs8ODjY7vqOHj2KDRs2QKvVIjk5GYsXL3YqNTmWMGaCblhYGOtmjwTDZSQA1oHOZDKhvr4eXV1dzFeppaUF1dXVbGtIObpSqdSlQMzl8nLrtnzOzXaCKM9oxO9/yZg+O92JN37ug36E8ddRCaFVpcf7LmTW83Ij8H+z46HRaPDmJ6cGWZE7ww+tBL8xyJHyyzb1Qhwe4UIqFuCmmIEmaXJy8rAezLasDuDXRnJtbS2TWKypqUFHRwfLil0dmuCC1oUJIcjLy3PaWKPr8/LyQmlpKaKjo/H0008zlcDy8vKLnpnAB2Mm6A4FVzR1DQYDe4+rjARqTkmNGqdMmTLogqYqTlRARKfTORyWsEV3dzeqqqowYcIEt2wPJRIJu1HvDQvDldGV+LahH2+e0LmsTcsHrtR09/7chtzogSGIzj7XAqU9V98Pj7dekAHX10OARZleuO/abLeWnSwWCxoaGqBQKJCTk8OYO9wdz9mzZ1npiW79aWY8VCBub29HbW0t4uPjGdd2KBw9ehRr1qzBH/7wBzzzzDOspzF9+vQRnevFhDEzkUYIYcHSFoWFhcjLy+NVnO/o6IBKpUJCQgIT3OATbIEBo8bq6moEBAQgLi6OdzOA24ygXwaDAV5eXowaJpFIUFs7IASelJTklsEJCirQ3tXVhcTERAQGBlpJFp7Pi8CVSTB7CPYSYem0UFQoTdhzvGMUVjh8BEiBm2OBW3IiEBkZyVww3IGenh5UVFQgNDTUauLSEbilMbVabdWj4AZikUgEvV6P8vJyiEQipKSk8Hrwa7VaPPHEEzhx4gRee+01JCcnu+U8L2A4DBiXRNAtLS1Feno6LxEQqlubkJDA62kPDMyoV1VVMS1cd6g7UXpOd3c3mpqaGGl9woQJLCPmw5gY6me0t7ejrq4OkZGRiIqKsnu+fBkDowEBgNMbZoxoDVKRYFSy9uHit1lBuDlSB19fXwQFBTGBmL6+Piu9BK4dEV+YTCZUV1ejr69v2AMUFFS8hhuIDQYDTCYTQkNDERERMeQ1SAhh2e3ixYtx//33u8VL7SLApR10T5w4gbi4OKeDEbSUYDKZ0NTUBLVazRoRNNu0peYMNdww0vPh1m0jIyOthiVUKhV6e3tBCLG6Sfk+KDQaDSorK+Hl5YWEhIQhSe5DcWPHMRgioQALcsNxpEoxwPn1k+LOdG9kyfUONQ2oXgLVI7ANxM6Ea2gzizvk4C7odDqUlZXBw8MDYWFh6O/vZ4HYYrEMknOkfoMbN27EmTNn8NprryExMdFt67kIMPaDLgCHTr5nzpxBeHi4XWL5UHVbo9HIgpxarUZ/fz+kUikIIejv78fEiRMRFRXl1gucW7eNi4tzun3jZiM0EHOpYbRRwn1Q1NTUoK+vDykpKUNO6DkDNxALBYBIAFzA5IBzCi8PER6/KZnVlKmweEREBKKjo126XkwmEwvCarV6UCD29PREU9OAZkRKSopb68K0R9HU1ISkpCS7zWrKE6fr+/jjj/Hvf/8bOp0OU6dOxYoVKzB16lS3sBPuvfdefPLJJwgJCcGpU6fsrnfVqlX47LPP4OXlhR07diA3NxcA8M477+Af//gHAOCvf/0rFi1aNOL1OMGlEXQdKY1VVVVBLpdbkcCH2ySjzQM/Pz94eHhAo9EMqr8ORyMBGOBQVlVVwWKxICkpadgXKb1J6RfN2AUCAbRaLWJjYx2WEoYLjUaDtw+exKs/n98a8LlAqI8EEwNkKGoY7JV0R34ENtyYwv5tMBhQWVkJo9GI1NRUtwmL079xU1MTFAoFJBLJoIGJkbASgIE6bFlZGXx8fJCYmMirLNDb24u//e1vqKiowEMPPYSuri78+OOPuOuuuzBt2rRhr4Xim2++gY+PD+6++267Qfezzz7DSy+9hM8++wyFhYVYtWoVCgsLoVQqkZ+fj5KSEggEAuTl5aG0tHQ03SfGvsqYM3BpYFy5RVeaZHS4wdvbG/n5+Vbbca6XWUdHB6qrq9mWi6uR4OgGoPQyhULBGlkjPV+uupRSqURFRQV8fHzg7+8PpVKJ5uZmJqPHpa65CqPRyOxyFs/MRHx8nxUta4JMjN+kh+BIlWLUx2nPBUJ9xHhtbig0Gg3esAjwTTNhmf5vc0JZwCWEoLW1FWfPnkV8fDxCQkLcuhsyGo2or6+HTCYb5OTA5YlTVoK9gR1H4AqXp6am8iqbEULw7bffYu3atVi6dCm2b9/Ofs4999zjjlMGAFx99dWor693+P19+/bh7rvvhkAgwLRp09DT04PW1lYcPnwYs2fPZvfE7NmzsX//ftx5551uWxtfjKmg60jekV6QQ3mS2UN/fz+qq6thMpmQmppql7Bu62UG/LrlotmIRqNhNwANxF5eXmhra8PZs2cRHR2NgoICt2af/f39qKysBADk5OQMyrK4jImmpibo9Xpmyc1lTdgDlxpna5djOz1F8cnJNqz/uBzGC6ip5Qo8JUL8eXYSkpIGzm/75MGsk6NHj8LDwwP9/f3w9vZGVlaWW0n/3ICYkpJilanZ42E7o4fZC8R9fX0oKyuDXC7nLVyu0WiwYcMG1NbWYu/evefVGr25udlqYCkqKgrNzc0OXz8fGFNB1xFEIhGjYQmFQl7ZLZ32UiqVw8o+uXXVqKgo9pkajYbpiqpUKkgkEoSEhEAikTBx85FmRNQJVqFQICkpyeH0ka0BIa1T27oPU3lJuVwOX19fJkXp7+9v1y7HEWgw5hpPUr2BcLkU1yQF4vNT7VDpRsallYgGHr7DUVSjgjx0PawJxmzff32g2Ep0UvHvjo4OREREwGQyoaysjDk4c+vsw+FXc73E+AZEVwIxMNAXSUhIQFhYGK+M+MiRI3j00UexfPly/Otf/3Jr0jBWMaaDLi0j+Pr6oqOjA6WlpVaC4nK5fNB8usViQUtLCxobGxEdHY0pU6a4bVtIHVQbGxshFosxbdo0SCQS1gRzdVDC3vlSClhUVJTLmbNAIICXlxe8vLysBE0oY6KlpQWdnZ2wWCwICAiAp6cn+vr6nJZObMEV3bFFZ2cnrgvoRlm/D/59Qm1VjhAKgCmxE3BW2c+CIA2KrSo9C5ZBMiFuTRBCLBLhv1UmJp1oIQOc3WBPC8q7fy0JLMyzrsEOF5QXGxISgqlTpw5yfablJ9uHGTfjdBSIuW4LfORJh4JtINZoNDhz5gx8fHwQEhICpVKJhoYGpxmxRqPBX//6VzQ0NODjjz/GxIkTR7QmdyEyMhKNjY3s301NTYiMjERkZCQOHz5s9fr5GsgYU400k8nEygeOmmTcbJN2gj08PCCXyyEQCNDR0YHg4GDExsa6VemIb93WdlBCpVLxEjNXq9WorKyEt7c3LwqYK7BYLGhqakJzczMSEhIQGBg4iLrmTOxnKPT396OiogJCoRDJycluMVXkCsJ0dnait7cXMpkMwcHBrHTi6vi1PRiNRlRXV6O/vx+pqam8ebHcQEy/TCaT1d+Z7irKy8sRHh6OmJgYt9aFubY89oK5vYGJ7du3Q6vV4tSpU7jnnnuwfv16t5lgAsD+/fuxatUqmM1mLFmyBGvXrrX6/urVq3HgwAHU1dUhOjoaHR0d6OkZsDoSiUSYOHEiurq6cM0112D9+vV44IEHUFRUBKVSiby8PPz4448AgNzcXJSWlrpdg4KDS4O9YDQaYTKZXG6SUTqPxWJh9V+6HaQ36HADMLehMlzPM262Sb9oBu/t7Y2enh6YTKYRU8Dsobu7G5WVlQgMDERcXJzDLS33BlWpVIzWxH1Q2NtVnD17Fu3t7Q7pSCOBVqtFRUUFPDw8kJSUBEKI1e/QdldBAzEfUJnO2traQSIvw4UtD7ujowNmsxn+/v4ICAhggdgdyYBarUZZWRlCQkIwceJEXtekWq3GunXr0NLSgssuuwy1tbUoLy/Ht99+65aHvNlsRnJyMr788ku2U9u1axcmTZrEjrnzzjtx+PBhdHV1wdvbGxkZGfj9738PAPjLX/4CjUaDFStWYP/+/fDy8sLbb7+N/Px8AMBbb72FTZs2AQDWr1+PxYsXj3jNTnBpBN01a9bAx8cH+fn5yMvLY55PjsDlrCYlJTGDSVrbVKlULCPmCoXL5XJeQwjUIkgulyM+Pt6t7qUmkwm1tbVoa2uDl5cXs3d3VjpxBTqdDlVVVSyYD2eyiXKcudQ1Smuiu4qwsDDExsa6tRZIg3lHRweSk5Md0oKoJxd3jdxmIv2yDShUHpEGc3fuKoBfhxwmTpyI0NBQVmfnPnCpp5mrgdhsNqO2thY9PT2YNGkSryYfIQQHDx7E+vXrsWrVKixevHhUardHjx7F448/jgMHDgAANm/eDAB49NFH7R5/+eWXY+PGjZg9ezaAAZ836vl2AeDSCLoVFRU4duwYCgsL8eOPP8JgMCAjIwN5eXkoKChAeno6JBIJ+vr60NjYiJ6eHsTFxfGi81A2Ag3CGo3GKpOTy+VsWo3ybemT292SdVTjISgoiBkqAoP5ubR04oqiGRVJaWtrQ0JCgpU0nzugVqtRUVEBg8EAqVQKo9E47Bq2PdBdS2hoKO8MjgtuM5F+cRthOp0OPT09oyKPaDAYUF5eDsD5kAPXPYRei9ypMEcj4j09PSgvL3dpQEOlUmHdunXo6OjAK6+8YsUAcDf27NmD/fv344033gAA7Ny5E4WFhXj55ZcHHXv27FlMmzYNTU1N7DzFYjFycnIgFouxdu1a3HrrraO2Vh64NHi6KSkpSElJYZMmOp0Ox48fx7Fjx7B9+3acOnUKOp0O/f39WLJkCebNm4fg4GBeFx83i6SgQY5uBbVaLcxmM8xmM6KjoxEVFeXWLEir1aKyshJCoRBZWVmDKGC2/Fxg4EamDwpuo447yEHXqFAoUFVVxVwt3J19NjY2orW1lekKA46dkbnlHT6miNwhBL72MPbgqJnY0dGBqqoqSCQSiEQiVj/nBjl3lKASExOHfNA5cg/p6+uDSqUaZOPk4+MDlUoFg8GArKwsXrsWQgi+/PJLbNiwAQ899BDuvvvuC4qZsHv3bsyfP9/qwXL27FlERkaitrYWM2fORGZmJhISEs7jKu1jTGW6zmA2m/Gb3/wGsbGxmDVrFsrLy1FcXMxqrQUFBcjLy0N+fj78/f1d1relN01oaCi8vb1Zs85gMAyiXLl6c/KlgPFdq06nY4GYUulMJhMkEgni4+MRFBTkVlESWhe2zcydrdG2yeTIkJPLFx6NIQSTyYSamhpoNBornjbXuFGlUvHONm3R39+PsrIyyGQyJCUlubV5a7FY0NzcjLq6OshkMtZY5q6RKodx0dPTg0cffRRKpRKvvPIKC+yjDVfKC5MnT8b27dtx+eWX2/2se+65B3PmzMH8+fNHb8HOcWmUF4aCQqEY1KyxWCyor69HYWEhCgsLUVJSAo1Gg7S0NBaEs7OzHXZoe3p6UFVVBT8/P7t1WxpAuEGOEMKm1eRyucNOP9fLarhNOGcwm82s9km3jXS7yl2jK0I6XOj1elRVVcFoNA67LkzB3VJTxoTJZGLMjoSEBMjlcrf+fmhtlQoO8S1Bcbf9AOwKEhFC0NDQgNbW1kFDDu6AyWRCVVUVdDod0tLS2PVrTzkMGNhFlZSUwMvLC++88w4efvhh/P73v3fr73MoZsKbb76JpUuXIjk5GRKJBN3d3di/fz/S09OtdBPuvfdevPrqq6irq2N/k+7ubnh5eUEqlaKrqwuXXXYZ9u3bZ9WEO8cYD7quwGg04uTJkywQnzhxAmKxGLm5ucjNzUV+fj4EAgGOHj2K7Oxsl+u2FovFirbW29vL6sM0EBsMBlRVVcHHxwcJCQlubcIBYLba4eHhiI6OHnRz0ZvTdo22E3X2AhEhBE1NTWhqamJ1YXdmn2azmTk9R0dHw2Qy8RL74QuqFysQCEYsIGMb5OgDzWAwsAe1KzxnPujq6kJVVRUmTpyI8PDwIc/fbDbj1KlT+Mc//oGamhrIZDJIpVLcf//9+MMf/uCWNfFhJuzYsQN79uxBZWUlzGYz7r33Xqxfvx5r1qzBu+++i7KyMggEAsTHx2PRokV44YUX2Ht/+OEHLFu2DEKhEBaLBQ8++CDuu+8+t6x9mLg0arrugkQiYQF2+fLlIIRAo9GgpKQE3377Le666y4olUrk5uaivr4e+fn5yM/PR2hoKO/6MA2uFLTTr1QqWXZIa5kqlcpuF304oDQqiUTi1CCTSlrarpE+LGgNmzbqaCDW6XSorKyEv7//sNyOh0JHRwdqamoQHR2NpKQk9vummTqXh11bWwutVguxWGwViB0553IfFklJSVb+Y8MF9/dIy0R0ytFkMqGhoWGQathwmSdGoxEVFRUwm82DLNUdgRCCAwcOYOPGjXjkkUdw1113QSgUQqvVupUJUFRUhMTERMTHD7g633HHHXYz0djYWHzyySdWr+Xm5kKtVrOy2sKFCzF16lSrYy6//HKcPHnSbesdTYwHXR6gU2wzZ87EkSNHsHLlSixduhQdHR0sG/7Xv/6Frq4uJCUlMcpabm4u75tHJBKht7cXXV1dSE5ORnBwMGuCdXd3o76+nm2l6U3siog594ZPTk4elvavRCIZ1KijlCulUony8nKYTCbI5XKIRCL09PQMe+TVFnSAgu44HAUUe2Ov3EGJtrY2Js/JDcR0ZFcul4/Kw6K7uxsVFRUIDw9HQUHBoGuCq6NbU1Pj0sMC+PVhFBcXx/vhr1Qq8cgjj6C/vx9ffPEF0w0BwJqJ7oI97YPCwsJBx/33v//FN998g+TkZDz//POIjo6+oHQT3IHxoOsiNm7cyP4/MkKBVowAAB4aSURBVDIS8+bNw7x58wAMBLaysjIUFhbio48+wmOPPQaz2YysrCyWDaelpQ1qllDWQHBwsNUNT7URqP8UlzxPO9RUxJxbe+XecFwSP93WuXOr7+HhAb1ez4J5SEgIYyN0dXWhtraWNcG4zUS+QY1LYUtOTh5WE9Gecy6XMVFZWQm9Xg+5XA6xWOzWhwWtrfb39ztlVdh7oHHdc6nNOpcC6OfnB6FQiIqKCiZXyGc3RAjBp59+ir///e9Yt24d7rjjDrdeE8PF3Llzceedd0IqleLVV1/FokWLcPDgwfO9LLdjPOi6ESKRCBkZGcjIyMB9993HOJ+lpaUoKirCs88+i/LycsjlcuTl5WHixIn47LPPsHbtWrsqYLYQCATw8fGBj48PIiIiANi3facZklQqRXt7O7y9vXnfkK6Acm6pIhV9mNhSrrhNsJaWlkENJkfNRKpnQB9G7qx7Ut0IpVKJmJgYREVFsUDsTOzHFXYBd8ghNTXV5cBmz92XO8xRV1eHvr4+eHt7IygoiJWhnJUVFAoF1qxZA5PJhK+++oq3oeRI4UgTgQvueS5ZsgQPP/wwe++FopvgDow30s4xCCGor6/HI488gh9++AEZGRloampCbGwsy4Zzc3OZFsRwQLfiGo0Gnp6eMJlMjJs7lGQjH3A1dB3JXQ4Fe3P9tJno7e0NpVIJo9E4Yp8ve6CcXirX6YiZ4mj82pbVYZu16/V6ln2mpKS4/WGn0+lQXl4OiUSCpKQkWCwWp1N1AoEAAQEB+Pjjj7Fp0yasX78et99++znNbk0mE5KTk/H1118jMjISBQUFeO+995Cens6OaW1tZSWOjz76CE899RSOHTt2PnQT3IHxRtqFApqtzpo1C7t27YJIJILFYkF1dTUKCwtx4MABbNq0CVqtFunp6SwQZ2RkDNkY4fKFY2JikJ2dzTSGbbM4Kq5iy3vl+/kTJ05kGrrDgUgkwoQJE6xqywaDAfX19aipqYGnpycsFgvKy8tHLLRub/18OL32dha2Osm02USzdoPBgLa2NiQlJbl9mo/rm5ecnGyVGdpKdNK/d3d3N1asWMHoVffcc4/bucxDUcGee+45vPHGG7BYLEhPT0dwcDCWLl2K9PR0CAQCxMbGQi6XswlKOuSzY8cOAEBAQAAee+wxFBQUAAA2bNhwoQdcpxjPdC9QGAwGHD9+HIWFhSgqKsKpU6fg6emJyZMns0AcHx/Pttzd3d2oqanhTTGz5b1qNBqmFEYDMZdu1dvbi/Ly8lGjsPX29qKiooJxbunn2yqucafVXNGmpWpdtp/vDpjNZkbBAwbYKbZNsJHoYADDG6IghGDv3r3YsmUL1q9fj8mTJ6O0tBTV1dXYsGHDsNfCBR8q2KFDhzB16lR4eXnhX//6Fw4fPoz//Oc/AC44vQR3Ypyne7GDEIKenh4UFxejsLAQxcXFqK2tRUhICIxGI/z9/bF161aXTQ+5oFt+KvRDvdWoS3JycjKCgoLczrmlrIqUlBQripo9OJpWczQJRodfOjs7kZKS4lbHZvr5tNHHtbZxJvZDv/gI1lMaW3Nzs0uNxI6ODvz5z3+GRCLBSy+95Pasm8JVkZqffvoJK1aswPfffw9gPOjaw3jQvYDx0UcfYd26dbj22mshkUhQUlLCxFioyA/tmLsaKLkmnAEBAUxsXafTQSaTWWk3DDdr7OrqQnV19bAccrng6g5wJ8GkUik0Gg2Cg4ORlJTkdhqYRqNBWVkZk70cqtHH1cGwJy1pK/aj1Wpx5swZ+Pn5ISEhgdf6CSH48MMPsXXrVjz++OOYN2/eqNZuXRGpAYAVK1YgLCwMf/3rXwFccCI17sR4TXcsIjMzE0VFRVYauiaTCadPn8axY8ewa9cuPPLIIxAIBMjJyWFjzSkpKU5vYGrH4+npadeEk2o3cClhfE04AbABCmDAu22kIthcARjg1yGBvr4+hIeHo7+/H8XFxVZTf0PxXp2BZufd3d2YNGkS70aih4cHgoODWdbpTOyHPkhSU1N5D2m0t7fjz3/+M2QyGQ4dOuSW4Q534t///jdKSkpw5MgR9trFIlLjTly0me4HH3yAxx9/HGVlZSgqKmJCxbZwVOSvq6vDHXfcAYVCgby8POzcudPtXeYLAYQQ9Pb2orS0lJUlqCh5Xl4e8vLyMGXKFISFhUGj0eD06dMQiUS8tvoU3EyT6yRhq+1Lt8otLS1WSmPuPFdqV2RPWJxu+Wm2SYckuFn7UI06OuQw0uzcEejfgHK0NRqNQ7EfCovFgv/+97945pln8MQTT+DWW289Z8wEvuWFr776CitXrsSRI0dYw88WF4BIjTsx9soLZWVlEAqFWLZsGZ555hm7QddZkX/hwoWYN28e7rjjDvzpT39CdnY2li9ffh7O5NyDCukUFRXh2LFjKCoqYvq/t9xyC+bOnYvc3NxBgxauwNYWSaPRMMW1mJgY+Pv7j4iJYIvhCItzRczpOmmmyQ3EYrEYRqMRVVVV0Ov1SE1NHbZ0pCNQ4fXOzk6kpqZaSYjaa3oCwJdffgmLxYLCwkKEh4fjpZdecrv7xlDMhL6+PoSGhiIwMBAhISHo7e3Fnj17kJ6ejs2bN+PNN9+E0WiE0WjEkSNHkJSUxN57AYrUuBNjL+hSTJ8+3WHQdfQUXrt2LYKDg9HW1gaxWDzouEsNK1euhFqtxpIlS1BdXY2ioiL89NNPMBgMyMzMZPXhSZMmuVy/pR5iWq0W8fHxTJyG8klHaovkjok1LriNOm4gNhqNCA4ORlRUFJsEcxdobZhKX/L5bKPRiJdeegmffvopZDIZVCoVpFIpPvzwQzaUMlLwYSb885//xCeffILq6mqoVCqEhITg5MmTWL58OT7//HNUVFRg5syZKCwsREZGBgAgJiYGH3/88YUoUuNOXJo1XUfz3gqFAhMmTGA3+MU+yz1SbN68mdUlr7rqKuYdpdPp8NNPP+HYsWN46aWXcPr0afj4+LDacH5+PmJiYhzKUtKtvu1EFremSW2RKOXKbDbztkVSqVSoqKhAYGCg2ybWBAIBvL294e3tjYCAAJSXl8PT0xORkZHo7+9Hc3MzysvLWR3ZHr2OL6jLr6u14ba2NqxatQoBAQH49NNP2YOGmm+6C3xEavbt24fHH38cl112GUwmE8LCwkAIQUxMDJYtWwapVIrvv/8e119/PTuO4mISqXEnLuige+2116KtrW3Q608++SRuueWWEX++UqnE7bffjurqanR2dqK7u3uQrumhQ4ewevVq9u/y8nLs3r0bt956K+655x4cOXKE1T537NiBnJycEa/rXMPRze7p6YnLLruM3SiEECgUChQXF+PYsWPYvXs3GhoaEBMTw0R+8vLy0NzcjMrKSqSlpSE/P99hdsx1aaCTSFxbpMbGRru2SBKJBLW1tdBoNC4FK77gDiHYqo1FRUUBGMgCKQuhtrYWfX19kEgkVlm7M0qYSqVCeXk5QkNDmVToULBYLNi9eze2bduGTZs24aabbrJ6n7t/D3xEarjHiMViyOVyKBQKNDc3Y9q0aVbvvZQTGy4u6KD71Vdfjej9jua9AwMD0dPTg02bNmHWrFl44oknsGjRImzZsgVPPfWU1WfMmDEDx48fBwAmyXfdddex7z/99NNjpfA/JAQCAYKCgnDDDTfghhtuAPCrjXdhYSG++OILrFy5EiaTCVdddRXa29vR29uLrKws3gyFoWyRaCCWyWQICQlBf38/JBKJ2+rDWq0WZWVl8Pb2ttKTsIVIJHKqZsa1RuI+MEQiEWpqaqBWq5GRkcFbh7m1tRWrVq1CcHAwjhw54nbR83GcO1zQQXekKCgoQFVVFerq6hAZGYndu3fjvffeg0AgwIwZM/Dee++htLQUGzduxOLFi7Fjx45BQZeLPXv24IYbbnC7FsDFDKFQiISEBCQkJOCbb77BqlWr8Kc//Qnl5eUoLCzE22+/jZMnTzL9XlofTkxM5F0OEIvF8PLyQmNjI2QyGbKzs5mdukqlYlSrkYjT0Npwe3v7sIcobNXMbClh1Hna29sbYWFh0Ov1kEqlTtdpsVjw3nvv4eWXX8bmzZtx4403njNmAh+RGnpMVFQUq9cHBgbyeu+liou2kfbRRx9h5cqV6OzsxIQJE5CTk4MDBw6gpaUFS5YswWeffQYA+Oyzz/Dggw9aKdEDQG1tLZKTkxEbG4vJkydj586dCAsLQ09Pj8OfOXPmTDz00EOYM2cOgAGKy9GjRyGVSjFr1ixs2bLFrR35iw2EEIfi4Gq1GiUlJWysuaamBqGhoVb1YXuaAHyFxUdii+TqkIOrMJlMrJmYmpo6SKCGGkja+pa1tLTggQceQHh4OJ599lm3T9NR0DJbfX09YmNj8f7778Pf399KpKarqwvTp09HWFgYvL29mWjO9u3bsW3bNhgMBlgsFqhUKhw+fBgSiQR33XUXioqK0NLSglmzZqGqqsrtAyoXMMYue2EoOKsLL1q0yCrI+vv7o7u72+7ntLa2IisrCy0tLaxG2drairCwMLS1tSEvLw96vR65ubnsorWFSCRCZmYmgF87uMClwxnmghpKFhYW4tixYyguLoZCoUBycjKrDxNCUFhYiJtvvpn3RBYX1BaJ66vGHZDw8fFBS0sLVCoV0tLS3F4TBQakFCsrKxETE4OIiAi7DyXuOtVqNTZv3ozTp0+jp6cHf/jDH3DfffchJSVl1Nx4H374YQQEBGDt2rXYsmULuru72Y6PJi06nQ7z58/Hc889h9WrV2PHjh2oq6uDp6cnUlNTodPpEBMTg927d7PG25NPPom33noLYrEYL7zwAitJXSK4dIOuM6SkpODw4cMIDw9Ha2srpk+fjoqKCrvHvvjiizh9+jRee+21Qd97+OGH0dPTg5aWFlx55ZVWFy0XjubML2XOMBdmsxlnzpzBt99+i9dffx2tra1ISkpibhz5+flITU0dkWMuHZBoa2tDR0cHRCKRVTbsLlsko9GIyspKGAwGK2PIodDU1IQHHngAkZGR+O1vf4szZ86guLgYTz31FGJjY0e8Lntw5T6gyM7Oxp49e5CUlDTWhhrchfGgaw9r1qxBYGAge8IrlUps3brV7rHTpk3D5s2bMWPGDPYa1f9MSUnB1VdfjcDAQKxatcrhRWsv6BJCxjnDNnj55ZdBCMHy5cuh1+uZCHxhYSEqKirg7+/PmBIFBQW8nHopbIccPD09odfrWVlCpVLBZDKxAQlXnS6AX8XL7U3FOYLFYsG7776LV199FU8//TRmz559zmq3EyZMYDs+Qgj8/f2dltmKioqwaNEinD59GkKhcLzMZh/jQdceFAoFFi5ciIaGBkycOBHvv/8+AgICUFJSgldeeYWJeNTX1+OKK65AY2Oj1RZv5syZ6OzsxJkzZ3DnnXfilVdegbe3t8OL1p64R1dXF6ZNm4bq6moAQGNjI2644QacOnXq3PwSLjIQQtDZ2cm86WjNMC4uzkoEnop3c0F9xIYKhlzxcjoBNpQtEjDAXqioqAAhxCUX4cbGRqxcuRLx8fHYunWrFXPDXXBnmW369Ol45513GCWMltkMBgOWLl2KhIQEt0lHXsQYD7ojhTsu2ubmZitxj6+//hpyuZwFXaVSiVtuuQVFRUW4+uqr7daGjx8/juXLl0OtVkMkErGGBoAxwxt2FVQEno40l5aWQqfTMRF4+kB94IEHkJaWNqzyAdfpQqVSWdkiyeVyGAwGNDY2IiEhgbcFjsViwY4dO/D666/j2WefxaxZs86LVxnf8oJarcb06dOxbt06h6WEw4cP45lnnhnk6HsJYjzojiaGUxOjdbDf/va3rLywbt06aDQa1NbWYsaMGXZrw5WVlRAIBEhKSkJLSwvy8vJQVlaGCRMmjNfWONDr9fjpp5/w4osv4ssvv0RaWhpMJhNyc3NZRjxSpoLRaERXVxdz4hCLxUz2cihbpIaGBqxYsQLJycnYunXrqDTx+IJPmc1gMOCGG27A3Llz8eCDD1p9j5bZCCFYvXo1PD09sWXLlnN5ChciLs0x4HOFm2++Ge+88w7Wrl2Ld955x+60nK24x/fff4+HH36YcYb37NmDffv2YcqUKbjllltw2223Yfr06YOCbnJyMvv/iIgIhISEMNrcOH6FVCpFREQEwsPDUV9fD29vb/T09LDa8Icffsj42zQI5+XlITAwkFe2ScscZ8+eZdY8Q9kitbe3IzU1Fbt378bbb7+NZ599FjNnzhy17NYRFcwWzz77LLy9vfG3v/0Nfn5+LGHYt28f/vjHP8LPzw9BQUEoLS2FQqFgNjp0J/W73/0OnZ2dIIQgJycHr7zyyqicz5gBIcTZ1zh4oKuri8ycOZMkJiaSWbNmEYVCQQghpLi4mNx3332EEEK+//57kpGRQbKyskhGRgZ544032PtrampIQUEBEQqFZP78+USn0xGLxULkcrnTn1tYWEhSU1OJ2WwmhBCyaNEikpycTDIzM8mDDz5IdDrdKJ3x2IDZbCZ1dXVk165dZPXq1eTKK68kmZmZZMGCBWTLli3k66+/Jl1dXaSvr8/qS6FQkO+++44UFxeTnp6eQd/nfmk0GtLW1kYqKyvJggULSExMDAkNDSVLliwhb7/9NjEYDKN2fmvWrCGbN28mhBCyefNm8vDDD9s9ztvb2+7rCxYsILt27SKEELJs2TLyz3/+c3QWOjbhMK6OlxfOMUarobF//37cf//9zHywqqrKqqGh1+tx9913o7S0FIGBgfjPf/7DKEhUgk8kEmHbtm24/vrr3X/iFwmMRiMTgS8uLsbx48chFAoxefJk5ObmoqamBh4eHvjTn/7EW0bRbDbjzTffxI4dO/DCCy+goKAAx48fR0lJCVauXDlq/Fu+Za9xVs2owPH2xVlEPg9Ph0saycnJpKWlhRBCSEtLC0lOTrZ7nEqlIpMnTyYffPABIYQQk8lE4uPjSU1NDdHr9SQrK4u8/fbb5KabbmLv2b59O1m2bBkhhJBdu3aRhQsXEkIIOX36NMnKyiI6nY7U1taS+Ph4YjKZRvM0LypYLBaiVqvJe++9RxITE0l2djbJzs4mM2bMIH/5y1/I7t27SU1NDent7bWb6Z46dYrMmDGDPPDAA6S3t/ecrp27U3K2cxKJRCQvL49MnTqVfPTRR4QQQjo7O0lCQgI7pqGhgaSnp4/ugscWHMbV8ZruBQQ+tWGDwYDbbrsNd999N2uYUQk+mUwGDw8P3H777Xj99ddx1VVXsfdRCT4AmD9/PlasWAFCCPbt24c77rgDUqkUcXFxSExMRFFRkZUE36UM6pCsUqmwY8cOXHHFFczKnYrAv/baa+jo6EBiYiIba87OzsauXbuwc+dOvPjii7jqqqtGpXbrbOdkex6Ofr49yxy+riHjGAacReTz8HS4pMGnNrxz504iFotZxpWdnU22bt1K7rvvPjJjxgySkZFBIiMjSXJyMtFoNOyz09PTSWNjI/t3fHw86ezsJPfffz/ZuXMne/3ee+9lGfQ4+MNkMpHTp0+Tt956iyxbtoxMnDiRLFiwgPT19Z23NfHdOXGxaNEi8sEHHxCLxUICAwOJ0WgkhBDyww8/kOuuu25U1zvG4DCujgfdMYAPPviABWVCCHn33XfJ/fffb3UM36B73XXXkfDwcJKQkMCaMFw8++yzJC0tjWRmZpKZM2eS+vp69j2hUMgeBHPnznXnKV50sFgs53sJ5C9/+YtVI23NmjWDjlEqlazh2tnZSRITE8np06cJIYTMnz/fqpG2ffv2c7TyMQGHcXV0KvjjOKdwRYIPgEMJPrPZjG+//Rbbtm3DmTNnsGvXLpw5c8bqcyZPnoySkhKcOHEC8+fPx8MPP8y+J5PJcPz4cRw/fpyJ+VyqGM0hB6VSidmzZyMpKQmzZ8+222w9dOgQPv/8czz55JOQSqVYt24dE1uaM2cOfH19kZOTgylTpiAjIwPZ2dmYMWMG1q5dy5whnnrqKTz33HNITEyEQqEYS1Y65xfOIvJ5eT6Mw2UYjUYSFxdHamtrWSPt1KlTVse8/PLLVo20BQsWEEIIOXXqFGuk7dmzh8hkMtZI27RpE9m0aZPDn/vjjz+Syy+/nP3bEfVoHO4FXyoYhUKhIP7+/qzUQUsI4xhVjGe6YxlisRgvv/wyrr/+eqSlpWHhwoVIT0/Hhg0bWMZ53333QaFQIDExEc899xybGEpPT8fChQsxadIkrFy5EldddRUTdxnKYuXNN9+0kuvT6XTIz8/HtGnTsGHDBqSkpCAxMdHudNKOHTsQHByMnJwc5OTkMJ0LAHjnnXeYutg777zjlt/RWMK+ffuwaNEiAMCiRYuwd+9ep8ePi+9fYHAWkc/L82Ec5w18asMUO3fuJFOnTrUawGhqaiKEEFJZWUnEYjE5ePAgy7xpnZDi7bfftvvZCoWCxMXFEYVCQZRKJYmLiyNKpdIdpzdmwJcKRjFjxgzyv//9j/17fIjmnGA80x3H0OBrsfLVV1/hySefxMcff2ylpEWP7erqQkhICBQKBTw8PJiLLB8cOHAAs2fPRkBAAPz9/TF79mzs379/hGd28eHaa69FRkbGoC/b36MzKhgwMERz8uRJq4GXzZs3o7y8HMXFxVAqlU4tqsbhfozzdMfB4MhTjouffvoJy5Ytw/79+xESEsJe52pLlJWVobe3lzVk7LnIAsB///tffPPNN0hOTsbzzz+P6Ohouw60l6KLrDNT1tDQUCYy09raavV3sMX777+P2267zUp4hzovS6VSLF68GM8884z7Fj6OITGe6Y6DgU9teM2aNejt7cWCBQuQk5ODm2++GQBQVlbGhgIef/xxZGVlsaBrD3PnzkV9fT1OnDiB2bNnsxqlPZSXlzutD69evZrVhpOTk63Ef0QiEfseXevFDjpEA8DhEA3Frl27cOedd1q91traCmCgtLh3715kZGSM3mLHMRjOag/noxAyjosftkT6oVgQJpOJ+Pn5EUIIee+998jSpUvZ95YsWUJCQkKsRpxt68NcbNu2jSxevJj9+2JiVLz//vtk0qRJRCAQkOLiYofH/ec//yEymYxIJBKSkJDAhmj27t1LgoODSUJCAlm4cCGpqKggERERTBCJgg7RpKenk9/97ndWQzTjcBvGhyPGce7Ah8JGJ6UIIeTDDz8kU6dOJYQMNNJiY2OJUqkkSqWShIWFkRkzZrBjhwrgl112Gfniiy/Yvy+moHvmzBlSXl5OrrnmGodB157OBn0IjauCXVAYb6SN49yBT5li27ZtSE9PR3Z2NrZt28Y0WgMCAvDYY4+hoKAABQUFmDdvHnOXBZzXeM+ePYu6ujrMnDmTvcalsQ1FrTrfSEtLQ0pKitNjqM5GfHy8VZOSEIKDBw8yPQ4+VLJxnB+MB91xjApuvPFGVFZWoqamBuvXrwcAPPHEE6yuSm3Gf/75Zxw6dAipqansvffeey+qq6tRXV1tZQQ6FHbv3o358+dbmUiePXsWJSUleO+99/D73/8egYGBDmuYhBA88MADSExMRFZWFn788Uf2vQuFO+yo0ahQKDBhwgTmlHypNiAvBoyzF8ZxQYMvjQ0YCLrbt28f9H4AiI+Px5VXXomZM2fi3Xfftfv+zz//HFVVVaiqqkJhYSGWL1+OwsJCKJVKbNy4ESUlJRAIBMjLy8PNN99s14VhKDhTBXPWEBvHGIKz2sP41/jX+f7CQGJQCyAOgAeAnwGk2zkuFUA9fvH9++U1fwDSX/4/CEAVgGsBnHLws14FcCfn3xUAwgHcCeBVR8eNwjkfBpDv4HuXATjA+fejv3wJAHQBENs7bvzrwvkaLy+M44IGIcQE4P/bu1+QBoMwjuPfH6J9oMU/oMFgsIrRpLBmMyl2u01lGuwWg1odlsHAgWkmi4IahkUsbtoGFpPwGO7GXuZ0Ci+b6PNp93t53/eu3B0P78utAWfAHXBiZhVJOUnJb8CWgLzFGSeaAq4k3QJlYBe4/+J1I8Bjol2N2Wd5L1wCk5ImJA0Qxl2M4y4DjVNJV4Dv/ZHiusrLC+7XM7MSUGrJNlraW23uuwCmk5mk8dQ7mBJJi8AeMAScSroxswVJw8CBmWXN7E1SYxHqA47MrBIfsQ7kJe0A18BhD4bhOvBJ17mmGjCWaI/GrAbMteTnab/czApAoU3+BGQT7Q+LUMwfgJm0++XS5eUF55qKwLKCWeDFzJ4Ju8p5SRlJGWA+Zs79mO903b8h6ZiwYx2UVAU2gX4AM9sn7B6zhLrvK7Aar9UlbRPqqQA5M6t3t/fur+h0BLtzzrkUvQMm3C24FFqmhwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6FVQAflffQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ba4d273-b3b1-4bc9-b9a5-995ac883146a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow import keras\n",
        "import keras.backend as kb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kacclnf9fglJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (3,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(3, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1misytrfmUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate = 0.01)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6XLrbgufomH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training dataset\n",
        "batch_size = 100"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt-4C5xIfpRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tf.cast(X_train, tf.float32)\n",
        "Y_train = tf.cast(Y_train, tf.float32)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBaHU1stfuGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val = tf.cast(X_val, tf.float32)\n",
        "Y_val = tf.cast(Y_val, tf.float32)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPD_YTIzfrU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tf.cast(X_test, tf.float32)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--05jxiSf12q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a dataset with a separate element fro each row of the input tensor\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(buffer_size= 3000).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cNuUWdXgG8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "epochs = 500"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9M4pfzBg_Pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an loss function # not effective\n",
        "def Keras_loss_function(y_actual, y_predicted):\n",
        "  mse = kb.mean(kb.sum(kb.square(y_actual - y_predicted))) # (1/n*(sum(sqr(Y-Y_hat))))\n",
        "  return mse\n",
        "\n",
        "# first 100 epochs is useful"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UCU9yX8gL0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ddf8e62-5c28-4a22-ce48-a5d77ec84b04"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # print(linear)\n",
        "      # compute the loss value for this minibatch\n",
        "      mse = Keras_loss_function(y_batch_train, linear)\n",
        "      # mse = tf.keras.losses.MSE(y_batch_train, linear)\n",
        "      # print(mse)\n",
        "      # mse = keras.losses.mean_squared_error(y_batch_train, linear) # mse gives 3 number\n",
        "    # print(mse)\n",
        "    # use the gradient tap to automatically retrieve the gradients of the \n",
        "    # trainable variables with respect to the loss\n",
        "    grads = tape.gradient(mse, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training loss (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(mse))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    val_mse = Keras_loss_function(y_batch_val, val_linear)\n",
        "\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MSE: %.4f\" % (float(val_mse)))\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 161.4005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 147.6059\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 137.6904\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 58.1657\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 120.0262\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 117.2203\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 104.5565\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 49.0517\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 95.2784\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 87.5972\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 79.9248\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 37.1034\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 79.6720\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 70.6396\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 64.2727\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 33.4465\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 63.4362\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 62.3900\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 53.8432\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 28.8013\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 57.6579\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 44.4591\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 42.6775\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 20.9858\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 39.9979\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 41.8617\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 36.7153\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 15.3387\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 30.5219\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 30.8408\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 26.2918\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 13.7357\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 30.2422\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 24.4422\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 22.3499\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 10.1965\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 22.9489\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 18.6880\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 17.2945\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 7.4732\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss (for one batch) at step 0: 15.9625\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 16.5612\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 12.4640\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 6.6211\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss (for one batch) at step 0: 14.5706\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 12.1288\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 12.9339\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 5.6841\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss (for one batch) at step 0: 14.6339\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 11.8489\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 10.4062\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 5.9855\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss (for one batch) at step 0: 10.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 11.2406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 9.8656\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 5.5439\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss (for one batch) at step 0: 11.7557\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 9.1354\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 10.0619\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.3575\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss (for one batch) at step 0: 10.9192\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 10.0775\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 8.3272\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 7.4379\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss (for one batch) at step 0: 9.2606\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.1842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 10.3609\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.8895\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss (for one batch) at step 0: 8.9730\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 8.9031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 7.1273\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.1364\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss (for one batch) at step 0: 6.7487\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 8.2961\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 7.5687\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.3266\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss (for one batch) at step 0: 7.9723\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.7419\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 8.4609\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.9592\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss (for one batch) at step 0: 6.9390\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 8.5832\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.9196\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.2151\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss (for one batch) at step 0: 8.3061\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.8629\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.9617\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.6981\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss (for one batch) at step 0: 8.4706\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 6.1370\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 7.2550\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.0965\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss (for one batch) at step 0: 6.9504\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 6.8260\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.3372\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.9809\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss (for one batch) at step 0: 5.9817\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.1455\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.8666\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.9382\n",
            "\n",
            "Start of epoch 25\n",
            "Training loss (for one batch) at step 0: 6.4239\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 6.1291\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.5271\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.9455\n",
            "\n",
            "Start of epoch 26\n",
            "Training loss (for one batch) at step 0: 6.6714\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.4135\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.0965\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.1333\n",
            "\n",
            "Start of epoch 27\n",
            "Training loss (for one batch) at step 0: 8.5832\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.0511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.8042\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.9451\n",
            "\n",
            "Start of epoch 28\n",
            "Training loss (for one batch) at step 0: 5.8039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.3209\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.7682\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.9315\n",
            "\n",
            "Start of epoch 29\n",
            "Training loss (for one batch) at step 0: 5.9864\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.2103\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.2727\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.0019\n",
            "\n",
            "Start of epoch 30\n",
            "Training loss (for one batch) at step 0: 5.0548\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.5250\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.6478\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8881\n",
            "\n",
            "Start of epoch 31\n",
            "Training loss (for one batch) at step 0: 6.3607\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.4465\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.3335\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.3976\n",
            "\n",
            "Start of epoch 32\n",
            "Training loss (for one batch) at step 0: 5.4797\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.1126\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.6231\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0937\n",
            "\n",
            "Start of epoch 33\n",
            "Training loss (for one batch) at step 0: 5.4853\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.8223\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.8291\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.3813\n",
            "\n",
            "Start of epoch 34\n",
            "Training loss (for one batch) at step 0: 4.8349\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.6266\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.5216\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.3329\n",
            "\n",
            "Start of epoch 35\n",
            "Training loss (for one batch) at step 0: 5.9206\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3111\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.3586\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.1555\n",
            "\n",
            "Start of epoch 36\n",
            "Training loss (for one batch) at step 0: 6.1300\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.3937\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.3623\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.1308\n",
            "\n",
            "Start of epoch 37\n",
            "Training loss (for one batch) at step 0: 5.6769\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3645\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.6614\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.3299\n",
            "\n",
            "Start of epoch 38\n",
            "Training loss (for one batch) at step 0: 4.5351\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.8096\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.9980\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0442\n",
            "\n",
            "Start of epoch 39\n",
            "Training loss (for one batch) at step 0: 5.0252\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6148\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.4934\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.6997\n",
            "\n",
            "Start of epoch 40\n",
            "Training loss (for one batch) at step 0: 4.5830\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3361\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.0236\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.4017\n",
            "\n",
            "Start of epoch 41\n",
            "Training loss (for one batch) at step 0: 5.4540\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.1556\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.7203\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5791\n",
            "\n",
            "Start of epoch 42\n",
            "Training loss (for one batch) at step 0: 4.7401\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4485\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1128\n",
            "\n",
            "Start of epoch 43\n",
            "Training loss (for one batch) at step 0: 3.5598\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.9240\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.9539\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8893\n",
            "\n",
            "Start of epoch 44\n",
            "Training loss (for one batch) at step 0: 4.3813\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.9342\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.9947\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1818\n",
            "\n",
            "Start of epoch 45\n",
            "Training loss (for one batch) at step 0: 4.5287\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.6597\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4426\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8483\n",
            "\n",
            "Start of epoch 46\n",
            "Training loss (for one batch) at step 0: 5.1917\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.4089\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.2259\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1631\n",
            "\n",
            "Start of epoch 47\n",
            "Training loss (for one batch) at step 0: 3.7250\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.6367\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.0000\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.5545\n",
            "\n",
            "Start of epoch 48\n",
            "Training loss (for one batch) at step 0: 4.7059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.9614\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.2236\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.6832\n",
            "\n",
            "Start of epoch 49\n",
            "Training loss (for one batch) at step 0: 3.0964\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1740\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.5082\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.6696\n",
            "\n",
            "Start of epoch 50\n",
            "Training loss (for one batch) at step 0: 4.0435\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.6406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4092\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4593\n",
            "\n",
            "Start of epoch 51\n",
            "Training loss (for one batch) at step 0: 3.1931\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.2903\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.7256\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6036\n",
            "\n",
            "Start of epoch 52\n",
            "Training loss (for one batch) at step 0: 4.6940\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6163\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.6057\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.9271\n",
            "\n",
            "Start of epoch 53\n",
            "Training loss (for one batch) at step 0: 3.2313\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8138\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.0431\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6836\n",
            "\n",
            "Start of epoch 54\n",
            "Training loss (for one batch) at step 0: 2.8758\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.1996\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.0268\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4621\n",
            "\n",
            "Start of epoch 55\n",
            "Training loss (for one batch) at step 0: 3.6751\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.8736\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4258\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2221\n",
            "\n",
            "Start of epoch 56\n",
            "Training loss (for one batch) at step 0: 3.0680\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9169\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.3309\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5673\n",
            "\n",
            "Start of epoch 57\n",
            "Training loss (for one batch) at step 0: 2.7438\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.3628\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4544\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4664\n",
            "\n",
            "Start of epoch 58\n",
            "Training loss (for one batch) at step 0: 3.4573\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6859\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9818\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0119\n",
            "\n",
            "Start of epoch 59\n",
            "Training loss (for one batch) at step 0: 3.9461\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6269\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4193\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8424\n",
            "\n",
            "Start of epoch 60\n",
            "Training loss (for one batch) at step 0: 2.8683\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1723\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.3538\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.2304\n",
            "\n",
            "Start of epoch 61\n",
            "Training loss (for one batch) at step 0: 3.5622\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.7039\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0905\n",
            "\n",
            "Start of epoch 62\n",
            "Training loss (for one batch) at step 0: 2.9276\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.0941\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.1288\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.9646\n",
            "\n",
            "Start of epoch 63\n",
            "Training loss (for one batch) at step 0: 2.6209\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.7903\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.4926\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.2800\n",
            "\n",
            "Start of epoch 64\n",
            "Training loss (for one batch) at step 0: 3.8605\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.5895\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4741\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.7785\n",
            "\n",
            "Start of epoch 65\n",
            "Training loss (for one batch) at step 0: 3.0242\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9253\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2571\n",
            "\n",
            "Start of epoch 66\n",
            "Training loss (for one batch) at step 0: 3.1141\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3944\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.1803\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0197\n",
            "\n",
            "Start of epoch 67\n",
            "Training loss (for one batch) at step 0: 2.8410\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9430\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9759\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5403\n",
            "\n",
            "Start of epoch 68\n",
            "Training loss (for one batch) at step 0: 3.0726\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.7035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.5890\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.3464\n",
            "\n",
            "Start of epoch 69\n",
            "Training loss (for one batch) at step 0: 3.3589\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1289\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7489\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0643\n",
            "\n",
            "Start of epoch 70\n",
            "Training loss (for one batch) at step 0: 2.8664\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.2929\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7890\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1638\n",
            "\n",
            "Start of epoch 71\n",
            "Training loss (for one batch) at step 0: 2.5620\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.7403\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.3094\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2305\n",
            "\n",
            "Start of epoch 72\n",
            "Training loss (for one batch) at step 0: 2.5207\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6283\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9305\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.7219\n",
            "\n",
            "Start of epoch 73\n",
            "Training loss (for one batch) at step 0: 3.2698\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5613\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.0439\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.7442\n",
            "\n",
            "Start of epoch 74\n",
            "Training loss (for one batch) at step 0: 2.2441\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6619\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0443\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1322\n",
            "\n",
            "Start of epoch 75\n",
            "Training loss (for one batch) at step 0: 2.4507\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.4444\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3147\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8361\n",
            "\n",
            "Start of epoch 76\n",
            "Training loss (for one batch) at step 0: 2.3897\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.3674\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.5445\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6227\n",
            "\n",
            "Start of epoch 77\n",
            "Training loss (for one batch) at step 0: 2.1708\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8098\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1817\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1695\n",
            "\n",
            "Start of epoch 78\n",
            "Training loss (for one batch) at step 0: 3.0982\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.7786\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3401\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1979\n",
            "\n",
            "Start of epoch 79\n",
            "Training loss (for one batch) at step 0: 2.9169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9654\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9181\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5812\n",
            "\n",
            "Start of epoch 80\n",
            "Training loss (for one batch) at step 0: 2.5255\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3228\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.2280\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0088\n",
            "\n",
            "Start of epoch 81\n",
            "Training loss (for one batch) at step 0: 2.6575\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9927\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7589\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0442\n",
            "\n",
            "Start of epoch 82\n",
            "Training loss (for one batch) at step 0: 3.5510\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4712\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3000\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9910\n",
            "\n",
            "Start of epoch 83\n",
            "Training loss (for one batch) at step 0: 2.2460\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6399\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5630\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5014\n",
            "\n",
            "Start of epoch 84\n",
            "Training loss (for one batch) at step 0: 2.6309\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6540\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3569\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0785\n",
            "\n",
            "Start of epoch 85\n",
            "Training loss (for one batch) at step 0: 2.0259\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4197\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0923\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8822\n",
            "\n",
            "Start of epoch 86\n",
            "Training loss (for one batch) at step 0: 2.2104\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3580\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9857\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5740\n",
            "\n",
            "Start of epoch 87\n",
            "Training loss (for one batch) at step 0: 2.1709\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1688\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6821\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4141\n",
            "\n",
            "Start of epoch 88\n",
            "Training loss (for one batch) at step 0: 1.9257\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1702\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6543\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0796\n",
            "\n",
            "Start of epoch 89\n",
            "Training loss (for one batch) at step 0: 2.1333\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3025\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.8367\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1648\n",
            "\n",
            "Start of epoch 90\n",
            "Training loss (for one batch) at step 0: 2.0139\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5765\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7031\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4796\n",
            "\n",
            "Start of epoch 91\n",
            "Training loss (for one batch) at step 0: 2.8871\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5762\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7302\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9554\n",
            "\n",
            "Start of epoch 92\n",
            "Training loss (for one batch) at step 0: 1.9396\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2297\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6002\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1053\n",
            "\n",
            "Start of epoch 93\n",
            "Training loss (for one batch) at step 0: 1.8817\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1344\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.2133\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.9579\n",
            "\n",
            "Start of epoch 94\n",
            "Training loss (for one batch) at step 0: 2.2038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2833\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0275\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8422\n",
            "\n",
            "Start of epoch 95\n",
            "Training loss (for one batch) at step 0: 1.9234\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8422\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7894\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1764\n",
            "\n",
            "Start of epoch 96\n",
            "Training loss (for one batch) at step 0: 1.9614\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6975\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4036\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0166\n",
            "\n",
            "Start of epoch 97\n",
            "Training loss (for one batch) at step 0: 3.0127\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6297\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4336\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0470\n",
            "\n",
            "Start of epoch 98\n",
            "Training loss (for one batch) at step 0: 1.9880\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3899\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9978\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8056\n",
            "\n",
            "Start of epoch 99\n",
            "Training loss (for one batch) at step 0: 2.8859\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.2984\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6247\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7205\n",
            "\n",
            "Start of epoch 100\n",
            "Training loss (for one batch) at step 0: 2.0890\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2611\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8366\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7854\n",
            "\n",
            "Start of epoch 101\n",
            "Training loss (for one batch) at step 0: 2.1319\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9132\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3305\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4953\n",
            "\n",
            "Start of epoch 102\n",
            "Training loss (for one batch) at step 0: 2.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2316\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.5878\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9412\n",
            "\n",
            "Start of epoch 103\n",
            "Training loss (for one batch) at step 0: 2.2152\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6724\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7925\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0498\n",
            "\n",
            "Start of epoch 104\n",
            "Training loss (for one batch) at step 0: 1.8064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7180\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9688\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5222\n",
            "\n",
            "Start of epoch 105\n",
            "Training loss (for one batch) at step 0: 2.0685\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5401\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5102\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9693\n",
            "\n",
            "Start of epoch 106\n",
            "Training loss (for one batch) at step 0: 1.9050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4961\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9055\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6831\n",
            "\n",
            "Start of epoch 107\n",
            "Training loss (for one batch) at step 0: 2.4128\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4879\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.8362\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7396\n",
            "\n",
            "Start of epoch 108\n",
            "Training loss (for one batch) at step 0: 1.7929\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4333\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1480\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2103\n",
            "\n",
            "Start of epoch 109\n",
            "Training loss (for one batch) at step 0: 1.7326\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0308\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3413\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8071\n",
            "\n",
            "Start of epoch 110\n",
            "Training loss (for one batch) at step 0: 1.3639\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2224\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1343\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5592\n",
            "\n",
            "Start of epoch 111\n",
            "Training loss (for one batch) at step 0: 1.9410\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6514\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.4109\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8256\n",
            "\n",
            "Start of epoch 112\n",
            "Training loss (for one batch) at step 0: 2.3453\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6280\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.8573\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1556\n",
            "\n",
            "Start of epoch 113\n",
            "Training loss (for one batch) at step 0: 1.8310\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6247\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7694\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4016\n",
            "\n",
            "Start of epoch 114\n",
            "Training loss (for one batch) at step 0: 2.1006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2877\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9786\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1502\n",
            "\n",
            "Start of epoch 115\n",
            "Training loss (for one batch) at step 0: 1.6588\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8841\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.4592\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2881\n",
            "\n",
            "Start of epoch 116\n",
            "Training loss (for one batch) at step 0: 3.4464\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8177\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0800\n",
            "\n",
            "Start of epoch 117\n",
            "Training loss (for one batch) at step 0: 2.1663\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7166\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9593\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7827\n",
            "\n",
            "Start of epoch 118\n",
            "Training loss (for one batch) at step 0: 2.1488\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8269\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5254\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3995\n",
            "\n",
            "Start of epoch 119\n",
            "Training loss (for one batch) at step 0: 1.5045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0306\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3495\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0403\n",
            "\n",
            "Start of epoch 120\n",
            "Training loss (for one batch) at step 0: 1.6905\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8131\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6350\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8351\n",
            "\n",
            "Start of epoch 121\n",
            "Training loss (for one batch) at step 0: 1.9286\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6336\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.2042\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6534\n",
            "\n",
            "Start of epoch 122\n",
            "Training loss (for one batch) at step 0: 1.9398\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6130\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0226\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8153\n",
            "\n",
            "Start of epoch 123\n",
            "Training loss (for one batch) at step 0: 1.8303\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4915\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5235\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7792\n",
            "\n",
            "Start of epoch 124\n",
            "Training loss (for one batch) at step 0: 2.2011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5909\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5401\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6716\n",
            "\n",
            "Start of epoch 125\n",
            "Training loss (for one batch) at step 0: 1.8069\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7832\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9743\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7946\n",
            "\n",
            "Start of epoch 126\n",
            "Training loss (for one batch) at step 0: 1.9476\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8533\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0338\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0166\n",
            "\n",
            "Start of epoch 127\n",
            "Training loss (for one batch) at step 0: 1.5204\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2114\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8328\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8898\n",
            "\n",
            "Start of epoch 128\n",
            "Training loss (for one batch) at step 0: 1.6748\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5077\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3706\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7607\n",
            "\n",
            "Start of epoch 129\n",
            "Training loss (for one batch) at step 0: 1.5120\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9832\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9976\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7708\n",
            "\n",
            "Start of epoch 130\n",
            "Training loss (for one batch) at step 0: 1.7985\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4874\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5076\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7573\n",
            "\n",
            "Start of epoch 131\n",
            "Training loss (for one batch) at step 0: 1.8226\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6452\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1190\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0087\n",
            "\n",
            "Start of epoch 132\n",
            "Training loss (for one batch) at step 0: 2.0092\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4777\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6616\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1827\n",
            "\n",
            "Start of epoch 133\n",
            "Training loss (for one batch) at step 0: 2.2702\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9737\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5372\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0941\n",
            "\n",
            "Start of epoch 134\n",
            "Training loss (for one batch) at step 0: 1.9282\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6285\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.2200\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8169\n",
            "\n",
            "Start of epoch 135\n",
            "Training loss (for one batch) at step 0: 2.0751\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4268\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3448\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7912\n",
            "\n",
            "Start of epoch 136\n",
            "Training loss (for one batch) at step 0: 1.4561\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7289\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6456\n",
            "\n",
            "Start of epoch 137\n",
            "Training loss (for one batch) at step 0: 1.4184\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7268\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5513\n",
            "\n",
            "Start of epoch 138\n",
            "Training loss (for one batch) at step 0: 1.6622\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6152\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0724\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1067\n",
            "\n",
            "Start of epoch 139\n",
            "Training loss (for one batch) at step 0: 1.5167\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7115\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7065\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7297\n",
            "\n",
            "Start of epoch 140\n",
            "Training loss (for one batch) at step 0: 1.7559\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1615\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7153\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9518\n",
            "\n",
            "Start of epoch 141\n",
            "Training loss (for one batch) at step 0: 1.4967\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4839\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9030\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7691\n",
            "\n",
            "Start of epoch 142\n",
            "Training loss (for one batch) at step 0: 1.2859\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8825\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8854\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9788\n",
            "\n",
            "Start of epoch 143\n",
            "Training loss (for one batch) at step 0: 2.0166\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6967\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8762\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0649\n",
            "\n",
            "Start of epoch 144\n",
            "Training loss (for one batch) at step 0: 1.4295\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9903\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2370\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8335\n",
            "\n",
            "Start of epoch 145\n",
            "Training loss (for one batch) at step 0: 1.2711\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9570\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6061\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2820\n",
            "\n",
            "Start of epoch 146\n",
            "Training loss (for one batch) at step 0: 1.8889\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8636\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9652\n",
            "\n",
            "Start of epoch 147\n",
            "Training loss (for one batch) at step 0: 1.6557\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7957\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7241\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5703\n",
            "\n",
            "Start of epoch 148\n",
            "Training loss (for one batch) at step 0: 1.5615\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5012\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5215\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8248\n",
            "\n",
            "Start of epoch 149\n",
            "Training loss (for one batch) at step 0: 1.0664\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8147\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5908\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8394\n",
            "\n",
            "Start of epoch 150\n",
            "Training loss (for one batch) at step 0: 2.0575\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5771\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6822\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7167\n",
            "\n",
            "Start of epoch 151\n",
            "Training loss (for one batch) at step 0: 1.6212\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6224\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6632\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3384\n",
            "\n",
            "Start of epoch 152\n",
            "Training loss (for one batch) at step 0: 1.9661\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2233\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5020\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8555\n",
            "\n",
            "Start of epoch 153\n",
            "Training loss (for one batch) at step 0: 1.2540\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9951\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1981\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8765\n",
            "\n",
            "Start of epoch 154\n",
            "Training loss (for one batch) at step 0: 1.7372\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5641\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9478\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7560\n",
            "\n",
            "Start of epoch 155\n",
            "Training loss (for one batch) at step 0: 1.7874\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1889\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8942\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4667\n",
            "\n",
            "Start of epoch 156\n",
            "Training loss (for one batch) at step 0: 1.3985\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3562\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7692\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7454\n",
            "\n",
            "Start of epoch 157\n",
            "Training loss (for one batch) at step 0: 1.6663\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5330\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8290\n",
            "\n",
            "Start of epoch 158\n",
            "Training loss (for one batch) at step 0: 1.7429\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.5135\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8410\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7023\n",
            "\n",
            "Start of epoch 159\n",
            "Training loss (for one batch) at step 0: 1.6197\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3396\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3250\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5261\n",
            "\n",
            "Start of epoch 160\n",
            "Training loss (for one batch) at step 0: 1.3233\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4585\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3111\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1113\n",
            "\n",
            "Start of epoch 161\n",
            "Training loss (for one batch) at step 0: 2.2184\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5321\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1886\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7215\n",
            "\n",
            "Start of epoch 162\n",
            "Training loss (for one batch) at step 0: 1.2845\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7087\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0373\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7035\n",
            "\n",
            "Start of epoch 163\n",
            "Training loss (for one batch) at step 0: 2.0563\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3524\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6467\n",
            "\n",
            "Start of epoch 164\n",
            "Training loss (for one batch) at step 0: 1.8244\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6424\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4358\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6013\n",
            "\n",
            "Start of epoch 165\n",
            "Training loss (for one batch) at step 0: 1.8339\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1949\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3530\n",
            "\n",
            "Start of epoch 166\n",
            "Training loss (for one batch) at step 0: 1.3296\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2179\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5686\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1807\n",
            "\n",
            "Start of epoch 167\n",
            "Training loss (for one batch) at step 0: 1.6424\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2869\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4166\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5090\n",
            "\n",
            "Start of epoch 168\n",
            "Training loss (for one batch) at step 0: 1.4427\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3698\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6111\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8447\n",
            "\n",
            "Start of epoch 169\n",
            "Training loss (for one batch) at step 0: 1.0169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4185\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4977\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8508\n",
            "\n",
            "Start of epoch 170\n",
            "Training loss (for one batch) at step 0: 2.1959\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0691\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2602\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1318\n",
            "\n",
            "Start of epoch 171\n",
            "Training loss (for one batch) at step 0: 1.3798\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0636\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6888\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6035\n",
            "\n",
            "Start of epoch 172\n",
            "Training loss (for one batch) at step 0: 1.5042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2092\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9954\n",
            "\n",
            "Start of epoch 173\n",
            "Training loss (for one batch) at step 0: 1.5788\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4104\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7363\n",
            "\n",
            "Start of epoch 174\n",
            "Training loss (for one batch) at step 0: 2.0137\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1340\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2505\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7182\n",
            "\n",
            "Start of epoch 175\n",
            "Training loss (for one batch) at step 0: 1.2315\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5596\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5528\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6750\n",
            "\n",
            "Start of epoch 176\n",
            "Training loss (for one batch) at step 0: 1.5906\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9306\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5360\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6940\n",
            "\n",
            "Start of epoch 177\n",
            "Training loss (for one batch) at step 0: 1.6272\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1976\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5550\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6778\n",
            "\n",
            "Start of epoch 178\n",
            "Training loss (for one batch) at step 0: 1.3726\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0945\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2806\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6962\n",
            "\n",
            "Start of epoch 179\n",
            "Training loss (for one batch) at step 0: 1.4089\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0315\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5347\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6744\n",
            "\n",
            "Start of epoch 180\n",
            "Training loss (for one batch) at step 0: 1.3851\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1897\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0630\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8689\n",
            "\n",
            "Start of epoch 181\n",
            "Training loss (for one batch) at step 0: 1.5214\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3025\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6125\n",
            "\n",
            "Start of epoch 182\n",
            "Training loss (for one batch) at step 0: 1.4768\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3739\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9420\n",
            "\n",
            "Start of epoch 183\n",
            "Training loss (for one batch) at step 0: 1.2138\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6245\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8150\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1057\n",
            "\n",
            "Start of epoch 184\n",
            "Training loss (for one batch) at step 0: 1.2016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3498\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3476\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5774\n",
            "\n",
            "Start of epoch 185\n",
            "Training loss (for one batch) at step 0: 1.2960\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1087\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0028\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4963\n",
            "\n",
            "Start of epoch 186\n",
            "Training loss (for one batch) at step 0: 1.2774\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1320\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3615\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8613\n",
            "\n",
            "Start of epoch 187\n",
            "Training loss (for one batch) at step 0: 0.9367\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2936\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4828\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5987\n",
            "\n",
            "Start of epoch 188\n",
            "Training loss (for one batch) at step 0: 1.3236\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1800\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2885\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6027\n",
            "\n",
            "Start of epoch 189\n",
            "Training loss (for one batch) at step 0: 1.1305\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1318\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8201\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7457\n",
            "\n",
            "Start of epoch 190\n",
            "Training loss (for one batch) at step 0: 1.2434\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3824\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8254\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9233\n",
            "\n",
            "Start of epoch 191\n",
            "Training loss (for one batch) at step 0: 1.1165\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1808\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5960\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1288\n",
            "\n",
            "Start of epoch 192\n",
            "Training loss (for one batch) at step 0: 1.3035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5769\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0071\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7276\n",
            "\n",
            "Start of epoch 193\n",
            "Training loss (for one batch) at step 0: 1.7639\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4086\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3086\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5929\n",
            "\n",
            "Start of epoch 194\n",
            "Training loss (for one batch) at step 0: 1.1319\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4617\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7866\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7660\n",
            "\n",
            "Start of epoch 195\n",
            "Training loss (for one batch) at step 0: 1.4162\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4542\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0947\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8517\n",
            "\n",
            "Start of epoch 196\n",
            "Training loss (for one batch) at step 0: 1.3369\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0146\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2387\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7022\n",
            "\n",
            "Start of epoch 197\n",
            "Training loss (for one batch) at step 0: 1.0274\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3517\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2017\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9107\n",
            "\n",
            "Start of epoch 198\n",
            "Training loss (for one batch) at step 0: 1.3191\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3078\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0963\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0777\n",
            "\n",
            "Start of epoch 199\n",
            "Training loss (for one batch) at step 0: 1.0223\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9854\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0735\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1431\n",
            "\n",
            "Start of epoch 200\n",
            "Training loss (for one batch) at step 0: 1.3475\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2679\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9629\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7054\n",
            "\n",
            "Start of epoch 201\n",
            "Training loss (for one batch) at step 0: 0.9543\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9215\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5543\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8276\n",
            "\n",
            "Start of epoch 202\n",
            "Training loss (for one batch) at step 0: 2.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7176\n",
            "\n",
            "Start of epoch 203\n",
            "Training loss (for one batch) at step 0: 1.4340\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4414\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8063\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6263\n",
            "\n",
            "Start of epoch 204\n",
            "Training loss (for one batch) at step 0: 1.1422\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2667\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1566\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5211\n",
            "\n",
            "Start of epoch 205\n",
            "Training loss (for one batch) at step 0: 1.5170\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4404\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1847\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5416\n",
            "\n",
            "Start of epoch 206\n",
            "Training loss (for one batch) at step 0: 1.2105\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3288\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9107\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8553\n",
            "\n",
            "Start of epoch 207\n",
            "Training loss (for one batch) at step 0: 1.6613\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6323\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0210\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5792\n",
            "\n",
            "Start of epoch 208\n",
            "Training loss (for one batch) at step 0: 0.9294\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3578\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0760\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5148\n",
            "\n",
            "Start of epoch 209\n",
            "Training loss (for one batch) at step 0: 1.5028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1491\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2019\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3574\n",
            "\n",
            "Start of epoch 210\n",
            "Training loss (for one batch) at step 0: 1.1687\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3549\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5469\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5403\n",
            "\n",
            "Start of epoch 211\n",
            "Training loss (for one batch) at step 0: 1.0325\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4267\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2230\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3587\n",
            "\n",
            "Start of epoch 212\n",
            "Training loss (for one batch) at step 0: 1.3030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4950\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7204\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4373\n",
            "\n",
            "Start of epoch 213\n",
            "Training loss (for one batch) at step 0: 1.5634\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0454\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4242\n",
            "\n",
            "Start of epoch 214\n",
            "Training loss (for one batch) at step 0: 1.5786\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2435\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3258\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5253\n",
            "\n",
            "Start of epoch 215\n",
            "Training loss (for one batch) at step 0: 1.1049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0236\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9475\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6495\n",
            "\n",
            "Start of epoch 216\n",
            "Training loss (for one batch) at step 0: 0.8357\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0799\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7638\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4296\n",
            "\n",
            "Start of epoch 217\n",
            "Training loss (for one batch) at step 0: 1.4345\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0152\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0091\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6206\n",
            "\n",
            "Start of epoch 218\n",
            "Training loss (for one batch) at step 0: 1.0344\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0491\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3907\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5475\n",
            "\n",
            "Start of epoch 219\n",
            "Training loss (for one batch) at step 0: 1.0097\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2767\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2771\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6528\n",
            "\n",
            "Start of epoch 220\n",
            "Training loss (for one batch) at step 0: 1.3207\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3426\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4927\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6146\n",
            "\n",
            "Start of epoch 221\n",
            "Training loss (for one batch) at step 0: 0.9842\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3908\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5757\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8169\n",
            "\n",
            "Start of epoch 222\n",
            "Training loss (for one batch) at step 0: 1.7290\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1218\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8459\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3991\n",
            "\n",
            "Start of epoch 223\n",
            "Training loss (for one batch) at step 0: 1.2797\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1147\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0497\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5589\n",
            "\n",
            "Start of epoch 224\n",
            "Training loss (for one batch) at step 0: 1.0019\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8875\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9340\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7198\n",
            "\n",
            "Start of epoch 225\n",
            "Training loss (for one batch) at step 0: 0.9107\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4711\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7002\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7253\n",
            "\n",
            "Start of epoch 226\n",
            "Training loss (for one batch) at step 0: 1.0515\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4920\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1373\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7863\n",
            "\n",
            "Start of epoch 227\n",
            "Training loss (for one batch) at step 0: 1.0510\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1264\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9344\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9712\n",
            "\n",
            "Start of epoch 228\n",
            "Training loss (for one batch) at step 0: 1.1199\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0114\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2220\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9542\n",
            "\n",
            "Start of epoch 229\n",
            "Training loss (for one batch) at step 0: 1.6525\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8525\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0275\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7483\n",
            "\n",
            "Start of epoch 230\n",
            "Training loss (for one batch) at step 0: 0.9888\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6547\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4380\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4454\n",
            "\n",
            "Start of epoch 231\n",
            "Training loss (for one batch) at step 0: 0.9056\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6808\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1616\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6282\n",
            "\n",
            "Start of epoch 232\n",
            "Training loss (for one batch) at step 0: 1.0997\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9219\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6711\n",
            "\n",
            "Start of epoch 233\n",
            "Training loss (for one batch) at step 0: 1.0589\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9538\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9537\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7611\n",
            "\n",
            "Start of epoch 234\n",
            "Training loss (for one batch) at step 0: 1.2122\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1246\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2679\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4717\n",
            "\n",
            "Start of epoch 235\n",
            "Training loss (for one batch) at step 0: 1.4702\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9943\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7852\n",
            "\n",
            "Start of epoch 236\n",
            "Training loss (for one batch) at step 0: 1.3407\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1390\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9060\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6230\n",
            "\n",
            "Start of epoch 237\n",
            "Training loss (for one batch) at step 0: 0.9494\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2196\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3370\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4571\n",
            "\n",
            "Start of epoch 238\n",
            "Training loss (for one batch) at step 0: 0.9680\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2797\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2105\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6602\n",
            "\n",
            "Start of epoch 239\n",
            "Training loss (for one batch) at step 0: 0.8601\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8929\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3703\n",
            "\n",
            "Start of epoch 240\n",
            "Training loss (for one batch) at step 0: 1.0997\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4334\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0409\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8825\n",
            "\n",
            "Start of epoch 241\n",
            "Training loss (for one batch) at step 0: 0.9282\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8625\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2936\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5224\n",
            "\n",
            "Start of epoch 242\n",
            "Training loss (for one batch) at step 0: 1.0381\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7220\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6892\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8266\n",
            "\n",
            "Start of epoch 243\n",
            "Training loss (for one batch) at step 0: 0.8926\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0618\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1565\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6043\n",
            "\n",
            "Start of epoch 244\n",
            "Training loss (for one batch) at step 0: 1.1226\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9448\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4611\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4196\n",
            "\n",
            "Start of epoch 245\n",
            "Training loss (for one batch) at step 0: 0.9801\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9789\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3825\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6576\n",
            "\n",
            "Start of epoch 246\n",
            "Training loss (for one batch) at step 0: 1.3340\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8572\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2866\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3714\n",
            "\n",
            "Start of epoch 247\n",
            "Training loss (for one batch) at step 0: 1.3524\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0496\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1088\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5224\n",
            "\n",
            "Start of epoch 248\n",
            "Training loss (for one batch) at step 0: 1.8186\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8515\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6250\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3997\n",
            "\n",
            "Start of epoch 249\n",
            "Training loss (for one batch) at step 0: 0.9415\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1436\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9409\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5939\n",
            "\n",
            "Start of epoch 250\n",
            "Training loss (for one batch) at step 0: 0.8614\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3792\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9236\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4733\n",
            "\n",
            "Start of epoch 251\n",
            "Training loss (for one batch) at step 0: 0.7997\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6365\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2239\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6241\n",
            "\n",
            "Start of epoch 252\n",
            "Training loss (for one batch) at step 0: 1.0610\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2582\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9681\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7830\n",
            "\n",
            "Start of epoch 253\n",
            "Training loss (for one batch) at step 0: 1.2835\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9195\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9154\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5007\n",
            "\n",
            "Start of epoch 254\n",
            "Training loss (for one batch) at step 0: 1.2999\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9887\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1841\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4436\n",
            "\n",
            "Start of epoch 255\n",
            "Training loss (for one batch) at step 0: 1.3940\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1285\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4175\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5876\n",
            "\n",
            "Start of epoch 256\n",
            "Training loss (for one batch) at step 0: 0.8168\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9203\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8124\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4730\n",
            "\n",
            "Start of epoch 257\n",
            "Training loss (for one batch) at step 0: 0.9665\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2271\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1423\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6575\n",
            "\n",
            "Start of epoch 258\n",
            "Training loss (for one batch) at step 0: 1.0862\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0820\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1551\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7572\n",
            "\n",
            "Start of epoch 259\n",
            "Training loss (for one batch) at step 0: 1.1958\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3583\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2242\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4802\n",
            "\n",
            "Start of epoch 260\n",
            "Training loss (for one batch) at step 0: 1.1903\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9078\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2115\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4887\n",
            "\n",
            "Start of epoch 261\n",
            "Training loss (for one batch) at step 0: 1.2370\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9788\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4083\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5931\n",
            "\n",
            "Start of epoch 262\n",
            "Training loss (for one batch) at step 0: 1.2373\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4504\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8527\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7393\n",
            "\n",
            "Start of epoch 263\n",
            "Training loss (for one batch) at step 0: 1.3434\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1138\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1036\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5043\n",
            "\n",
            "Start of epoch 264\n",
            "Training loss (for one batch) at step 0: 1.2658\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3260\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0697\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5349\n",
            "\n",
            "Start of epoch 265\n",
            "Training loss (for one batch) at step 0: 1.2841\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2880\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1756\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7440\n",
            "\n",
            "Start of epoch 266\n",
            "Training loss (for one batch) at step 0: 0.9769\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4111\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0435\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6345\n",
            "\n",
            "Start of epoch 267\n",
            "Training loss (for one batch) at step 0: 0.8164\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8099\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8710\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5727\n",
            "\n",
            "Start of epoch 268\n",
            "Training loss (for one batch) at step 0: 0.8539\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0980\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9584\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8287\n",
            "\n",
            "Start of epoch 269\n",
            "Training loss (for one batch) at step 0: 1.0766\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9350\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3066\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5970\n",
            "\n",
            "Start of epoch 270\n",
            "Training loss (for one batch) at step 0: 1.1627\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0695\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8166\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6682\n",
            "\n",
            "Start of epoch 271\n",
            "Training loss (for one batch) at step 0: 1.0479\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8660\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8704\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0693\n",
            "\n",
            "Start of epoch 272\n",
            "Training loss (for one batch) at step 0: 1.2467\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8951\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3322\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4709\n",
            "\n",
            "Start of epoch 273\n",
            "Training loss (for one batch) at step 0: 0.9013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3255\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7778\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6108\n",
            "\n",
            "Start of epoch 274\n",
            "Training loss (for one batch) at step 0: 1.1881\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9075\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5965\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6289\n",
            "\n",
            "Start of epoch 275\n",
            "Training loss (for one batch) at step 0: 1.0105\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1860\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1243\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5310\n",
            "\n",
            "Start of epoch 276\n",
            "Training loss (for one batch) at step 0: 1.0142\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4554\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0441\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4829\n",
            "\n",
            "Start of epoch 277\n",
            "Training loss (for one batch) at step 0: 0.9591\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2597\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5368\n",
            "\n",
            "Start of epoch 278\n",
            "Training loss (for one batch) at step 0: 1.0778\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3405\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8824\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6000\n",
            "\n",
            "Start of epoch 279\n",
            "Training loss (for one batch) at step 0: 1.1291\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9554\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4493\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8546\n",
            "\n",
            "Start of epoch 280\n",
            "Training loss (for one batch) at step 0: 1.0452\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2879\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8688\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6745\n",
            "\n",
            "Start of epoch 281\n",
            "Training loss (for one batch) at step 0: 1.0369\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4642\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1099\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6622\n",
            "\n",
            "Start of epoch 282\n",
            "Training loss (for one batch) at step 0: 1.5473\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1283\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8723\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6313\n",
            "\n",
            "Start of epoch 283\n",
            "Training loss (for one batch) at step 0: 0.7447\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0439\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4655\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7091\n",
            "\n",
            "Start of epoch 284\n",
            "Training loss (for one batch) at step 0: 0.8629\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9861\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0337\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4586\n",
            "\n",
            "Start of epoch 285\n",
            "Training loss (for one batch) at step 0: 0.7962\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2227\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0086\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4689\n",
            "\n",
            "Start of epoch 286\n",
            "Training loss (for one batch) at step 0: 0.8951\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9750\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6528\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5474\n",
            "\n",
            "Start of epoch 287\n",
            "Training loss (for one batch) at step 0: 1.2891\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0186\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5186\n",
            "\n",
            "Start of epoch 288\n",
            "Training loss (for one batch) at step 0: 1.3760\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0695\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0304\n",
            "\n",
            "Start of epoch 289\n",
            "Training loss (for one batch) at step 0: 1.3774\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1439\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1384\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9102\n",
            "\n",
            "Start of epoch 290\n",
            "Training loss (for one batch) at step 0: 1.4171\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3563\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0499\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8552\n",
            "\n",
            "Start of epoch 291\n",
            "Training loss (for one batch) at step 0: 1.2409\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3866\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8621\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6541\n",
            "\n",
            "Start of epoch 292\n",
            "Training loss (for one batch) at step 0: 1.0199\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8917\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8867\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6389\n",
            "\n",
            "Start of epoch 293\n",
            "Training loss (for one batch) at step 0: 1.0955\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0518\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5076\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4765\n",
            "\n",
            "Start of epoch 294\n",
            "Training loss (for one batch) at step 0: 0.8155\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8432\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6905\n",
            "\n",
            "Start of epoch 295\n",
            "Training loss (for one batch) at step 0: 0.7704\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5212\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0299\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7925\n",
            "\n",
            "Start of epoch 296\n",
            "Training loss (for one batch) at step 0: 0.9681\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9618\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0843\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4550\n",
            "\n",
            "Start of epoch 297\n",
            "Training loss (for one batch) at step 0: 0.8869\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3038\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7770\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5114\n",
            "\n",
            "Start of epoch 298\n",
            "Training loss (for one batch) at step 0: 0.9067\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4254\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5335\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4277\n",
            "\n",
            "Start of epoch 299\n",
            "Training loss (for one batch) at step 0: 0.9428\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1916\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9460\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5791\n",
            "\n",
            "Start of epoch 300\n",
            "Training loss (for one batch) at step 0: 1.5521\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3343\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4284\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4454\n",
            "\n",
            "Start of epoch 301\n",
            "Training loss (for one batch) at step 0: 1.0881\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8739\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2516\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4668\n",
            "\n",
            "Start of epoch 302\n",
            "Training loss (for one batch) at step 0: 1.3197\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8836\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7942\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4965\n",
            "\n",
            "Start of epoch 303\n",
            "Training loss (for one batch) at step 0: 0.9757\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1625\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8868\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6096\n",
            "\n",
            "Start of epoch 304\n",
            "Training loss (for one batch) at step 0: 0.8351\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8430\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1951\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4576\n",
            "\n",
            "Start of epoch 305\n",
            "Training loss (for one batch) at step 0: 1.2817\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9413\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8025\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4171\n",
            "\n",
            "Start of epoch 306\n",
            "Training loss (for one batch) at step 0: 0.9760\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0120\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0648\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3313\n",
            "\n",
            "Start of epoch 307\n",
            "Training loss (for one batch) at step 0: 0.9356\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9158\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2342\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6247\n",
            "\n",
            "Start of epoch 308\n",
            "Training loss (for one batch) at step 0: 1.1765\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0384\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0176\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9655\n",
            "\n",
            "Start of epoch 309\n",
            "Training loss (for one batch) at step 0: 1.3599\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4445\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9764\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7479\n",
            "\n",
            "Start of epoch 310\n",
            "Training loss (for one batch) at step 0: 0.6935\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2352\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8139\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5659\n",
            "\n",
            "Start of epoch 311\n",
            "Training loss (for one batch) at step 0: 0.7931\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7716\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8169\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4627\n",
            "\n",
            "Start of epoch 312\n",
            "Training loss (for one batch) at step 0: 1.0619\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9588\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5353\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4383\n",
            "\n",
            "Start of epoch 313\n",
            "Training loss (for one batch) at step 0: 0.7763\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8391\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8154\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8288\n",
            "\n",
            "Start of epoch 314\n",
            "Training loss (for one batch) at step 0: 0.8318\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0453\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4865\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5399\n",
            "\n",
            "Start of epoch 315\n",
            "Training loss (for one batch) at step 0: 1.7077\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0475\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8028\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4093\n",
            "\n",
            "Start of epoch 316\n",
            "Training loss (for one batch) at step 0: 1.2486\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8174\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0168\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6303\n",
            "\n",
            "Start of epoch 317\n",
            "Training loss (for one batch) at step 0: 1.7850\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3494\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9346\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4762\n",
            "\n",
            "Start of epoch 318\n",
            "Training loss (for one batch) at step 0: 1.3170\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5797\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4925\n",
            "\n",
            "Start of epoch 319\n",
            "Training loss (for one batch) at step 0: 1.6668\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8755\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1448\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3361\n",
            "\n",
            "Start of epoch 320\n",
            "Training loss (for one batch) at step 0: 1.1571\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9401\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0056\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9565\n",
            "\n",
            "Start of epoch 321\n",
            "Training loss (for one batch) at step 0: 1.1607\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7914\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7775\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5986\n",
            "\n",
            "Start of epoch 322\n",
            "Training loss (for one batch) at step 0: 1.0609\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8385\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1803\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7245\n",
            "\n",
            "Start of epoch 323\n",
            "Training loss (for one batch) at step 0: 1.2662\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7188\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3070\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0826\n",
            "\n",
            "Start of epoch 324\n",
            "Training loss (for one batch) at step 0: 1.4263\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3187\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4532\n",
            "\n",
            "Start of epoch 325\n",
            "Training loss (for one batch) at step 0: 0.7483\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8368\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8261\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5449\n",
            "\n",
            "Start of epoch 326\n",
            "Training loss (for one batch) at step 0: 0.9004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4105\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3080\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5508\n",
            "\n",
            "Start of epoch 327\n",
            "Training loss (for one batch) at step 0: 1.0627\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8237\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7883\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4223\n",
            "\n",
            "Start of epoch 328\n",
            "Training loss (for one batch) at step 0: 2.1038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0931\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4034\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5292\n",
            "\n",
            "Start of epoch 329\n",
            "Training loss (for one batch) at step 0: 0.9344\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6138\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4773\n",
            "\n",
            "Start of epoch 330\n",
            "Training loss (for one batch) at step 0: 0.8514\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8680\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2024\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4111\n",
            "\n",
            "Start of epoch 331\n",
            "Training loss (for one batch) at step 0: 0.9573\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7377\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8108\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7607\n",
            "\n",
            "Start of epoch 332\n",
            "Training loss (for one batch) at step 0: 0.8440\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8493\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8292\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4394\n",
            "\n",
            "Start of epoch 333\n",
            "Training loss (for one batch) at step 0: 1.2998\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1795\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5139\n",
            "\n",
            "Start of epoch 334\n",
            "Training loss (for one batch) at step 0: 0.8038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8819\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2021\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4184\n",
            "\n",
            "Start of epoch 335\n",
            "Training loss (for one batch) at step 0: 0.9033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9771\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4590\n",
            "\n",
            "Start of epoch 336\n",
            "Training loss (for one batch) at step 0: 1.2489\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2171\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9628\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6212\n",
            "\n",
            "Start of epoch 337\n",
            "Training loss (for one batch) at step 0: 1.3117\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9846\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0022\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5848\n",
            "\n",
            "Start of epoch 338\n",
            "Training loss (for one batch) at step 0: 1.0309\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0896\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8321\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7030\n",
            "\n",
            "Start of epoch 339\n",
            "Training loss (for one batch) at step 0: 1.2604\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7714\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8822\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5666\n",
            "\n",
            "Start of epoch 340\n",
            "Training loss (for one batch) at step 0: 1.3555\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9142\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3204\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4780\n",
            "\n",
            "Start of epoch 341\n",
            "Training loss (for one batch) at step 0: 0.9452\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1857\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9106\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7933\n",
            "\n",
            "Start of epoch 342\n",
            "Training loss (for one batch) at step 0: 0.9321\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2814\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0495\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3947\n",
            "\n",
            "Start of epoch 343\n",
            "Training loss (for one batch) at step 0: 0.6947\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8854\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0404\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5915\n",
            "\n",
            "Start of epoch 344\n",
            "Training loss (for one batch) at step 0: 1.3008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9751\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8316\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7120\n",
            "\n",
            "Start of epoch 345\n",
            "Training loss (for one batch) at step 0: 1.5694\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0681\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3726\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4685\n",
            "\n",
            "Start of epoch 346\n",
            "Training loss (for one batch) at step 0: 0.8617\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8994\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2093\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6318\n",
            "\n",
            "Start of epoch 347\n",
            "Training loss (for one batch) at step 0: 0.8020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9108\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9649\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5892\n",
            "\n",
            "Start of epoch 348\n",
            "Training loss (for one batch) at step 0: 0.8858\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9915\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8473\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3195\n",
            "\n",
            "Start of epoch 349\n",
            "Training loss (for one batch) at step 0: 1.1793\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7725\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0568\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3995\n",
            "\n",
            "Start of epoch 350\n",
            "Training loss (for one batch) at step 0: 0.9260\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9126\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0058\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5271\n",
            "\n",
            "Start of epoch 351\n",
            "Training loss (for one batch) at step 0: 1.0200\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9608\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9838\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4208\n",
            "\n",
            "Start of epoch 352\n",
            "Training loss (for one batch) at step 0: 1.3469\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2370\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7058\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4322\n",
            "\n",
            "Start of epoch 353\n",
            "Training loss (for one batch) at step 0: 1.0286\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1533\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2369\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6393\n",
            "\n",
            "Start of epoch 354\n",
            "Training loss (for one batch) at step 0: 0.9958\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8746\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9379\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4391\n",
            "\n",
            "Start of epoch 355\n",
            "Training loss (for one batch) at step 0: 0.9443\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8411\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6248\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7986\n",
            "\n",
            "Start of epoch 356\n",
            "Training loss (for one batch) at step 0: 1.0141\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5853\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8191\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5432\n",
            "\n",
            "Start of epoch 357\n",
            "Training loss (for one batch) at step 0: 1.0352\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9986\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3922\n",
            "\n",
            "Start of epoch 358\n",
            "Training loss (for one batch) at step 0: 1.1105\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8341\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0025\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7122\n",
            "\n",
            "Start of epoch 359\n",
            "Training loss (for one batch) at step 0: 0.7676\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2661\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4243\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5824\n",
            "\n",
            "Start of epoch 360\n",
            "Training loss (for one batch) at step 0: 1.6082\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9650\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4839\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4303\n",
            "\n",
            "Start of epoch 361\n",
            "Training loss (for one batch) at step 0: 0.7553\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7984\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9431\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8527\n",
            "\n",
            "Start of epoch 362\n",
            "Training loss (for one batch) at step 0: 0.7064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8572\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1419\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8705\n",
            "\n",
            "Start of epoch 363\n",
            "Training loss (for one batch) at step 0: 0.8843\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9499\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9149\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4940\n",
            "\n",
            "Start of epoch 364\n",
            "Training loss (for one batch) at step 0: 0.8524\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9554\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1976\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5822\n",
            "\n",
            "Start of epoch 365\n",
            "Training loss (for one batch) at step 0: 0.7740\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0169\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8673\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5601\n",
            "\n",
            "Start of epoch 366\n",
            "Training loss (for one batch) at step 0: 0.8863\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3571\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6609\n",
            "\n",
            "Start of epoch 367\n",
            "Training loss (for one batch) at step 0: 0.9122\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8877\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9339\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7216\n",
            "\n",
            "Start of epoch 368\n",
            "Training loss (for one batch) at step 0: 0.9220\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1770\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4758\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3933\n",
            "\n",
            "Start of epoch 369\n",
            "Training loss (for one batch) at step 0: 0.7540\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3350\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8264\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3792\n",
            "\n",
            "Start of epoch 370\n",
            "Training loss (for one batch) at step 0: 0.8109\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7867\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9466\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5186\n",
            "\n",
            "Start of epoch 371\n",
            "Training loss (for one batch) at step 0: 0.7762\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4308\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3699\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4059\n",
            "\n",
            "Start of epoch 372\n",
            "Training loss (for one batch) at step 0: 0.9574\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0578\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4902\n",
            "\n",
            "Start of epoch 373\n",
            "Training loss (for one batch) at step 0: 0.8424\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0171\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0934\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4815\n",
            "\n",
            "Start of epoch 374\n",
            "Training loss (for one batch) at step 0: 1.3969\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2826\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2289\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5061\n",
            "\n",
            "Start of epoch 375\n",
            "Training loss (for one batch) at step 0: 0.8135\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3979\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9623\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6053\n",
            "\n",
            "Start of epoch 376\n",
            "Training loss (for one batch) at step 0: 0.8222\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8651\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2250\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3685\n",
            "\n",
            "Start of epoch 377\n",
            "Training loss (for one batch) at step 0: 1.0083\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7517\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7785\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7546\n",
            "\n",
            "Start of epoch 378\n",
            "Training loss (for one batch) at step 0: 0.7709\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8823\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8838\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4388\n",
            "\n",
            "Start of epoch 379\n",
            "Training loss (for one batch) at step 0: 0.8053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4215\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8151\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5033\n",
            "\n",
            "Start of epoch 380\n",
            "Training loss (for one batch) at step 0: 1.6353\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3070\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1028\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6393\n",
            "\n",
            "Start of epoch 381\n",
            "Training loss (for one batch) at step 0: 1.3161\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3731\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9231\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5377\n",
            "\n",
            "Start of epoch 382\n",
            "Training loss (for one batch) at step 0: 1.2985\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0467\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7617\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4713\n",
            "\n",
            "Start of epoch 383\n",
            "Training loss (for one batch) at step 0: 1.2529\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.6950\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8039\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5734\n",
            "\n",
            "Start of epoch 384\n",
            "Training loss (for one batch) at step 0: 0.6853\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0660\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9474\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4225\n",
            "\n",
            "Start of epoch 385\n",
            "Training loss (for one batch) at step 0: 1.4494\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2833\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8030\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5194\n",
            "\n",
            "Start of epoch 386\n",
            "Training loss (for one batch) at step 0: 0.9844\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3091\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0329\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4117\n",
            "\n",
            "Start of epoch 387\n",
            "Training loss (for one batch) at step 0: 0.9929\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7668\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7766\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5210\n",
            "\n",
            "Start of epoch 388\n",
            "Training loss (for one batch) at step 0: 1.0495\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8951\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4366\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3849\n",
            "\n",
            "Start of epoch 389\n",
            "Training loss (for one batch) at step 0: 0.8150\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7588\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3384\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5475\n",
            "\n",
            "Start of epoch 390\n",
            "Training loss (for one batch) at step 0: 1.2759\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9244\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7145\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4518\n",
            "\n",
            "Start of epoch 391\n",
            "Training loss (for one batch) at step 0: 0.9998\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2555\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1481\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4176\n",
            "\n",
            "Start of epoch 392\n",
            "Training loss (for one batch) at step 0: 0.8844\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7738\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8989\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7511\n",
            "\n",
            "Start of epoch 393\n",
            "Training loss (for one batch) at step 0: 1.2058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8388\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3062\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7308\n",
            "\n",
            "Start of epoch 394\n",
            "Training loss (for one batch) at step 0: 1.0332\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9590\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3862\n",
            "\n",
            "Start of epoch 395\n",
            "Training loss (for one batch) at step 0: 0.8733\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.6966\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9346\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3214\n",
            "\n",
            "Start of epoch 396\n",
            "Training loss (for one batch) at step 0: 0.9577\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9454\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6474\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6160\n",
            "\n",
            "Start of epoch 397\n",
            "Training loss (for one batch) at step 0: 1.1357\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0481\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7554\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7418\n",
            "\n",
            "Start of epoch 398\n",
            "Training loss (for one batch) at step 0: 0.8028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3258\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7269\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8842\n",
            "\n",
            "Start of epoch 399\n",
            "Training loss (for one batch) at step 0: 1.0320\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8296\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3113\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4689\n",
            "\n",
            "Start of epoch 400\n",
            "Training loss (for one batch) at step 0: 0.7373\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8648\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3413\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9459\n",
            "\n",
            "Start of epoch 401\n",
            "Training loss (for one batch) at step 0: 0.8293\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7944\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9494\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4774\n",
            "\n",
            "Start of epoch 402\n",
            "Training loss (for one batch) at step 0: 1.0936\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3865\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5519\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6798\n",
            "\n",
            "Start of epoch 403\n",
            "Training loss (for one batch) at step 0: 0.8121\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3165\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5051\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3766\n",
            "\n",
            "Start of epoch 404\n",
            "Training loss (for one batch) at step 0: 0.9659\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0676\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8527\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4053\n",
            "\n",
            "Start of epoch 405\n",
            "Training loss (for one batch) at step 0: 0.9376\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8959\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9260\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6130\n",
            "\n",
            "Start of epoch 406\n",
            "Training loss (for one batch) at step 0: 0.8256\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1587\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0396\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5510\n",
            "\n",
            "Start of epoch 407\n",
            "Training loss (for one batch) at step 0: 1.1979\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9483\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0493\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4241\n",
            "\n",
            "Start of epoch 408\n",
            "Training loss (for one batch) at step 0: 1.5755\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8110\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0349\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6149\n",
            "\n",
            "Start of epoch 409\n",
            "Training loss (for one batch) at step 0: 1.0673\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2783\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8960\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4591\n",
            "\n",
            "Start of epoch 410\n",
            "Training loss (for one batch) at step 0: 0.8748\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0771\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1486\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4393\n",
            "\n",
            "Start of epoch 411\n",
            "Training loss (for one batch) at step 0: 1.3559\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8787\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0019\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4837\n",
            "\n",
            "Start of epoch 412\n",
            "Training loss (for one batch) at step 0: 1.3765\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1407\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8405\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4633\n",
            "\n",
            "Start of epoch 413\n",
            "Training loss (for one batch) at step 0: 0.8551\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8440\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4905\n",
            "\n",
            "Start of epoch 414\n",
            "Training loss (for one batch) at step 0: 0.8352\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8954\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5651\n",
            "\n",
            "Start of epoch 415\n",
            "Training loss (for one batch) at step 0: 1.0767\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7271\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1916\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5490\n",
            "\n",
            "Start of epoch 416\n",
            "Training loss (for one batch) at step 0: 1.1565\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0628\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9941\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6492\n",
            "\n",
            "Start of epoch 417\n",
            "Training loss (for one batch) at step 0: 0.8631\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1664\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3701\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6603\n",
            "\n",
            "Start of epoch 418\n",
            "Training loss (for one batch) at step 0: 0.7962\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9731\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7993\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5402\n",
            "\n",
            "Start of epoch 419\n",
            "Training loss (for one batch) at step 0: 1.2797\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8221\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8968\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5581\n",
            "\n",
            "Start of epoch 420\n",
            "Training loss (for one batch) at step 0: 1.3723\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0015\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0712\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4597\n",
            "\n",
            "Start of epoch 421\n",
            "Training loss (for one batch) at step 0: 0.8681\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0265\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5028\n",
            "\n",
            "Start of epoch 422\n",
            "Training loss (for one batch) at step 0: 0.7599\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9591\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1039\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5562\n",
            "\n",
            "Start of epoch 423\n",
            "Training loss (for one batch) at step 0: 0.9804\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8483\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0400\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4336\n",
            "\n",
            "Start of epoch 424\n",
            "Training loss (for one batch) at step 0: 0.9186\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2988\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7747\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6552\n",
            "\n",
            "Start of epoch 425\n",
            "Training loss (for one batch) at step 0: 0.8496\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0683\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6814\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3538\n",
            "\n",
            "Start of epoch 426\n",
            "Training loss (for one batch) at step 0: 1.2274\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9068\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7022\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4882\n",
            "\n",
            "Start of epoch 427\n",
            "Training loss (for one batch) at step 0: 0.7448\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8203\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9820\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6805\n",
            "\n",
            "Start of epoch 428\n",
            "Training loss (for one batch) at step 0: 0.7413\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9190\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7201\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3639\n",
            "\n",
            "Start of epoch 429\n",
            "Training loss (for one batch) at step 0: 0.9705\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0095\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6593\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5751\n",
            "\n",
            "Start of epoch 430\n",
            "Training loss (for one batch) at step 0: 1.4040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9307\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8987\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5998\n",
            "\n",
            "Start of epoch 431\n",
            "Training loss (for one batch) at step 0: 1.1617\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9247\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9539\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7305\n",
            "\n",
            "Start of epoch 432\n",
            "Training loss (for one batch) at step 0: 1.3169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8782\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7750\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4744\n",
            "\n",
            "Start of epoch 433\n",
            "Training loss (for one batch) at step 0: 0.8325\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2686\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8394\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5023\n",
            "\n",
            "Start of epoch 434\n",
            "Training loss (for one batch) at step 0: 1.3786\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7216\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3066\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5023\n",
            "\n",
            "Start of epoch 435\n",
            "Training loss (for one batch) at step 0: 1.0829\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0270\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8815\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4634\n",
            "\n",
            "Start of epoch 436\n",
            "Training loss (for one batch) at step 0: 1.6227\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9300\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8012\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5240\n",
            "\n",
            "Start of epoch 437\n",
            "Training loss (for one batch) at step 0: 1.0252\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9961\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2345\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3943\n",
            "\n",
            "Start of epoch 438\n",
            "Training loss (for one batch) at step 0: 0.7571\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4645\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2270\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6196\n",
            "\n",
            "Start of epoch 439\n",
            "Training loss (for one batch) at step 0: 0.9756\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8640\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1907\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6472\n",
            "\n",
            "Start of epoch 440\n",
            "Training loss (for one batch) at step 0: 1.6593\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0710\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4356\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4209\n",
            "\n",
            "Start of epoch 441\n",
            "Training loss (for one batch) at step 0: 0.8664\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8881\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5485\n",
            "\n",
            "Start of epoch 442\n",
            "Training loss (for one batch) at step 0: 0.9456\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0884\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7464\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7749\n",
            "\n",
            "Start of epoch 443\n",
            "Training loss (for one batch) at step 0: 1.1163\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8467\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9668\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4706\n",
            "\n",
            "Start of epoch 444\n",
            "Training loss (for one batch) at step 0: 0.9600\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3602\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4967\n",
            "\n",
            "Start of epoch 445\n",
            "Training loss (for one batch) at step 0: 0.9594\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7805\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2091\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7793\n",
            "\n",
            "Start of epoch 446\n",
            "Training loss (for one batch) at step 0: 0.7919\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8236\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9283\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5700\n",
            "\n",
            "Start of epoch 447\n",
            "Training loss (for one batch) at step 0: 1.2169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9487\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8067\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5539\n",
            "\n",
            "Start of epoch 448\n",
            "Training loss (for one batch) at step 0: 0.9871\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1798\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4135\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7990\n",
            "\n",
            "Start of epoch 449\n",
            "Training loss (for one batch) at step 0: 1.1013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7693\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3092\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5223\n",
            "\n",
            "Start of epoch 450\n",
            "Training loss (for one batch) at step 0: 0.7017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1798\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7513\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4670\n",
            "\n",
            "Start of epoch 451\n",
            "Training loss (for one batch) at step 0: 1.6727\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3647\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1139\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5472\n",
            "\n",
            "Start of epoch 452\n",
            "Training loss (for one batch) at step 0: 0.8779\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8792\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0518\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5206\n",
            "\n",
            "Start of epoch 453\n",
            "Training loss (for one batch) at step 0: 1.1008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8253\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8709\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5249\n",
            "\n",
            "Start of epoch 454\n",
            "Training loss (for one batch) at step 0: 1.3632\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7194\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4412\n",
            "\n",
            "Start of epoch 455\n",
            "Training loss (for one batch) at step 0: 0.7931\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8169\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8280\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4599\n",
            "\n",
            "Start of epoch 456\n",
            "Training loss (for one batch) at step 0: 1.1762\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8063\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4251\n",
            "\n",
            "Start of epoch 457\n",
            "Training loss (for one batch) at step 0: 1.1838\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0163\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4447\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6680\n",
            "\n",
            "Start of epoch 458\n",
            "Training loss (for one batch) at step 0: 0.8646\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0083\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1467\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4090\n",
            "\n",
            "Start of epoch 459\n",
            "Training loss (for one batch) at step 0: 1.1791\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8845\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8083\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7729\n",
            "\n",
            "Start of epoch 460\n",
            "Training loss (for one batch) at step 0: 0.8874\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9333\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7299\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7430\n",
            "\n",
            "Start of epoch 461\n",
            "Training loss (for one batch) at step 0: 1.0278\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2483\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8600\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5126\n",
            "\n",
            "Start of epoch 462\n",
            "Training loss (for one batch) at step 0: 1.2094\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5749\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9662\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4319\n",
            "\n",
            "Start of epoch 463\n",
            "Training loss (for one batch) at step 0: 0.7435\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8140\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1185\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4025\n",
            "\n",
            "Start of epoch 464\n",
            "Training loss (for one batch) at step 0: 1.4946\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8149\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2015\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8090\n",
            "\n",
            "Start of epoch 465\n",
            "Training loss (for one batch) at step 0: 0.9523\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8362\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2080\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5269\n",
            "\n",
            "Start of epoch 466\n",
            "Training loss (for one batch) at step 0: 1.0166\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7889\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8759\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7356\n",
            "\n",
            "Start of epoch 467\n",
            "Training loss (for one batch) at step 0: 0.7324\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0262\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4866\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7496\n",
            "\n",
            "Start of epoch 468\n",
            "Training loss (for one batch) at step 0: 1.0247\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7137\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2722\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0872\n",
            "\n",
            "Start of epoch 469\n",
            "Training loss (for one batch) at step 0: 0.8885\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8173\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7386\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4750\n",
            "\n",
            "Start of epoch 470\n",
            "Training loss (for one batch) at step 0: 1.0104\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7702\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1307\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5769\n",
            "\n",
            "Start of epoch 471\n",
            "Training loss (for one batch) at step 0: 0.6914\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7885\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7867\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5110\n",
            "\n",
            "Start of epoch 472\n",
            "Training loss (for one batch) at step 0: 0.9141\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7863\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9278\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5305\n",
            "\n",
            "Start of epoch 473\n",
            "Training loss (for one batch) at step 0: 1.4471\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9948\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5730\n",
            "\n",
            "Start of epoch 474\n",
            "Training loss (for one batch) at step 0: 1.1776\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2164\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9575\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3267\n",
            "\n",
            "Start of epoch 475\n",
            "Training loss (for one batch) at step 0: 0.8928\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9712\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1604\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7247\n",
            "\n",
            "Start of epoch 476\n",
            "Training loss (for one batch) at step 0: 0.8872\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1349\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9932\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4781\n",
            "\n",
            "Start of epoch 477\n",
            "Training loss (for one batch) at step 0: 0.8591\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7325\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1852\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9865\n",
            "\n",
            "Start of epoch 478\n",
            "Training loss (for one batch) at step 0: 1.4980\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8211\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1686\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4542\n",
            "\n",
            "Start of epoch 479\n",
            "Training loss (for one batch) at step 0: 1.1310\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7257\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6682\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5316\n",
            "\n",
            "Start of epoch 480\n",
            "Training loss (for one batch) at step 0: 0.8583\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0809\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0329\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4696\n",
            "\n",
            "Start of epoch 481\n",
            "Training loss (for one batch) at step 0: 1.0552\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9542\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3063\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4584\n",
            "\n",
            "Start of epoch 482\n",
            "Training loss (for one batch) at step 0: 0.7814\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0857\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8331\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3449\n",
            "\n",
            "Start of epoch 483\n",
            "Training loss (for one batch) at step 0: 1.1423\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8394\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3498\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4840\n",
            "\n",
            "Start of epoch 484\n",
            "Training loss (for one batch) at step 0: 0.9551\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4149\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6639\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5223\n",
            "\n",
            "Start of epoch 485\n",
            "Training loss (for one batch) at step 0: 1.0199\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7381\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8554\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6488\n",
            "\n",
            "Start of epoch 486\n",
            "Training loss (for one batch) at step 0: 0.8589\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4978\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1224\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5936\n",
            "\n",
            "Start of epoch 487\n",
            "Training loss (for one batch) at step 0: 0.7976\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3701\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7310\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6519\n",
            "\n",
            "Start of epoch 488\n",
            "Training loss (for one batch) at step 0: 1.0164\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5527\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8023\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4310\n",
            "\n",
            "Start of epoch 489\n",
            "Training loss (for one batch) at step 0: 0.6715\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7429\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7559\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4767\n",
            "\n",
            "Start of epoch 490\n",
            "Training loss (for one batch) at step 0: 1.0626\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0284\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9894\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5293\n",
            "\n",
            "Start of epoch 491\n",
            "Training loss (for one batch) at step 0: 0.7467\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3265\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7086\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8584\n",
            "\n",
            "Start of epoch 492\n",
            "Training loss (for one batch) at step 0: 0.7196\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8378\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8649\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3725\n",
            "\n",
            "Start of epoch 493\n",
            "Training loss (for one batch) at step 0: 0.9109\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9283\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0492\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5298\n",
            "\n",
            "Start of epoch 494\n",
            "Training loss (for one batch) at step 0: 0.8318\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1598\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8873\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5636\n",
            "\n",
            "Start of epoch 495\n",
            "Training loss (for one batch) at step 0: 1.1732\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8670\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9675\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4712\n",
            "\n",
            "Start of epoch 496\n",
            "Training loss (for one batch) at step 0: 0.9016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2386\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9913\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4858\n",
            "\n",
            "Start of epoch 497\n",
            "Training loss (for one batch) at step 0: 0.8975\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0459\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6495\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4687\n",
            "\n",
            "Start of epoch 498\n",
            "Training loss (for one batch) at step 0: 1.1626\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2141\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2634\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5047\n",
            "\n",
            "Start of epoch 499\n",
            "Training loss (for one batch) at step 0: 0.7638\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8932\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8958\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UkV66dgmlyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "f1, f2, f3 = prediction.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(f1, f2, f3, 'o')"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZKPk2l1krmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (3,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(3, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "epochs = 2"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqPagbGMkovO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ec41d46-ac34-45d2-ca77-cd20b6cfce6a"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # print(linear)\n",
        "      # compute the loss value for this minibatch\n",
        "      # mse = Keras_loss_function(y_batch_train, linear)\n",
        "      mse = tf.keras.losses.MSE(y_batch_train, linear)\n",
        "      reconstruction_error = tf.reduce_mean(mse)\n",
        "      \n",
        "      print(mse)\n",
        "      # mse = keras.losses.mean_squared_error(y_batch_train, linear) # mse gives 3 number\n",
        "    # print(mse)\n",
        "    # use the gradient tap to automatically retrieve the gradients of the \n",
        "    # trainable variables with respect to the loss\n",
        "    grads = tape.gradient(mse, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training loss (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(reconstruction_error))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    val_mse = tf.keras.losses.MSE(y_batch_val, val_linear)\n",
        "    val_reconstruction_error = tf.reduce_mean(val_mse)\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MSE: %.4f\" % (float(val_reconstruction_error)))\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "tf.Tensor(\n",
            "[0.3842297  0.42797565 0.3472961  0.27937302 0.33427966 0.16493267\n",
            " 0.4086562  0.33521208 0.47289905 1.0540316  0.6916525  0.3622521\n",
            " 0.39382276 0.5528478  0.55679005 0.43935475 0.37818182 0.3618947\n",
            " 0.521801   0.7617163  0.35995445 0.4703331  0.40040436 0.5288081\n",
            " 0.7809743  0.37247488 0.5264825  0.4781715  0.83909756 0.34608588\n",
            " 0.3736016  0.25681335 0.2610033  0.45814824 0.41596866 0.17603369\n",
            " 0.37793037 0.3476676  0.26313162 0.7729811  0.476038   0.6464086\n",
            " 0.41478404 0.8852212  0.48054478 0.6424247  0.18614174 0.36385384\n",
            " 0.41569534 0.16256386 0.61835307 0.3670615  0.56214184 0.4034245\n",
            " 0.47499433 0.4112301  0.54172355 0.1362366  0.420256   0.07681898\n",
            " 0.7463608  1.0133843  0.3815819  0.28662837 0.60999894 0.57684857\n",
            " 0.31372806 0.5327858  0.281065   0.45715943 0.24788512 0.39179942\n",
            " 0.33333334 0.48476732 0.35954127 0.3360711  0.3392143  0.5164711\n",
            " 0.20567058 0.2057393  0.42267242 0.23170583 0.33772638 0.22380936\n",
            " 0.38552478 0.40180376 0.21150778 0.35532656 0.4767847  0.46774587\n",
            " 0.40611672 0.5190676  0.38034818 1.6588454  0.35808685 0.5656989\n",
            " 0.32684463 0.347855   0.5492076  0.5368646 ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 0: 0.4450\n",
            "seen so far: 100 samples\n",
            "tf.Tensor(\n",
            "[0.33860537 0.41040483 0.48373207 0.33541802 0.47145763 0.3492386\n",
            " 0.46276578 0.3469976  0.37813625 0.38515183 0.42063832 0.45781818\n",
            " 0.5603331  0.09026703 0.28138673 0.36046323 0.27292114 0.51712584\n",
            " 0.4488442  0.26163995 0.31794998 0.46203247 0.14701332 0.3603612\n",
            " 0.33832398 0.15801458 0.5733414  0.348163   0.4345752  0.33633122\n",
            " 0.5036835  0.36150765 0.20709808 0.8765311  0.29300377 0.33649907\n",
            " 0.32682645 0.8774343  0.15217887 0.45081353 0.37289107 0.4008452\n",
            " 0.3402873  0.21822572 0.26459107 0.16481005 0.42958677 0.40201747\n",
            " 0.298101   0.36909544 0.6067802  0.33647582 0.38005757 1.0637578\n",
            " 0.42574596 0.86553836 0.34986925 0.33925804 0.21397112 0.76335615\n",
            " 0.4631168  0.48089024 0.45570305 0.2862741  0.4776434  0.21954946\n",
            " 0.44486538 0.40049195 0.4243425  0.48926648 0.48389956 0.37723914\n",
            " 0.35537207 0.3527999  0.4002842  0.4848771  0.7268116  0.38477603\n",
            " 0.210315   0.5238376  0.15521008 0.36547065 0.2937604  0.39938772\n",
            " 0.15684216 0.616399   0.3907454  0.23303841 0.221061   0.38182923\n",
            " 0.34628275 1.0876843  0.50028425 0.20555873 0.66256684 0.38983345\n",
            " 0.47678408 0.40423474 0.24957202 0.3520956 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.29152432 0.3233721  0.13705064 0.7551074  0.34089518 0.38406906\n",
            " 0.32917568 0.58325094 0.6762213  0.15430054 0.3481998  0.2750775\n",
            " 0.3565358  0.6689251  0.1349848  0.31611362 0.36076617 0.21965432\n",
            " 0.4221066  0.3578694  0.52965456 0.44622853 0.36026847 0.1767257\n",
            " 0.33140525 0.32598022 0.6832847  0.4423437  0.44871172 0.33427373\n",
            " 0.50595737 0.34445098 0.31876943 0.22113378 0.41460505 0.427222\n",
            " 0.66325665 0.25512716 0.42291987 0.2891665  0.5147335  0.354877\n",
            " 0.48030153 0.3628141  0.3559177  0.43480864 0.32507205 0.8571439\n",
            " 0.50447065 0.34010923 0.35244584 0.31774402 0.36836326 0.3328918\n",
            " 0.37989524 0.49973473 0.21609624 0.44154337 0.29927945 0.47681364\n",
            " 0.36677876 0.56768316 0.35133287 0.531841   0.56575507 0.33775818\n",
            " 0.4209367  0.21335632 0.36291084 0.3265238  0.43359384 0.33860376\n",
            " 0.43042126 0.34266153 0.08904785 0.54525936 0.47716907 0.19346558\n",
            " 0.5428558  0.42337263 0.5542352  0.36558667 0.49470648 0.3424088\n",
            " 0.33048424 0.40352002 0.40909854 0.54604733 0.28603986 0.4485502\n",
            " 0.33456084 0.34096074 0.38397405 0.43466106 1.5555377  0.40743902\n",
            " 1.4628962  0.3162957  0.45734012 0.3377998 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.28318462 0.401673   0.13908519 0.31875578 0.3217967  0.18928666\n",
            " 0.33611917 0.34430727 0.34520265 0.3192033  0.38713858 0.33224156\n",
            " 0.19869792 0.32757887 0.2679485  0.33568704 0.12757322 0.3467072\n",
            " 0.28354523 0.49371478 0.39409292 0.3078964  0.38514245 0.23425718\n",
            " 0.33313212 0.42288363 0.07868618 0.5823947  0.33471116 0.5458235\n",
            " 0.47892368 0.322184   0.21033865 0.3391601  0.560061   0.37789145\n",
            " 0.42036656 0.3891503  0.3390735  0.5285423  0.38732564 0.19867122\n",
            " 0.3463936  0.44539198 0.33046493 0.3250334  0.37361374 0.42149124\n",
            " 0.36662826 0.43403038 0.3923709  0.34364018 0.39686707 0.3436644\n",
            " 0.2908488  0.56065404 0.18323909 0.4539996  0.40700746 0.25386265\n",
            " 0.547029   0.36603415 0.34087506 0.37716976 0.3509071  0.19781764\n",
            " 0.38281384 0.10220256 0.9254668  0.20705914 0.38527408 0.3434782\n",
            " 0.3754398  0.40569648 0.655157   0.33498192 0.54593617 0.3424355\n",
            " 0.43429923 0.48092857 0.38116562 0.29646543 0.3918167  0.18564002\n",
            " 0.49099353 0.32984874 0.49954614 0.3212475  0.3652924  0.38029277\n",
            " 0.47627833 0.5438964  0.94633454 0.34116924 0.43481383 0.594202\n",
            " 0.5726839  0.34449705 0.42033967 0.34761918], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.36479703 0.7680208  0.39052573 0.5850437  0.24678463 0.2805087\n",
            " 0.6381304  0.7198604  0.3373996  0.3713905  0.27557495 0.34028196\n",
            " 0.34641376 0.4339594  0.24646421 0.34682956 0.3497517  0.3382124\n",
            " 0.41500232 0.21743159 0.34860948 0.30904448 0.28564999 0.3341032\n",
            " 0.3449494  0.2083264  0.5284645  0.34286967 0.33418044 0.32979655\n",
            " 0.24534105 0.6422638  0.33207396 0.19303834 0.8241555  0.47107843\n",
            " 0.51415354 0.21926261 0.3522004  0.14978655 0.34031752 0.8636196\n",
            " 0.3539501  0.7644763  0.44225731 0.37046626 0.40592337 0.3369093\n",
            " 0.24946189 0.31013772 0.20059876 0.2935204  0.16897418 0.5394059\n",
            " 0.35958812 0.4462502  0.21144663 0.44009137 0.3840337  0.41190633\n",
            " 0.38709506 0.39659503 0.38236508 0.48395625 0.2960538  0.47936383\n",
            " 0.32475045 0.5442054  0.29057527 0.37892938 0.42641315 0.58221\n",
            " 0.3497555  0.4765422  0.4534469  0.354197   0.36141133 0.22210272\n",
            " 0.5372376  0.1992395  0.12595765 0.34308004 0.36940417 0.3433002\n",
            " 0.28238046 0.2847316  0.34197536 0.12924929 0.34051016 0.39327565\n",
            " 0.4690628  0.33421257 0.4025921  0.56337184 0.41138878 0.35470316\n",
            " 0.26187548 0.35324803 0.37458658 0.40019867], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.33299348 0.34766078 0.35073122 1.039867   0.34316134 0.14037597\n",
            " 0.4022998  0.4162738  0.28016987 0.3580291  0.10929954 0.3251094\n",
            " 0.3419079  0.39452192 0.41266575 0.33764803 0.3510331  0.3510716\n",
            " 0.34500894 0.26614782 0.3849074  0.27073678 0.4129386  0.35264564\n",
            " 0.35324168 0.3450705  0.2948717  0.35821438 0.36778986 0.22274512\n",
            " 0.32088348 0.34103826 0.3102553  0.329868   0.26212865 0.16584499\n",
            " 0.30236587 0.34565952 0.41835546 0.4439373  0.3354818  0.3235592\n",
            " 0.38401    0.35623923 0.32063818 0.34819278 0.3377924  0.22799258\n",
            " 0.83690447 0.34957287 0.36310136 0.36216065 0.33535758 0.31379086\n",
            " 0.20011495 0.375716   0.07764735 0.13229452 0.34084842 0.34211683\n",
            " 0.3967187  0.3099968  0.16968083 0.24635988 0.4253602  0.23808591\n",
            " 0.33114538 0.558217   0.4247626  0.14247154 0.34569195 0.35400596\n",
            " 0.37147436 0.3523489  0.43949652 0.47408772 0.37984434 0.08132473\n",
            " 0.2179149  0.2977538  0.20160933 0.32961443 0.10687136 0.17839813\n",
            " 0.38436186 0.87266535 0.38569817 0.2961361  0.3579836  0.3785585\n",
            " 0.36578247 0.42422572 0.36369693 0.64489174 0.36969137 0.29083893\n",
            " 0.39458176 0.28632703 0.0956659  0.3756474 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.35162473 0.27470377 0.38727543 0.40015456 0.27356613 0.3442669\n",
            " 0.3322688  0.33290204 0.43995777 0.35583213 0.35837865 0.21290426\n",
            " 0.28384316 0.25786963 0.41519126 0.32032108 0.37899622 0.19962953\n",
            " 0.3211309  0.37739214 0.31819186 0.35486507 0.30338216 0.34549618\n",
            " 0.27450132 0.40810642 0.32473382 0.34061813 0.4753978  0.3415061\n",
            " 0.34177694 0.3580489  0.35367072 0.3349214  0.15647776 0.2903227\n",
            " 0.34560072 0.40749708 0.24014932 0.30344942 0.24531214 0.32055223\n",
            " 0.36529675 0.4013507  0.34342647 0.32537994 0.35906267 0.5894883\n",
            " 0.316673   0.4111867  0.13513616 0.29687557 0.14405863 0.36149868\n",
            " 0.3537192  0.28611448 0.35897052 0.55801976 0.34373593 0.3356603\n",
            " 0.36259887 0.30754527 0.3795717  0.4992133  0.36305287 0.5335761\n",
            " 0.31395096 0.3207828  0.3397441  0.34015557 0.41417518 0.3523886\n",
            " 0.21524926 0.35270187 0.30758214 0.5694886  0.21051015 0.39728966\n",
            " 0.30510342 0.30744165 0.3371625  0.36203137 0.48383275 0.35023427\n",
            " 0.1973873  0.340323   0.34272763 0.6334752  0.3369259  0.37891793\n",
            " 0.346248   0.31795862 0.27040634 0.75140125 0.19667935 0.4908402\n",
            " 0.50517535 0.35266724 0.37160775 0.32099232], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.36673483 0.28527582 0.2980406  0.41611052 0.3696483  0.24737345\n",
            " 0.28314316 0.42966557 0.24397688 0.36961198 0.36368242 0.35794184\n",
            " 0.35775414 0.32390913 0.59346354 0.38881323 0.37482837 0.22639436\n",
            " 0.3400289  0.481513   0.3601997  0.4132885  0.37805584 0.30555838\n",
            " 0.34687057 0.34786272 0.31006023 0.3302621  0.26223573 0.34281144\n",
            " 0.3675406  0.57567257 0.2933918  0.36863947 0.3401974  0.18469654\n",
            " 0.4367089  0.2695022  0.23089288 0.36361805 0.28551435 0.2783998\n",
            " 0.15753521 0.35827184 0.34504136 0.3414662  0.26248544 0.16004036\n",
            " 0.10380874 0.30834666 0.35203317 0.31872293 0.31551185 0.5070054\n",
            " 0.34778515 0.28517127 0.28655684 0.2593713  0.6627396  0.5029884\n",
            " 0.3296826  0.5026198  0.19734687 0.36074212 0.11928969 0.3604797\n",
            " 0.16915856 0.32166    0.36339197 0.49195564 0.34708384 0.34248564\n",
            " 0.33956742 0.3865156  0.33837947 0.28677064 0.29182604 0.18323736\n",
            " 0.32959148 0.30778876 0.3348308  0.20408969 0.32248372 0.35309386\n",
            " 0.36090687 0.38818774 0.3051309  0.20301133 0.33459404 0.33626512\n",
            " 0.3075248  0.29782453 0.30338505 0.3005467  0.28678006 0.21022652\n",
            " 0.40975496 0.32075024 0.45192674 0.338858  ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.2799919  0.3433522  0.35423675 0.33659473 0.06014466 0.3535144\n",
            " 0.30666628 0.36865214 0.35031167 0.36830446 0.17436342 0.31843537\n",
            " 0.37068248 0.3486707  0.31602353 0.35064185 0.33956465 0.3058185\n",
            " 0.09961583 0.5102458  0.31886068 0.31123972 0.36899176 0.3477162\n",
            " 0.32849583 0.15567212 0.3220156  0.29143426 0.27703688 0.36721215\n",
            " 0.2766848  0.28136137 0.36223975 0.3852546  0.27071854 0.2675875\n",
            " 0.37405622 0.23712474 0.3367881  0.1215502  0.36814    0.43975404\n",
            " 0.26930127 0.33729294 0.353676   0.3244782  0.27934995 0.26430824\n",
            " 0.3770313  0.31177324 0.28009066 0.3890896  0.2667968  0.3128083\n",
            " 0.34462574 0.15333669 0.3584713  0.33556136 0.29186454 0.4366397\n",
            " 0.31274807 0.3918287  0.2599719  0.30514428 0.4142312  0.35322425\n",
            " 0.278739   0.353825   0.28266588 0.34256935 0.25851253 0.24076845\n",
            " 0.25094557 0.36666045 0.31899348 0.26694208 0.21589291 0.07612073\n",
            " 0.32031587 0.2331603  0.38829234 0.35331905 0.26742348 0.37241563\n",
            " 0.2971698  0.3498559  0.29411155 0.3342098  0.4159862  0.35912275\n",
            " 0.40698016 0.36188254 0.36080444 0.35791937 0.29378107 0.41792372\n",
            " 0.3503283  0.30921972 0.36850068 0.51692015], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.22131492 0.38821253 0.3604989  0.3175108  0.36168262 0.30705604\n",
            " 0.337715   0.31452498 0.36701488 0.27662802 0.28024146 0.3716818\n",
            " 0.3603491  0.351657   0.2607252  0.3384039  0.31399682 0.2727524\n",
            " 0.135605   0.27479157 0.32083076 0.24283426 0.32635322 0.35371137\n",
            " 0.37152684 0.3847586  0.33451006 0.3078226  0.30451432 0.3497498\n",
            " 0.32923833 0.32635024 0.45178172 0.22755402 0.30769244 0.26453483\n",
            " 0.37273097 0.37662888 0.28134575 0.24684606 0.29841724 0.3603066\n",
            " 0.3108292  0.33653113 0.25915816 0.35195997 0.27680948 0.2973177\n",
            " 0.3356905  0.3452311  0.31607082 0.3383911  0.28466377 0.32576528\n",
            " 0.3564837  0.30770636 0.280396   0.2800002  0.39334455 0.3639116\n",
            " 0.3635541  0.2894334  0.26365522 0.33390418 0.3294582  0.42845544\n",
            " 0.28371307 0.26356885 0.3842573  0.32132652 0.18483482 0.33661696\n",
            " 0.3396337  0.3284659  0.33010694 0.2713847  0.495996   0.29437253\n",
            " 0.26849273 0.27792928 0.37134132 0.26993755 0.1294331  0.3737929\n",
            " 0.37940657 0.29070652 0.34341195 0.28146377 0.36366597 0.16506658\n",
            " 0.2208661  0.3207397  0.27672452 0.3710829  0.3724749  0.30772695\n",
            " 0.40066925 0.29312262 0.3560617  0.29665896], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.23923405 0.43167862 0.26871106 0.16908579 0.3568894  0.36826363\n",
            " 0.28987956 0.36387345 0.33328503 0.31078085 0.35023192 0.28920376\n",
            " 0.21538341 0.27764285 0.38147104 0.15858702 0.3604919  0.30237737\n",
            " 0.29941747 0.323021   0.28151244 0.24814898 0.19540851 0.33324423\n",
            " 0.07399183 0.28819457 0.20476724 0.25104877 0.28109506 0.2967434\n",
            " 0.31538066 0.34878823 0.44300103 0.297784   0.3346604  0.3036486\n",
            " 0.28030252 0.42320383 0.14463076 0.25609845 0.3689867  0.24860741\n",
            " 0.2778025  0.29478118 0.30015048 0.3639984  0.27358702 0.43162528\n",
            " 0.3231435  0.3852464  0.02803682 0.23246926 0.19514853 0.05272064\n",
            " 0.29090592 0.2573625  0.320873   0.38342956 0.25669202 0.33366886\n",
            " 0.37135103 0.28030428 0.32032314 0.1360559  0.21752109 0.29154608\n",
            " 0.30862698 0.2868385  0.27823263 0.29988745 0.30750522 0.24726824\n",
            " 0.20788603 0.197807   0.27760383 0.29728118 0.48043606 0.30844972\n",
            " 0.2876789  0.30425695 0.14233257 0.19092686 0.27597764 0.2751819\n",
            " 0.10031665 0.31047568 0.34494635 0.27960065 0.27863464 0.2680494\n",
            " 0.33747876 0.32948574 0.24073939 0.27468583 0.38045263 0.4051981\n",
            " 0.29064503 0.20718129 0.3444945  0.3051335 ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 10: 0.2860\n",
            "seen so far: 1100 samples\n",
            "tf.Tensor(\n",
            "[0.28896827 0.18377559 0.27005562 0.29697183 0.356186   0.22489923\n",
            " 0.331864   0.37530577 0.29366317 0.22447167 0.58997357 0.29026875\n",
            " 0.24698885 0.2923431  0.3112485  0.33719158 0.34648314 0.3328317\n",
            " 0.24244069 0.2562863  0.3367093  0.34576336 0.21974094 0.33198932\n",
            " 0.36843607 0.31976137 0.30052015 0.28279078 0.11944842 0.25076565\n",
            " 0.33524594 0.33446303 0.342011   0.33336964 0.24224758 0.25098586\n",
            " 0.3461883  0.23080963 0.3085197  0.37730107 0.28225562 0.38059244\n",
            " 0.29004732 0.21401608 0.34589925 0.33092538 0.2566547  0.23428945\n",
            " 0.32929716 0.14023946 0.26974997 0.16437632 0.2713466  0.47651914\n",
            " 0.33686122 0.3843179  0.2653737  0.3220872  0.22170214 0.273645\n",
            " 0.24297774 0.17439348 0.25838488 0.35460064 0.32996485 0.30259857\n",
            " 0.23562343 0.2729177  0.2614543  0.3158305  0.31948084 0.4260838\n",
            " 0.31771302 0.34631887 0.34803352 0.36586782 0.39252463 0.28118125\n",
            " 0.32977614 0.358554   0.37840405 0.3722254  0.24857302 0.25415245\n",
            " 0.32314548 0.2927304  0.19427669 0.31295338 0.361959   0.28466025\n",
            " 0.36825135 0.15722175 0.243756   0.28970245 0.5106651  0.39126644\n",
            " 0.26358736 0.21391742 0.26149392 0.14834173], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.23696004 0.32690254 0.25778756 0.34244177 0.28681016 0.3094726\n",
            " 0.28743148 0.27890146 0.29454497 0.23455207 0.30408713 0.31401375\n",
            " 0.39477614 0.23979662 0.28936514 0.34231713 0.2942169  0.55700886\n",
            " 0.33127224 0.2650075  0.32399806 0.18708141 0.22000362 0.41248903\n",
            " 0.24901567 0.27384853 0.15352559 0.2569198  0.2826409  0.27231586\n",
            " 0.24673323 0.3298781  0.30710098 0.30125004 0.29648018 0.25528288\n",
            " 0.19141363 0.06896492 0.26776418 0.2708753  0.31614026 0.26379368\n",
            " 0.26872256 0.29396182 0.3335271  0.32977697 0.2137994  0.2478928\n",
            " 0.30179983 0.30760923 0.25449643 0.23860908 0.24392302 0.24713093\n",
            " 0.23576547 0.36760232 0.23174386 0.31202355 0.35331497 0.38919365\n",
            " 0.28394642 0.3634454  0.3955301  0.2957924  0.20517941 0.301151\n",
            " 0.26248136 0.27742818 0.31890935 0.26781186 0.2605305  0.32571235\n",
            " 0.30538237 0.24851258 0.30596706 0.35831392 0.11175701 0.3286778\n",
            " 0.17257452 0.3357928  0.30100235 0.17594393 0.2783067  0.17165482\n",
            " 0.27326977 0.34126535 0.27835885 0.2719891  0.28605917 0.20277023\n",
            " 0.3616831  0.31726506 0.58742243 0.27343437 0.30354738 0.16523248\n",
            " 0.34375966 0.3038285  0.33178613 0.16623943], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.2685776  0.32618877 0.3249875  0.11446599 0.32999837 0.26023212\n",
            " 0.25093696 0.3172836  0.30831257 0.2761821  0.28273883 0.06830766\n",
            " 0.04342751 0.05556902 0.2747275  0.33743775 0.33672443 0.17521377\n",
            " 0.2670023  0.07991254 0.15319704 0.28685462 0.08886545 0.3115206\n",
            " 0.19382901 0.24978943 0.41486037 0.38398412 0.33366635 0.28283566\n",
            " 0.12353293 0.09854249 0.2767789  0.37031913 0.31082124 0.31187943\n",
            " 0.23953122 0.12802781 0.2530872  0.27224538 0.30205122 0.29981792\n",
            " 0.14017612 0.34522924 0.28378192 0.30021596 0.25377855 0.32234967\n",
            " 0.2770259  0.33713555 0.16024187 0.18009402 0.25516674 0.30105367\n",
            " 0.37853846 0.33283257 0.2987595  0.3469651  0.19645976 0.28955454\n",
            " 0.2423399  0.2859172  0.32615098 0.36354363 0.23641753 0.37241706\n",
            " 0.28166893 0.23622374 0.18219166 0.26048806 0.2303143  0.12238446\n",
            " 0.30174184 0.34594932 0.3704017  0.26562744 0.32192582 0.30739045\n",
            " 0.3156833  0.28402606 0.29879618 0.37288105 0.33251876 0.03968087\n",
            " 0.2967293  0.20007099 0.27721566 0.34025702 0.32315505 0.27150515\n",
            " 0.33754846 0.29916337 0.37292466 0.2824492  0.3163289  0.37773585\n",
            " 0.21061029 0.33140412 0.26379335 0.32768714], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.37390468 0.31336483 0.35865283 0.15390667 0.15047626 0.32166103\n",
            " 0.11441863 0.11874525 0.1854911  0.23841558 0.23349933 0.25924763\n",
            " 0.2908295  0.05824827 0.34697375 0.32483768 0.1981129  0.25992933\n",
            " 0.38642702 0.3925395  0.21787809 0.25616163 0.26646075 0.23119497\n",
            " 0.30468526 0.26187792 0.27819052 0.23235424 0.1999281  0.3284382\n",
            " 0.27859142 0.30242196 0.36099425 0.3116388  0.22106875 0.2908775\n",
            " 0.24910994 0.28648806 0.35629153 0.33678266 0.22700195 0.21730177\n",
            " 0.421618   0.24822426 0.24700426 0.21967687 0.27492177 0.37852868\n",
            " 0.26453397 0.18562062 0.33828554 0.21870004 0.26627597 0.31894478\n",
            " 0.43073288 0.30633903 0.28878033 0.32853517 0.14598542 0.24843852\n",
            " 0.36494732 0.22304732 0.23963802 0.07504886 0.36421737 0.28550252\n",
            " 0.26832113 0.22344844 0.26643097 0.3354504  0.3599856  0.33864686\n",
            " 0.33466616 0.26069722 0.23412871 0.27737212 0.3708721  0.288843\n",
            " 0.19428425 0.31091502 0.33192822 0.33527362 0.20475511 0.27175245\n",
            " 0.2728189  0.08373078 0.31999567 0.2769784  0.31392163 0.23855948\n",
            " 0.20863748 0.3116913  0.21204315 0.278562   0.2939252  0.1983006\n",
            " 0.26043007 0.29192355 0.23482615 0.29902646], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.27094567 0.15436926 0.16022637 0.31999752 0.17976744 0.37035668\n",
            " 0.27730158 0.33465973 0.22218452 0.29455188 0.23947094 0.25586233\n",
            " 0.21962075 0.26901007 0.38087955 0.34409752 0.27641404 0.27945796\n",
            " 0.3351821  0.32053775 0.26969877 0.31059426 0.31564    0.2219212\n",
            " 0.23584051 0.22507174 0.33121088 0.32017484 0.23841895 0.17949893\n",
            " 0.3489319  0.33720478 0.32176918 0.27839616 0.21491675 0.3330561\n",
            " 0.26996458 0.34585965 0.19277088 0.03252349 0.2594953  0.21170282\n",
            " 0.1861543  0.27810404 0.3305573  0.32914698 0.16892327 0.19528027\n",
            " 0.33174092 0.25788212 0.29457894 0.2816837  0.13545378 0.07181466\n",
            " 0.35160485 0.28022805 0.2869369  0.3138806  0.21260834 0.2918799\n",
            " 0.32496732 0.30713448 0.1939857  0.3272969  0.32036415 0.26196578\n",
            " 0.19564493 0.21834874 0.23408647 0.362456   0.11899447 0.34548226\n",
            " 0.17552121 0.34976754 0.18307775 0.27578217 0.26323292 0.12768847\n",
            " 0.33212885 0.21305571 0.19902967 0.23540562 0.3046259  0.35915157\n",
            " 0.30954447 0.35996374 0.13310944 0.31566313 0.11360919 0.03226601\n",
            " 0.31019264 0.22161289 0.24480325 0.28581688 0.27794358 0.26333413\n",
            " 0.3297069  0.3146987  0.09825637 0.23499888], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.30756798 0.3574636  0.29644406 0.34493235 0.32763603 0.21513219\n",
            " 0.0400536  0.20271952 0.13489424 0.3086586  0.32708308 0.2576398\n",
            " 0.34673557 0.2542626  0.28304183 0.21380186 0.3002254  0.23027025\n",
            " 0.29147097 0.02086367 0.2338685  0.35403347 0.33090547 0.12452856\n",
            " 0.31720915 0.15195623 0.19751544 0.07774805 0.18570499 0.32333153\n",
            " 0.3367362  0.3055738  0.21771717 0.15567166 0.299543   0.2737138\n",
            " 0.2387623  0.31054798 0.26748526 0.2660165  0.13420133 0.22306202\n",
            " 0.23318417 0.24193859 0.28115514 0.37076035 0.28189477 0.31177798\n",
            " 0.2695255  0.33924738 0.3643789  0.35040125 0.28899166 0.23704238\n",
            " 0.3221477  0.22502618 0.3212222  0.39078522 0.27434763 0.33850893\n",
            " 0.24721418 0.20122021 0.31354025 0.34900984 0.29653397 0.17063761\n",
            " 0.2427185  0.33019254 0.30944893 0.36936966 0.25921348 0.17701213\n",
            " 0.3110958  0.31878203 0.2553019  0.08773562 0.28890747 0.15277489\n",
            " 0.02581631 0.36837184 0.15023033 0.32627687 0.27123722 0.246103\n",
            " 0.34209046 0.0354803  0.34712747 0.28424358 0.29712757 0.25556377\n",
            " 0.25718507 0.31996307 0.2476425  0.2832339  0.30341396 0.3118625\n",
            " 0.31040633 0.28764936 0.31381372 0.15637387], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.28136435 0.3386174  0.28419036 0.30592853 0.26639268 0.24431555\n",
            " 0.3393575  0.18869448 0.33914903 0.2872932  0.17257577 0.27113184\n",
            " 0.18765378 0.18037444 0.26633322 0.3059413  0.29717132 0.16093957\n",
            " 0.3721896  0.3245947  0.24404502 0.332702   0.33221343 0.12189436\n",
            " 0.15401661 0.22319333 0.2965741  0.26730174 0.2918048  0.26008633\n",
            " 0.23175013 0.1858974  0.3026388  0.14500506 0.33067858 0.31353155\n",
            " 0.28764343 0.34157193 0.29565182 0.14002974 0.3087976  0.25637087\n",
            " 0.3261511  0.2305532  0.21112289 0.32834873 0.17606018 0.22056925\n",
            " 0.2295125  0.09144437 0.31403795 0.2561613  0.36402562 0.10566606\n",
            " 0.37515554 0.24493933 0.22308397 0.27453902 0.27158806 0.20223223\n",
            " 0.23078616 0.2414719  0.26379752 0.32462344 0.2805464  0.13866757\n",
            " 0.28692096 0.31292066 0.23948902 0.30586413 0.34261146 0.12848131\n",
            " 0.07041368 0.16874176 0.14793703 0.255622   0.31614497 0.19385417\n",
            " 0.32262358 0.30294356 0.29800153 0.21015619 0.27716228 0.23634446\n",
            " 0.31387672 0.30323794 0.28103667 0.2948245  0.07005589 0.2996424\n",
            " 0.02580774 0.29634377 0.16080965 0.24094678 0.3378485  0.23864268\n",
            " 0.15396264 0.19963259 0.15075308 0.11971133], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.27899376 0.20223342 0.32428262 0.2601827  0.25790596 0.23120002\n",
            " 0.15480058 0.14621772 0.03863489 0.24580853 0.2380581  0.2861302\n",
            " 0.31626424 0.17910765 0.2647022  0.34248415 0.23772015 0.16645043\n",
            " 0.24770086 0.3365165  0.33185384 0.19508064 0.25014162 0.11381338\n",
            " 0.3084469  0.21199794 0.22082861 0.16557495 0.2037425  0.25162706\n",
            " 0.3028513  0.25211605 0.3032678  0.24226522 0.12429819 0.1300106\n",
            " 0.2846058  0.02157947 0.24825712 0.01844059 0.09685019 0.3296539\n",
            " 0.1376819  0.2725282  0.23601294 0.31139198 0.04945783 0.23847745\n",
            " 0.08545903 0.09408927 0.0849008  0.27105227 0.30412763 0.34079003\n",
            " 0.26255187 0.3422179  0.12653257 0.15084668 0.26165605 0.28263092\n",
            " 0.29405278 0.23048122 0.27991188 0.2961829  0.11020321 0.2335202\n",
            " 0.32052872 0.29393232 0.2887891  0.27935544 0.03643813 0.20929553\n",
            " 0.3169907  0.3026336  0.18294974 0.33263794 0.25310674 0.08349339\n",
            " 0.28511965 0.37188387 0.15040636 0.1839004  0.22705276 0.29303232\n",
            " 0.19488513 0.31153622 0.33917832 0.24614753 0.2535246  0.1095294\n",
            " 0.12380942 0.2810613  0.19882245 0.22756557 0.32782575 0.19405161\n",
            " 0.3020491  0.16831149 0.11635818 0.20504892], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.24758779 0.32501325 0.2836285  0.23598379 0.2502075  0.1661918\n",
            " 0.26159665 0.21122618 0.14337231 0.34237382 0.31724262 0.21215373\n",
            " 0.31889802 0.18234454 0.08868485 0.22601919 0.35567698 0.27572647\n",
            " 0.2780191  0.00684245 0.24291222 0.2922146  0.28680933 0.30841467\n",
            " 0.20395081 0.29950497 0.348897   0.2704399  0.25970224 0.22062282\n",
            " 0.17899017 0.2760227  0.30989942 0.2786066  0.2102565  0.31867778\n",
            " 0.1913464  0.24920051 0.24049468 0.26769724 0.10720685 0.23156841\n",
            " 0.3302739  0.32172477 0.21634205 0.3410221  0.28036174 0.2935719\n",
            " 0.12738663 0.19478817 0.24594058 0.35374808 0.3200037  0.24476199\n",
            " 0.29225507 0.28935894 0.24050312 0.17560528 0.2888569  0.31032732\n",
            " 0.18335603 0.14973544 0.28072783 0.27131656 0.22910742 0.22226174\n",
            " 0.22302909 0.24118169 0.35821223 0.34155855 0.13636191 0.27453515\n",
            " 0.34278178 0.23067589 0.13038723 0.25895146 0.18740363 0.29457012\n",
            " 0.27581957 0.20327671 0.33077157 0.06344172 0.31783208 0.1975767\n",
            " 0.3314882  0.21058083 0.30736074 0.3521359  0.27697173 0.2307204\n",
            " 0.15285861 0.1351726  0.28948668 0.1494669  0.23520927 0.03789795\n",
            " 0.20448284 0.2177201  0.17183459 0.15633984], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.18041258 0.20900422 0.32073024 0.32724932 0.0278318  0.17862046\n",
            " 0.1706816  0.14145379 0.10026846 0.11940191 0.15938121 0.23851527\n",
            " 0.12853554 0.17460032 0.2950208  0.1646107  0.00533793 0.31643918\n",
            " 0.28081283 0.24241681 0.15693456 0.11520684 0.3142553  0.2060792\n",
            " 0.24041252 0.32594934 0.31317738 0.29918507 0.19183974 0.32283905\n",
            " 0.17659122 0.25170553 0.21909797 0.3320009  0.09381638 0.15966533\n",
            " 0.23108697 0.15000087 0.31861576 0.23287435 0.14111947 0.19602883\n",
            " 0.33418146 0.2512408  0.07719048 0.3732493  0.06216107 0.25909677\n",
            " 0.19615084 0.2952465  0.18556535 0.21958792 0.27443913 0.18809916\n",
            " 0.16690685 0.3391355  0.23980017 0.25559577 0.2018326  0.31721193\n",
            " 0.27673545 0.136243   0.28603724 0.31663343 0.12552182 0.18345343\n",
            " 0.2644833  0.02223705 0.28176025 0.17116104 0.1390916  0.18188679\n",
            " 0.26368317 0.16560407 0.26820415 0.14816895 0.17852394 0.21747947\n",
            " 0.21764386 0.3134917  0.31112382 0.14726205 0.22024828 0.3314125\n",
            " 0.33486918 0.3116422  0.33401987 0.22133379 0.18731217 0.18084814\n",
            " 0.32237262 0.2298444  0.20550056 0.2712831  0.29859674 0.33482158\n",
            " 0.12692825 0.12350497 0.33569315 0.11470162], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 20: 0.2211\n",
            "seen so far: 2100 samples\n",
            "tf.Tensor(\n",
            "[0.23144247 0.33879742 0.14677544 0.32218623 0.24731171 0.31881595\n",
            " 0.0322589  0.19507545 0.21366513 0.15076883 0.3021364  0.30647263\n",
            " 0.29544795 0.2658094  0.33354452 0.14610256 0.2908325  0.2815483\n",
            " 0.19021988 0.26818383 0.30070862 0.16753705 0.06329893 0.2712773\n",
            " 0.26427916 0.17794007 0.3052695  0.2076959  0.30157995 0.28655976\n",
            " 0.19358534 0.25618988 0.15995759 0.33109716 0.07970617 0.15572765\n",
            " 0.14743558 0.30454794 0.14479117 0.08843189 0.1596966  0.33878684\n",
            " 0.25615165 0.32414114 0.07296926 0.0207927  0.2918156  0.22373682\n",
            " 0.12730268 0.19687898 0.284346   0.28914568 0.18910599 0.12800036\n",
            " 0.24882846 0.2810168  0.21063952 0.307502   0.28494924 0.06836697\n",
            " 0.18239415 0.2843445  0.12444752 0.3216487  0.10240545 0.18738376\n",
            " 0.14800587 0.09900433 0.29729846 0.14789574 0.33585715 0.23302056\n",
            " 0.30963352 0.18515496 0.13043202 0.25644466 0.23138146 0.2714771\n",
            " 0.19289248 0.33132383 0.25383505 0.33627796 0.23028065 0.29238337\n",
            " 0.27415597 0.16782725 0.32218271 0.21131353 0.22528736 0.21138924\n",
            " 0.12736486 0.2977529  0.31002215 0.32946342 0.21727347 0.25066265\n",
            " 0.32362744 0.12874682 0.11915678 0.10172594], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.17323786 0.27629113 0.33320794 0.24291764 0.21127702 0.29972777\n",
            " 0.20413923 0.32141444 0.21231286 0.33384085 0.19803673 0.25013736\n",
            " 0.2786018  0.1973233  0.24335773 0.27714372 0.08895933 0.23679233\n",
            " 0.06152049 0.18640642 0.07742587 0.2693504  0.11678014 0.19947481\n",
            " 0.18959485 0.13829201 0.3358426  0.19450538 0.2632726  0.2824761\n",
            " 0.31687337 0.33575296 0.32476422 0.02742511 0.24321629 0.04998103\n",
            " 0.2420458  0.31923416 0.18567984 0.19833611 0.11476003 0.271932\n",
            " 0.24851859 0.30086723 0.31604055 0.1381449  0.19450374 0.26985928\n",
            " 0.21654384 0.09237063], shape=(50,), dtype=float32)\n",
            "validation MSE: 0.1899\n",
            "\n",
            "Start of epoch 1\n",
            "tf.Tensor(\n",
            "[0.3313953  0.20341645 0.3226659  0.14281751 0.3229989  0.16427433\n",
            " 0.3203505  0.24651998 0.16810526 0.10632781 0.2851185  0.31277844\n",
            " 0.30349603 0.1165634  0.11577926 0.19339967 0.3139977  0.10008118\n",
            " 0.29715848 0.27350724 0.1893477  0.05150051 0.30579758 0.2936509\n",
            " 0.25094804 0.25883448 0.3223711  0.19261725 0.25718763 0.18776089\n",
            " 0.3322799  0.2791425  0.17866875 0.2010556  0.18286656 0.14677624\n",
            " 0.10460207 0.12581028 0.29301003 0.13361266 0.29696926 0.21089351\n",
            " 0.30747142 0.30742544 0.32462665 0.13398473 0.13989346 0.275835\n",
            " 0.2574328  0.22609873 0.3057214  0.11069418 0.15286423 0.06490477\n",
            " 0.29733908 0.2003249  0.1292771  0.18319221 0.313012   0.27421117\n",
            " 0.15805368 0.19120967 0.08853001 0.12422273 0.32090035 0.01442142\n",
            " 0.14258109 0.2909715  0.08163998 0.16121013 0.17739117 0.29999918\n",
            " 0.3172763  0.14779983 0.28882733 0.2511148  0.1273496  0.07871453\n",
            " 0.22668286 0.31057915 0.04748614 0.23662925 0.08097426 0.32504904\n",
            " 0.24361496 0.19680683 0.31677738 0.15547365 0.18375619 0.15258591\n",
            " 0.18398635 0.15242405 0.17642485 0.28775528 0.31949314 0.21518667\n",
            " 0.28095192 0.20263462 0.16036656 0.13218848], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 0: 0.2129\n",
            "seen so far: 100 samples\n",
            "tf.Tensor(\n",
            "[0.2788829  0.18989624 0.08001024 0.24994554 0.1456657  0.24430859\n",
            " 0.26567966 0.17554156 0.22771217 0.1765126  0.20354761 0.17382276\n",
            " 0.09584603 0.25039425 0.16765118 0.29152402 0.2910491  0.06667724\n",
            " 0.12692258 0.27668932 0.22527172 0.14284652 0.11548334 0.22627951\n",
            " 0.15631033 0.24844886 0.08287787 0.11829377 0.24832135 0.18327796\n",
            " 0.27194777 0.17663203 0.23498331 0.17696299 0.36211988 0.13013415\n",
            " 0.14949797 0.08495104 0.21319087 0.31390125 0.14923966 0.23806201\n",
            " 0.29283604 0.21185867 0.09874019 0.31063575 0.16976845 0.21519653\n",
            " 0.20350908 0.3273119  0.11049669 0.09145246 0.25009882 0.09810459\n",
            " 0.15220726 0.13704379 0.1808422  0.09688931 0.30187726 0.23310421\n",
            " 0.2729698  0.13793981 0.05695805 0.18748158 0.18755777 0.22968173\n",
            " 0.06264815 0.28234825 0.3207487  0.18893169 0.19847864 0.28431755\n",
            " 0.28189173 0.14300491 0.27265653 0.04578687 0.1878783  0.31495425\n",
            " 0.17504936 0.2753934  0.14368324 0.25582644 0.32679674 0.32541588\n",
            " 0.202821   0.28046933 0.31993666 0.3287439  0.30716532 0.1223318\n",
            " 0.25688788 0.30592614 0.13291478 0.11572102 0.13937859 0.21122883\n",
            " 0.25655255 0.23618282 0.2643884  0.03441466], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.23382054 0.26412222 0.32478055 0.23527783 0.32518917 0.08591021\n",
            " 0.2334603  0.20749974 0.27625367 0.14379261 0.31760183 0.2045756\n",
            " 0.14743038 0.20971425 0.29545942 0.32084396 0.1748801  0.32109764\n",
            " 0.2918567  0.13205783 0.16136716 0.32502046 0.09301946 0.29278505\n",
            " 0.29694256 0.16582131 0.2928339  0.18230157 0.31830665 0.14851873\n",
            " 0.22533709 0.13488574 0.04154313 0.29428372 0.32676595 0.2925236\n",
            " 0.16527884 0.20415883 0.13996412 0.29891357 0.14486535 0.30839032\n",
            " 0.16795178 0.21105047 0.26752406 0.03055946 0.13671844 0.2156711\n",
            " 0.1813258  0.1673826  0.01699818 0.205157   0.07924041 0.13365348\n",
            " 0.29267958 0.07275685 0.16064835 0.10606257 0.02907775 0.23047888\n",
            " 0.09762049 0.10859185 0.31588057 0.20023918 0.20444727 0.25114766\n",
            " 0.24884613 0.26256973 0.2765286  0.2189005  0.2663566  0.29563913\n",
            " 0.20012458 0.29995874 0.16958727 0.23139746 0.2808356  0.13037686\n",
            " 0.29490134 0.18992637 0.0653742  0.3186722  0.30450615 0.16552232\n",
            " 0.3317945  0.23990737 0.12118348 0.27211222 0.30080286 0.24401033\n",
            " 0.18000843 0.10252295 0.02995016 0.21684875 0.27179527 0.07948302\n",
            " 0.18983652 0.2812647  0.16406165 0.23621118], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.25777656 0.19258189 0.09350592 0.11865472 0.261947   0.20781791\n",
            " 0.27571425 0.20527518 0.1745107  0.20631534 0.24930714 0.21303542\n",
            " 0.17808573 0.14637716 0.17292917 0.2874255  0.27077937 0.15680599\n",
            " 0.11172587 0.15173207 0.23702526 0.08934382 0.22575945 0.30873397\n",
            " 0.24325036 0.27373126 0.24403651 0.31195006 0.2590003  0.2090038\n",
            " 0.15201755 0.06677429 0.2630174  0.31558633 0.25079864 0.205728\n",
            " 0.04398829 0.13120395 0.20227586 0.18218638 0.08524146 0.09889463\n",
            " 0.23225562 0.29554668 0.28870788 0.19173147 0.19257934 0.05801718\n",
            " 0.19525816 0.23059683 0.19084918 0.12125436 0.27401957 0.23179471\n",
            " 0.1918056  0.32269374 0.23010898 0.18714209 0.14264916 0.30317497\n",
            " 0.09798816 0.11020934 0.06020801 0.00742118 0.1541168  0.23481934\n",
            " 0.29863587 0.11263785 0.16941871 0.16328286 0.02094063 0.29210186\n",
            " 0.19802266 0.21189862 0.3227519  0.02040861 0.32385972 0.30699098\n",
            " 0.31165555 0.08509827 0.28351203 0.21660955 0.1512655  0.11785677\n",
            " 0.12913649 0.19115113 0.19201805 0.15819317 0.02393563 0.13753453\n",
            " 0.23200661 0.01195907 0.1295662  0.20934938 0.3204493  0.26718244\n",
            " 0.2880462  0.04771848 0.0637312  0.30553135], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.24176419 0.15395159 0.10107813 0.07915679 0.09455231 0.11395892\n",
            " 0.16648242 0.10060275 0.30248645 0.08732174 0.22751564 0.30628654\n",
            " 0.23453806 0.22988985 0.28184754 0.10094791 0.14974551 0.14308715\n",
            " 0.1783074  0.07103221 0.19056354 0.20231158 0.01538976 0.19472957\n",
            " 0.31934887 0.249689   0.2974411  0.13030054 0.31867713 0.29453087\n",
            " 0.11243647 0.12194402 0.14504087 0.08199631 0.0814686  0.15949\n",
            " 0.21134293 0.04170766 0.12214424 0.24793816 0.08691579 0.22583467\n",
            " 0.16444866 0.29131904 0.24941425 0.2733371  0.32039875 0.117227\n",
            " 0.1595685  0.18873541 0.2629689  0.03375624 0.1546823  0.3105217\n",
            " 0.14540392 0.08097187 0.22709428 0.32405052 0.27096468 0.12953253\n",
            " 0.20297861 0.12717758 0.30639163 0.2587811  0.1134525  0.28572783\n",
            " 0.27879605 0.29970804 0.12646033 0.20292325 0.21022885 0.14355953\n",
            " 0.13957426 0.16904218 0.03287143 0.00553238 0.20508485 0.19054823\n",
            " 0.2775592  0.2923611  0.11979278 0.23253751 0.180833   0.08237056\n",
            " 0.21006697 0.20261078 0.20936157 0.11655737 0.13615455 0.2126496\n",
            " 0.21180642 0.24107705 0.18940915 0.32013616 0.16109699 0.30747858\n",
            " 0.2135507  0.1585475  0.06976875 0.22095335], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.10808279 0.08467861 0.2053147  0.11481135 0.22706966 0.20310257\n",
            " 0.11944052 0.03757229 0.08356666 0.15132287 0.2997266  0.29284048\n",
            " 0.14838919 0.27945238 0.22159712 0.11525929 0.16864012 0.2462186\n",
            " 0.15815657 0.11081598 0.11307871 0.21907233 0.03029384 0.22141439\n",
            " 0.2573119  0.1600431  0.25691766 0.05875902 0.24072935 0.19963603\n",
            " 0.29424694 0.27158132 0.16607143 0.06981918 0.05880145 0.04413529\n",
            " 0.06896394 0.13578407 0.30996686 0.20073898 0.2883004  0.12956637\n",
            " 0.21887983 0.2908533  0.12426078 0.10628065 0.13160563 0.06734829\n",
            " 0.01318046 0.03401805 0.15118767 0.07238529 0.2366674  0.20448701\n",
            " 0.30831027 0.30520198 0.19507422 0.10997212 0.19478963 0.18636943\n",
            " 0.19456922 0.14095251 0.1815954  0.20805667 0.22543706 0.22713257\n",
            " 0.15183382 0.1296193  0.11469332 0.10940394 0.14916225 0.2575174\n",
            " 0.30824015 0.30462593 0.26408297 0.08357042 0.1373743  0.2801158\n",
            " 0.1146253  0.12655218 0.18928385 0.26794913 0.11849455 0.11150906\n",
            " 0.21467906 0.30613717 0.28330156 0.0821962  0.21624947 0.17595439\n",
            " 0.24400727 0.11960107 0.28836164 0.18988484 0.1401368  0.13560538\n",
            " 0.31017885 0.30795178 0.25429606 0.00885126], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.2833903  0.11906537 0.2398556  0.14452495 0.17510335 0.18075477\n",
            " 0.19298561 0.20353629 0.2805951  0.132019   0.17417221 0.24838935\n",
            " 0.2013905  0.26349542 0.1773631  0.18467422 0.17094822 0.12424898\n",
            " 0.01109754 0.19640915 0.21006052 0.16049905 0.14358427 0.04109391\n",
            " 0.17343377 0.24327224 0.30194676 0.15062751 0.1649606  0.11678944\n",
            " 0.23685528 0.1050255  0.26029783 0.05214125 0.18497442 0.1879149\n",
            " 0.2322791  0.28940287 0.2926679  0.19355221 0.18896472 0.16719596\n",
            " 0.05301886 0.12729648 0.17172974 0.22291349 0.08500279 0.13882177\n",
            " 0.12552805 0.07168573 0.21041466 0.2719321  0.09890514 0.21525224\n",
            " 0.24403656 0.14617603 0.20517741 0.2958845  0.2290433  0.23676427\n",
            " 0.11846944 0.2038079  0.21812694 0.12416595 0.23659201 0.13891186\n",
            " 0.06895126 0.23316033 0.16281836 0.17482954 0.27763215 0.15559936\n",
            " 0.14354345 0.15733492 0.13375428 0.13288797 0.25254092 0.27618992\n",
            " 0.13028185 0.20436256 0.17637777 0.24150987 0.06592782 0.15410018\n",
            " 0.09624738 0.15081997 0.25764778 0.15248781 0.12649801 0.2879231\n",
            " 0.09676769 0.17019409 0.30579525 0.10583802 0.18411016 0.11356912\n",
            " 0.19271298 0.24970885 0.19538014 0.06411775], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.26547208 0.25851935 0.13506167 0.28881618 0.22837436 0.21720259\n",
            " 0.1499598  0.2796192  0.10640413 0.04001543 0.18007217 0.09304645\n",
            " 0.07575177 0.17589466 0.25908792 0.25931662 0.23953317 0.17585254\n",
            " 0.09371779 0.12616439 0.04872142 0.09263306 0.25340536 0.13818626\n",
            " 0.04957402 0.07953423 0.2777522  0.11270535 0.12422174 0.1937598\n",
            " 0.28973052 0.1611611  0.28942356 0.16480383 0.20420945 0.0373817\n",
            " 0.2965888  0.1936114  0.18834965 0.26814255 0.15678303 0.23658569\n",
            " 0.09126753 0.11351353 0.12925953 0.18979527 0.12017009 0.14403449\n",
            " 0.2780716  0.24152978 0.28654456 0.02285955 0.2042392  0.23600845\n",
            " 0.29230773 0.22677077 0.14803135 0.23934585 0.15065464 0.25578108\n",
            " 0.25756708 0.2467653  0.12382834 0.11936883 0.10377958 0.31840512\n",
            " 0.15660739 0.23510091 0.07326602 0.28124094 0.1041405  0.04626689\n",
            " 0.18408263 0.16281055 0.14645107 0.16774781 0.11970035 0.08210955\n",
            " 0.23552942 0.14966242 0.01502047 0.14640321 0.24196923 0.10576215\n",
            " 0.298892   0.26043501 0.25929382 0.06747166 0.22890238 0.10294145\n",
            " 0.18546982 0.24992937 0.19361895 0.23819076 0.29234952 0.12302601\n",
            " 0.1645227  0.11837403 0.22902232 0.2294359 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.04420858 0.2913695  0.14243488 0.10387576 0.26131594 0.314191\n",
            " 0.2652658  0.07875418 0.18675669 0.3006613  0.25002474 0.17988646\n",
            " 0.09310385 0.18119334 0.26555178 0.15077564 0.04892422 0.00729764\n",
            " 0.19305022 0.25781387 0.26333833 0.06021602 0.1800173  0.03138311\n",
            " 0.26477212 0.1173544  0.17283277 0.25630453 0.06228356 0.14919697\n",
            " 0.14281885 0.21224815 0.12378845 0.04838824 0.28475595 0.18991984\n",
            " 0.09389645 0.15339664 0.10880164 0.2449042  0.14837027 0.2786493\n",
            " 0.22353064 0.262185   0.14799474 0.15369727 0.20864773 0.11829624\n",
            " 0.21376853 0.24789931 0.25118265 0.11004138 0.24682038 0.28272137\n",
            " 0.05610858 0.1933698  0.04628487 0.1356666  0.06712297 0.18603027\n",
            " 0.24072511 0.09516314 0.21536304 0.0722265  0.3050194  0.06537366\n",
            " 0.17803095 0.27976832 0.06828941 0.27257782 0.09888818 0.15581934\n",
            " 0.13601297 0.20481974 0.03315905 0.2355145  0.29122302 0.20720385\n",
            " 0.29383722 0.10426769 0.17110647 0.2062149  0.1403868  0.10765412\n",
            " 0.28968787 0.24691625 0.04686219 0.15165326 0.04483102 0.15210508\n",
            " 0.08622509 0.08338066 0.19111367 0.14656202 0.23445316 0.19676991\n",
            " 0.08365368 0.12696035 0.2580609  0.17812298], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.26045635 0.15327276 0.01539098 0.24641962 0.14452685 0.08915076\n",
            " 0.21056795 0.10874116 0.02844579 0.07614483 0.17296845 0.26639244\n",
            " 0.167053   0.1448146  0.04047858 0.12621875 0.12192056 0.00764188\n",
            " 0.23762338 0.16098972 0.11319095 0.2658752  0.06300145 0.18733938\n",
            " 0.11539847 0.31742537 0.08012674 0.0612006  0.19920518 0.12805209\n",
            " 0.21387197 0.02587563 0.18482178 0.07984403 0.09492779 0.17268544\n",
            " 0.10543258 0.20099725 0.08931512 0.13447575 0.19565822 0.21330464\n",
            " 0.04824265 0.23795444 0.01470725 0.13043575 0.18129982 0.03108983\n",
            " 0.186702   0.02483695 0.08217073 0.283977   0.08240966 0.22491719\n",
            " 0.2654357  0.23342936 0.14894374 0.12183896 0.26281825 0.14253758\n",
            " 0.10974758 0.19071166 0.30548236 0.18685369 0.26784474 0.19926049\n",
            " 0.2730545  0.06560602 0.04665999 0.269327   0.17437778 0.01130496\n",
            " 0.20657624 0.09722283 0.0134156  0.24983655 0.24610992 0.13088052\n",
            " 0.10871349 0.29155365 0.14447168 0.24937582 0.28970936 0.27718925\n",
            " 0.26912168 0.26967505 0.04324751 0.19673796 0.09158254 0.3010825\n",
            " 0.12632622 0.23402952 0.28950095 0.20034854 0.14344494 0.1761989\n",
            " 0.21682204 0.14642213 0.18691675 0.30819124], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.12410807 0.05208427 0.14556025 0.02093737 0.15389432 0.06661571\n",
            " 0.2191974  0.13224936 0.12924305 0.2248437  0.10368226 0.1862521\n",
            " 0.19353844 0.09809218 0.11331906 0.22856086 0.26246822 0.15623422\n",
            " 0.15572742 0.40999314 0.10704377 0.28590137 0.22200452 0.02851581\n",
            " 0.13682584 0.08043086 0.11726256 0.14564215 0.1481979  0.18396969\n",
            " 0.24032551 0.11464509 0.12884918 0.08970768 0.07565497 0.03269265\n",
            " 0.03310516 0.19944708 0.19615169 0.12890957 0.10860822 0.17435122\n",
            " 0.17555694 0.09759447 0.1988805  0.07385551 0.09136317 0.26648438\n",
            " 0.13009097 0.09667057 0.20717597 0.1909548  0.10547329 0.10892638\n",
            " 0.16671453 0.06518807 0.14331429 0.0453807  0.23731928 0.21972235\n",
            " 0.10784885 0.05934403 0.23395054 0.0274915  0.16704561 0.24307692\n",
            " 0.15063216 0.12838657 0.11345362 0.11854541 0.1809669  0.22293842\n",
            " 0.2183178  0.12654588 0.27702126 0.12819894 0.22163148 0.06921074\n",
            " 0.02209129 0.26827514 0.17434959 0.14413275 0.07447702 0.16184945\n",
            " 0.22672015 0.26769438 0.26409346 0.18128274 0.11790267 0.13749875\n",
            " 0.13966049 0.05268531 0.14000547 0.14706971 0.0634261  0.15979989\n",
            " 0.24166997 0.12334517 0.16448043 0.176912  ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 10: 0.1502\n",
            "seen so far: 1100 samples\n",
            "tf.Tensor(\n",
            "[0.10193688 0.13752733 0.2500178  0.28169325 0.14554004 0.06444327\n",
            " 0.2588317  0.27178928 0.2175519  0.1982824  0.20155233 0.14569065\n",
            " 0.08875519 0.25812498 0.1640477  0.13498554 0.07088331 0.09836316\n",
            " 0.04431274 0.08182571 0.07417408 0.05011586 0.12199392 0.2133333\n",
            " 0.10940671 0.26860142 0.16549669 0.04342842 0.05742529 0.2786446\n",
            " 0.04299926 0.06906904 0.20951839 0.18241505 0.09901267 0.10552877\n",
            " 0.16451265 0.15763082 0.2785724  0.15796222 0.24215871 0.20496964\n",
            " 0.22040819 0.07960764 0.19122578 0.07591227 0.27940506 0.27791888\n",
            " 0.25443888 0.2719756  0.0534835  0.0878765  0.2697234  0.23311399\n",
            " 0.12705287 0.13120268 0.08490017 0.03506174 0.28790775 0.07206952\n",
            " 0.18373568 0.19630434 0.09021029 0.10242017 0.11096592 0.1066375\n",
            " 0.2668903  0.25284484 0.27387086 0.11182011 0.12734668 0.21228136\n",
            " 0.2495151  0.03634226 0.09453442 0.13172714 0.06560087 0.23762982\n",
            " 0.09684993 0.19114292 0.1607026  0.178644   0.17518075 0.10341177\n",
            " 0.1406771  0.1675518  0.1354963  0.02949275 0.06581574 0.2557539\n",
            " 0.14471903 0.04705715 0.23193    0.04438749 0.11430982 0.09106702\n",
            " 0.02158467 0.11011291 0.08255161 0.1229178 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.12263087 0.15819131 0.1264762  0.09428064 0.07756124 0.19156283\n",
            " 0.13367005 0.17567162 0.03906538 0.24548267 0.26808676 0.23629409\n",
            " 0.2409416  0.11413249 0.23837732 0.25103733 0.0260797  0.12522735\n",
            " 0.12374201 0.09006721 0.13402233 0.18104877 0.21211743 0.12133647\n",
            " 0.20227836 0.25554606 0.05235805 0.1827149  0.14490058 0.15398651\n",
            " 0.04680275 0.16713195 0.1088216  0.14094388 0.1370283  0.12428699\n",
            " 0.18245052 0.00908803 0.06787956 0.21529907 0.18287206 0.15942048\n",
            " 0.25577423 0.26078388 0.059889   0.17601466 0.11589896 0.12492438\n",
            " 0.2502815  0.14105503 0.14180237 0.154258   0.14574862 0.28507894\n",
            " 0.19712843 0.07841186 0.19229482 0.16607413 0.0396497  0.14113778\n",
            " 0.19145511 0.22975107 0.03675797 0.21337833 0.20000857 0.15991305\n",
            " 0.06769232 0.27057555 0.26182392 0.27706614 0.28999332 0.04534684\n",
            " 0.14892338 0.05490479 0.23189496 0.16727132 0.1376016  0.18323223\n",
            " 0.18887703 0.12736556 0.18728836 0.27066648 0.11166295 0.04423225\n",
            " 0.08982661 0.15218602 0.2016175  0.10141981 0.28028932 0.16392462\n",
            " 0.14540023 0.23720731 0.14117992 0.18091662 0.13084812 0.04090146\n",
            " 0.17971604 0.10081128 0.26437053 0.17237611], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.06986244 0.23210768 0.18001388 0.19334896 0.126628   0.23975998\n",
            " 0.13670371 0.14042611 0.07071336 0.04115032 0.1631348  0.20977867\n",
            " 0.06010218 0.22357212 0.18242244 0.05253332 0.18022488 0.11675598\n",
            " 0.21399991 0.15151818 0.16998808 0.19637902 0.09278346 0.26389173\n",
            " 0.1505049  0.10149459 0.20015478 0.25496837 0.0491778  0.09608825\n",
            " 0.23154055 0.188172   0.15880188 0.08394134 0.2046792  0.15302357\n",
            " 0.04659495 0.26556015 0.13417885 0.20441245 0.10472637 0.1352204\n",
            " 0.20322831 0.24807774 0.16828746 0.04870791 0.0185981  0.14403225\n",
            " 0.04929725 0.07278287 0.07565777 0.02798989 0.19812371 0.07296753\n",
            " 0.18536176 0.21457894 0.1191267  0.17529859 0.25673997 0.16849948\n",
            " 0.1371172  0.27826512 0.2675583  0.07410121 0.02571028 0.11136758\n",
            " 0.12989601 0.21017647 0.19681962 0.249651   0.07693257 0.06836507\n",
            " 0.17418014 0.09148435 0.01370937 0.01897451 0.0498349  0.23093069\n",
            " 0.09037719 0.05983033 0.28224525 0.11740986 0.04083401 0.18680634\n",
            " 0.11765312 0.09425464 0.23832424 0.21609025 0.02707865 0.18577003\n",
            " 0.07483929 0.10717642 0.0491177  0.15150388 0.06471597 0.04749532\n",
            " 0.20899479 0.11630967 0.1366879  0.18725409], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.14599636 0.08103971 0.26799756 0.2276497  0.04262462 0.11804583\n",
            " 0.02888056 0.13848065 0.04698271 0.15056613 0.26895878 0.04587873\n",
            " 0.2997924  0.11209124 0.05165921 0.06893717 0.11306106 0.03014869\n",
            " 0.05236301 0.10867856 0.15645073 0.02942633 0.0743917  0.01610467\n",
            " 0.12438434 0.18696062 0.04683352 0.10407972 0.202326   0.06344382\n",
            " 0.09190285 0.19906111 0.11527038 0.19701044 0.23356795 0.09825391\n",
            " 0.1680303  0.27650663 0.0344458  0.17522132 0.26250854 0.17589027\n",
            " 0.16215241 0.21618931 0.0974142  0.25844064 0.16188276 0.04619065\n",
            " 0.16034125 0.09876215 0.13447057 0.2612776  0.01589709 0.26902738\n",
            " 0.18074481 0.08353034 0.01797817 0.26982465 0.16192982 0.01989704\n",
            " 0.08258785 0.08886277 0.18926334 0.03932021 0.0596639  0.04787505\n",
            " 0.00636313 0.10386601 0.11756579 0.24934272 0.22384666 0.14424053\n",
            " 0.12406728 0.22357613 0.12370976 0.13734771 0.10878127 0.20985727\n",
            " 0.23643614 0.02385699 0.04613677 0.26817873 0.17109054 0.01865274\n",
            " 0.20815702 0.11499358 0.12031154 0.19165455 0.15601212 0.10294095\n",
            " 0.0364855  0.18639451 0.16854304 0.17412029 0.07945522 0.13921909\n",
            " 0.07026184 0.13500255 0.05190493 0.02482861], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.03230233 0.13582101 0.23432994 0.214323   0.13828184 0.03219241\n",
            " 0.11903588 0.13984899 0.04803591 0.03683135 0.13447566 0.08972775\n",
            " 0.06795735 0.06137952 0.12787057 0.10990623 0.0880748  0.19722676\n",
            " 0.05936221 0.17186701 0.13111295 0.2553284  0.15652774 0.1410688\n",
            " 0.01094082 0.12109118 0.22992945 0.24134414 0.15825112 0.17948334\n",
            " 0.18626992 0.1807533  0.1166429  0.01571116 0.14126806 0.1259136\n",
            " 0.14270356 0.09236327 0.01396399 0.15508057 0.1863967  0.15549983\n",
            " 0.09377076 0.14336006 0.23344682 0.09375653 0.07655151 0.16983171\n",
            " 0.05719886 0.0884397  0.05397779 0.0821311  0.10213501 0.06176896\n",
            " 0.10614029 0.4837118  0.13552074 0.15007813 0.06612125 0.0339708\n",
            " 0.03226788 0.02608937 0.23435225 0.24821734 0.08202129 0.04341219\n",
            " 0.12997998 0.1017684  0.13438535 0.09526998 0.2007748  0.1161802\n",
            " 0.16648541 0.06123284 0.06581632 0.2403623  0.04661013 0.04771423\n",
            " 0.02446193 0.09628228 0.1391688  0.06553481 0.10471871 0.03660424\n",
            " 0.24597983 0.11083361 0.16252166 0.1613143  0.15925848 0.22442119\n",
            " 0.07705371 0.0257499  0.13834496 0.03808688 0.19229583 0.14863099\n",
            " 0.05350865 0.1896811  0.09681927 0.09066186], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.1089628  0.12029624 0.09550003 0.03546701 0.12914188 0.23354214\n",
            " 0.09027305 0.03558332 0.11505304 0.13772045 0.08395231 0.04363019\n",
            " 0.10736159 0.20193924 0.08458712 0.00474567 0.03624218 0.06210193\n",
            " 0.06229116 0.0821721  0.10994717 0.21075709 0.1827231  0.12989728\n",
            " 0.08827438 0.10070169 0.04941064 0.05525376 0.24662031 0.14372267\n",
            " 0.07077007 0.08453688 0.17643453 0.1032193  0.22139233 0.03525971\n",
            " 0.2509924  0.25975767 0.02023165 0.09695514 0.06417167 0.09534431\n",
            " 0.05589123 0.17676438 0.05777389 0.23948689 0.0489942  0.12700213\n",
            " 0.14980148 0.14833917 0.2215565  0.06181975 0.08602902 0.26733664\n",
            " 0.0786768  0.1345316  0.21386041 0.10125869 0.1937655  0.02480895\n",
            " 0.19636166 0.23697084 0.1367063  0.14819469 0.08908603 0.17380762\n",
            " 0.27463612 0.02665603 0.04647295 0.10024218 0.05336409 0.14478317\n",
            " 0.23780452 0.05183309 0.23702244 0.05360791 0.11742207 0.1864336\n",
            " 0.1350312  0.03204067 0.11572615 0.2421878  0.26396316 0.00556698\n",
            " 0.2489848  0.04313284 0.20000316 0.0304858  0.23044379 0.1252584\n",
            " 0.0889244  0.21549125 0.25728378 0.05229089 0.12324497 0.08981815\n",
            " 0.20134597 0.12226004 0.2434839  0.24949302], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.08765975 0.07620061 0.0850262  0.08803643 0.03634645 0.21275832\n",
            " 0.13168792 0.15710686 0.17185111 0.25552177 0.14860122 0.14577533\n",
            " 0.08830243 0.11350176 0.0846206  0.20962979 0.19109541 0.0949759\n",
            " 0.10298551 0.03210601 0.20576082 0.09124479 0.24953611 0.19642448\n",
            " 0.06949715 0.12854964 0.25072572 0.22026189 0.18769945 0.03058992\n",
            " 0.0105003  0.1607428  0.16771066 0.19098286 0.0972493  0.03958925\n",
            " 0.16104703 0.24113615 0.08085681 0.11583847 0.15974174 0.11335101\n",
            " 0.18634753 0.0821901  0.08041751 0.08076917 0.05944761 0.01243898\n",
            " 0.18669151 0.2081626  0.08014136 0.23507889 0.1134127  0.13990013\n",
            " 0.16954796 0.12894721 0.21425998 0.13258845 0.04145507 0.10346341\n",
            " 0.06784064 0.13021307 0.15667869 0.23450442 0.0837618  0.18533099\n",
            " 0.30704907 0.24359901 0.11633131 0.19103312 0.12308455 0.21243359\n",
            " 0.02748768 0.274961   0.13229789 0.2149735  0.10465785 0.1769465\n",
            " 0.08994403 0.2051583  0.20659393 0.23817639 0.11277003 0.25350797\n",
            " 0.1570156  0.03796132 0.11474511 0.17093416 0.13200572 0.03645314\n",
            " 0.00560581 0.16973215 0.12821428 0.1413454  0.19589722 0.14735785\n",
            " 0.02512195 0.01323919 0.15695782 0.23645194], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.25104275 0.19303952 0.15892662 0.05431307 0.21518023 0.2378562\n",
            " 0.02484047 0.03101399 0.19080327 0.09078149 0.11827464 0.06860423\n",
            " 0.15148465 0.03988506 0.22251375 0.11216287 0.02604563 0.19452225\n",
            " 0.07973634 0.05512876 0.26471016 0.20277834 0.22506906 0.19885474\n",
            " 0.09622663 0.02919078 0.18249728 0.03701686 0.06194508 0.18097298\n",
            " 0.12837678 0.22135134 0.0584483  0.15398891 0.16784243 0.21120195\n",
            " 0.04122596 0.16592123 0.22724676 0.1726305  0.11991512 0.0759292\n",
            " 0.1086954  0.04508567 0.2566929  0.11100164 0.13159479 0.11762936\n",
            " 0.227314   0.13547663 0.0269574  0.1348285  0.14515309 0.05874744\n",
            " 0.16917707 0.1121305  0.29057094 0.01199482 0.02757549 0.07870553\n",
            " 0.20997912 0.21738426 0.2200462  0.00383417 0.14005907 0.00969123\n",
            " 0.16850938 0.16606711 0.07364786 0.005911   0.04178892 0.22584605\n",
            " 0.05460248 0.17399687 0.01332506 0.22538424 0.12510349 0.00379624\n",
            " 0.1039606  0.09493876 0.07712826 0.17001094 0.04877347 0.10863902\n",
            " 0.1741995  0.09807107 0.10779553 0.21505828 0.23803526 0.16791515\n",
            " 0.03592096 0.37429938 0.2324347  0.01578609 0.10103711 0.07613974\n",
            " 0.05881281 0.10145602 0.06787358 0.24767487], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.16242649 0.14310594 0.06262971 0.17963035 0.07618713 0.08503956\n",
            " 0.10534763 0.14081173 0.05061361 0.20590788 0.06242211 0.02197341\n",
            " 0.09426633 0.10850238 0.10694828 0.14053734 0.0960874  0.14112663\n",
            " 0.13008296 0.07727929 0.09386349 0.24041271 0.16243169 0.05513683\n",
            " 0.10867194 0.21682602 0.06603389 0.1502327  0.183421   0.05712813\n",
            " 0.07292796 0.10682539 0.17836972 0.03459315 0.11094484 0.1606629\n",
            " 0.0553393  0.04359951 0.09826308 0.23461665 0.19388159 0.07321572\n",
            " 0.17489295 0.08638299 0.09045648 0.16176282 0.03060067 0.24886452\n",
            " 0.05774263 0.12813668 0.08358058 0.12932786 0.07991301 0.09156394\n",
            " 0.1076531  0.16446856 0.08025948 0.04840925 0.16489454 0.15552181\n",
            " 0.23669182 0.22773717 0.22040516 0.20436883 0.02185234 0.12282624\n",
            " 0.01294256 0.09010989 0.16598912 0.17126818 0.03631352 0.09607851\n",
            " 0.22138418 0.09811398 0.10092161 0.09340908 0.16064896 0.19700097\n",
            " 0.11649726 0.13520212 0.15290359 0.20168702 0.21968943 0.1231159\n",
            " 0.02280951 0.07893016 0.20134048 0.07046714 0.24024057 0.08492061\n",
            " 0.05502505 0.09475944 0.07624229 0.20436282 0.19771107 0.12506737\n",
            " 0.07195619 0.05132234 0.00375306 0.027816  ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.21771018 0.14314835 0.04634861 0.04627129 0.07912228 0.22461195\n",
            " 0.09828281 0.00308021 0.16635126 0.06714094 0.06682156 0.15999375\n",
            " 0.08183388 0.04618227 0.12675567 0.03652835 0.1633607  0.10201939\n",
            " 0.02231883 0.18224259 0.02559385 0.22321205 0.22147562 0.187021\n",
            " 0.05740846 0.06138228 0.19725384 0.08089484 0.00812993 0.046482\n",
            " 0.02034249 0.01661388 0.06051704 0.19018506 0.17116334 0.21126741\n",
            " 0.08192962 0.10087845 0.04733881 0.09866009 0.05116795 0.09655934\n",
            " 0.19340704 0.14460991 0.22714306 0.10314375 0.1269039  0.05388863\n",
            " 0.14320935 0.15072174 0.04262312 0.14031765 0.19350672 0.21074337\n",
            " 0.14661898 0.17207204 0.1813305  0.06953308 0.17790438 0.17172958\n",
            " 0.22907765 0.20491469 0.20006217 0.04529919 0.24425775 0.2020415\n",
            " 0.18327643 0.1390913  0.18584096 0.19703168 0.10616666 0.18669222\n",
            " 0.06570604 0.19259696 0.17856686 0.13488774 0.15459935 0.04619363\n",
            " 0.11061566 0.05557472 0.03836817 0.13630904 0.17487483 0.06385063\n",
            " 0.13162291 0.10231179 0.06106044 0.08753437 0.32482204 0.19629367\n",
            " 0.1387827  0.10636421 0.07050654 0.00557718 0.10413629 0.17075133\n",
            " 0.07087282 0.13516815 0.03489714 0.0241413 ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 20: 0.1223\n",
            "seen so far: 2100 samples\n",
            "tf.Tensor(\n",
            "[0.05305724 0.15940745 0.08436912 0.07338754 0.05973347 0.08356518\n",
            " 0.08188417 0.10101346 0.02643691 0.11073181 0.05024974 0.14759515\n",
            " 0.0111099  0.13022201 0.00847946 0.16625465 0.06271806 0.19934648\n",
            " 0.02151326 0.1205103  0.06979799 0.08462852 0.05721307 0.12604146\n",
            " 0.11738924 0.23108931 0.11571088 0.13370347 0.07822075 0.09298623\n",
            " 0.13256902 0.04293966 0.22237831 0.15616684 0.13913226 0.05066796\n",
            " 0.1218005  0.1330053  0.08331897 0.24001952 0.11761171 0.03175687\n",
            " 0.21034777 0.23278745 0.07224399 0.24660404 0.10997534 0.07638108\n",
            " 0.22180903 0.20852877 0.09371275 0.09026625 0.06483634 0.12840885\n",
            " 0.13412587 0.20158333 0.09366571 0.08679885 0.22851507 0.13860208\n",
            " 0.11167402 0.1480806  0.04894427 0.16485138 0.09294283 0.08400401\n",
            " 0.10057796 0.22996242 0.10146173 0.09136364 0.00856537 0.22634842\n",
            " 0.06113872 0.09697485 0.00605936 0.05211107 0.10838688 0.03118072\n",
            " 0.14994599 0.22117989 0.21196313 0.17997533 0.12963386 0.0546221\n",
            " 0.01349061 0.01227262 0.14918138 0.04906755 0.00191588 0.0895802\n",
            " 0.03278978 0.07688961 0.07082786 0.05265345 0.01851128 0.20758905\n",
            " 0.2065655  0.06055497 0.08199655 0.1656986 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.11764292 0.10331783 0.08423856 0.08583949 0.1050837  0.02376318\n",
            " 0.01376242 0.05455611 0.02640394 0.23626041 0.0675372  0.13477957\n",
            " 0.06608228 0.17150836 0.20930547 0.11049739 0.14595348 0.17330019\n",
            " 0.20812123 0.02106757 0.07712548 0.20196168 0.01564169 0.11641435\n",
            " 0.03333508 0.14913861 0.22561258 0.29245448 0.23102708 0.22531027\n",
            " 0.23592341 0.02941599 0.15962237 0.22754438 0.09065429 0.06748082\n",
            " 0.00730838 0.05509123 0.10081244 0.2353303  0.04436046 0.12448078\n",
            " 0.23104258 0.03962379 0.18872851 0.09647026 0.20408778 0.06952699\n",
            " 0.09108593 0.1712933 ], shape=(50,), dtype=float32)\n",
            "validation MSE: 0.1149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYc1EE-yrmVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a684cd89-8fe8-4d5e-c687-becdc9a5ecd0"
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "f1, f2, f3 = prediction.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(f1, f2, f3, 'o')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7fdaa4e729b0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZAks17s1Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use Lecture's"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}