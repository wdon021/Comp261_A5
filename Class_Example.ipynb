{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZ3tCy4s4nziX5jnYWKNAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdon021/Comp261_A5/blob/master/Class_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRDi0yBKDM5I",
        "colab_type": "text"
      },
      "source": [
        "% Title\n",
        "% ----\n",
        "\\title{Nueral Network: Projects 3D normal-dsitrbuted data onto a unit-radius Sphere}\n",
        "%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqsKBN_cDmV7",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "Task: Create a (fully connected) neural network model that approximate the function for project input data x (normal-distributed) onto a unit-radius sphere space.\n",
        "\n",
        "$y = f(x; \\theta, w) = \\phi(x; \\theta)^T w$\n",
        "\n",
        "Structure of report\n",
        "1. create Dataset X and corresponding Y that projected on the unit sphere.\n",
        "2. Define a cost function\n",
        "3. Define a optimization procedure\n",
        "4. Define a model\n",
        "5. Train the Model\n",
        "6. Define parameters\n",
        "7. Model performance\n",
        "8. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-bkKamI1UeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from mpl_toolkits import mplot3d\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tro7iEX-ew8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = [0, 0, 0]\n",
        "cov = [[1, 0, 0],\n",
        "       [0, 1, 0],\n",
        "       [0, 0, 1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dV4KPTEUMdA",
        "colab_type": "text"
      },
      "source": [
        "$\\mu = [0, 0, 0]$\n",
        "\n",
        "$\\Sigma=\n",
        "\\begin{bmatrix} \n",
        "1 & 0 & 0 \\\\\n",
        "0 & 1 & 0\\\\\n",
        "0 & 0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "X is generated with mean =0 and variance =1 (standard deviation = 1), they are randomly cluster around co-cordinate of [0, 0, 0] in a 3-D space.\n",
        "$X_i$ is linearly independent to each other with $i = \\{1, 2, 3\\}$. \n",
        "\n",
        "Y = X normalized by its euclidean distance.\n",
        "\n",
        "$Y_{ij} = \\frac{X_{ij}}{\\sqrt{X_{1j}^2 + X_{2j}^2+X_{3j}^2}}$\n",
        "\n",
        "Denominator is the radius of the sphere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b135hxdIfNxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unit_sphere(data):\n",
        "  return np.array([[coord / math.sqrt(sum(coords * coords for coords in line)) for coord in line] for line in data])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTrKt37dfPnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "united = np.random.multivariate_normal(mean, cov, 4000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObgwOT9zfTjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x11, x22, x33 = united.T\n",
        "united_y = unit_sphere(united)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEqaNvahfVfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "46fe8530-8400-4862-c533-5d92232e9f7b"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(x11, x22, x33, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7fdaefa3f208>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eajOQWVmZ2Il",
        "colab_type": "text"
      },
      "source": [
        "### Training, validation, test split\n",
        "- using the train_test_split() function provided in sklearn.\n",
        "- Call the function twice to acquire Training set, Validation set and test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PmRxaopfXGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(united, united_y, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4uQtuxLfcLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stLTEF21nI8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7020bec9-9cbe-4901-afad-2b1c79be28aa"
      },
      "source": [
        "x11, x22, x33 = Y_test.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(x11, x22, x33, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7f71f987bda0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6FVQAflffQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "812d0bd3-8bee-4ed1-83c5-d270c1ad6f55"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow import keras\n",
        "import keras.backend as kb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kacclnf9fglJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (3,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(3, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtnkaX6_cChQ",
        "colab_type": "text"
      },
      "source": [
        "### Define Nueral Network architecture\n",
        "\n",
        "This is a Three-layer feedforward neural network\n",
        "\n",
        "$f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$\n",
        "\n",
        "$f^{(1)}$ is the first hidden layer with 10 neurons and activation function = ReLU\n",
        "\n",
        "$F^{(1)}(x) = W^Tx$\n",
        "\n",
        "In our case $x=[x_1, x_2, x_3]$\n",
        "\n",
        "$h_i = max\\{0, x* \\begin{bmatrix}{w_1 \\\\ w_2 \\\\ w_3} \\end{bmatrix} + bias\\}$\n",
        "\n",
        "$h_i$ is the first neuron of the first hidden layer, $i= \\{1,2,..,10\\}$, in 1 row.\n",
        "We have 10 neurons (unit) in the first hidden layer, so we have to multiply 10 sets of weights\n",
        "\n",
        "$$\n",
        "max\\{0,\n",
        "\\begin{bmatrix}\n",
        "w_{0,1} & w_{0,2} & w_{0,3} \\\\\n",
        "w_{1,1} & w_{1,2} & w_{1,3} \\\\\n",
        "w_{2,1} & w_{2,2} & w_{2,3} \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "w_{9,1} & w_{9,2} & w_{9,3} \\\\\n",
        "\\end{bmatrix} *\\begin{bmatrix}{x_0^{(0)} \\\\ x_1^{(0)} \\\\ x_2^{(0)}} \\end{bmatrix} + \\begin{bmatrix}{b_0 \\\\ \\vdots \\\\ b_9} \\end{bmatrix}\\}\n",
        "$$\n",
        "\n",
        "Applying ReLU function to each specific component of the resulting vector inside.\n",
        "\n",
        "Now we have a vector of activations for the first hidden layer $h^{(1)}$.\n",
        "\n",
        "\n",
        "$f^{(2)}$ is the second hidden layer with 10 neurons, and ReLU activation functions.\n",
        "\n",
        "\n",
        "$$\n",
        "max\\{0,\n",
        "\\begin{bmatrix}\n",
        "w_{0,1} & \\cdots & w_{0,9} \\\\\n",
        "w_{1,1} & \\cdots & w_{1,9} \\\\\n",
        "w_{2,1} & \\cdots & w_{2,9} \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "w_{9,1} & \\cdots & w_{9,9} \\\\\n",
        "\\end{bmatrix} *\\begin{bmatrix}{h_0^{(1)} \\\\ h_1^{(1)} \\\\ \\vdots \\\\ h_9^{(1)}} \\end{bmatrix} + \\begin{bmatrix}{b_0 \\\\ \\vdots \\\\ b_9} \\end{bmatrix}\\}\n",
        "$$\n",
        "\n",
        "Applying ReLU function to each specific component of the resulting vector inside.\n",
        "\n",
        "Now we have a vector of activations for the second hidden layer $h^{(2)}$.\n",
        "\n",
        "\n",
        "$f^{(3)}$ is an output layer with 3 output units, and Linear activation\n",
        "\n",
        "$$\n",
        "c*\\{\n",
        "\\begin{bmatrix}\n",
        "w_{0,1} & \\cdots & w_{0,9} \\\\\n",
        "w_{1,1} & \\cdots & w_{1,9} \\\\\n",
        "w_{2,1} & \\cdots & w_{2,9} \\\\\n",
        "\\end{bmatrix} *\\begin{bmatrix}{h_0^{(2)} \\\\ h_1^{(2)} \\\\ \\vdots \\\\ h_9^{(2)}} \\end{bmatrix} + \\begin{bmatrix}{b_0 \\\\ b_1 \\\\ b_2} \\end{bmatrix}\\}\n",
        "$$\n",
        "\n",
        "Applying Linear function to each specific component of the resulting vector inside.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6XLrbgufomH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training dataset\n",
        "batch_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt-4C5xIfpRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tf.cast(X_train, tf.float32)\n",
        "Y_train = tf.cast(Y_train, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBaHU1stfuGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val = tf.cast(X_val, tf.float32)\n",
        "Y_val = tf.cast(Y_val, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPD_YTIzfrU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tf.cast(X_test, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--05jxiSf12q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates a dataset with a separate element fro each row of the input tensor\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(buffer_size= 3000).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).shuffle(1000).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cNuUWdXgG8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9M4pfzBg_Pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an loss function # not effective\n",
        "def Keras_loss_function(y_actual, y_predicted):\n",
        "  mse = kb.mean(kb.sum(kb.square(y_actual - y_predicted))) # (1/n*(sum(sqr(Y-Y_hat))))\n",
        "  return mse\n",
        "\n",
        "# first 100 epochs is useful"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdPR2mxckwEg",
        "colab_type": "text"
      },
      "source": [
        "### Design a cost function\n",
        "For MiniBatch = 100\n",
        "Y_actual and Y_predict is a 100 (rows) * 3(columns) matrix\n",
        "\n",
        "2 Options\n",
        "\n",
        "First Option - custom cost function\n",
        "\n",
        "$$\n",
        "\\sum\\limits_{j=1}^{100} \\sum\\limits_{i=1}^3 (Y_{actual(j,i)} - Y_{predict(j,i)})^2\n",
        "$$\n",
        "\n",
        "This is more traditional, one loss score for each mini-batch trained\n",
        "\n",
        "Second Option - tf.keras.losses.MSE\n",
        "\n",
        "$$\n",
        "\\sum\\limits_{i=1}^3 (Y_{actual(j,i)} - Y_{predict(j,i)})^2\n",
        "$$\n",
        "\n",
        "output is a vector of loss score associates with each row in the minibatch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UCU9yX8gL0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ddf8e62-5c28-4a22-ce48-a5d77ec84b04"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # print(linear)\n",
        "      # compute the loss value for this minibatch\n",
        "      mse = Keras_loss_function(y_batch_train, linear)\n",
        "      # mse = tf.keras.losses.MSE(y_batch_train, linear)\n",
        "      # print(mse)\n",
        "      # mse = keras.losses.mean_squared_error(y_batch_train, linear) # mse gives 3 number\n",
        "    # print(mse)\n",
        "    # use the gradient tap to automatically retrieve the gradients of the \n",
        "    # trainable variables with respect to the loss\n",
        "    grads = tape.gradient(mse, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training loss (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(mse))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    val_mse = Keras_loss_function(y_batch_val, val_linear)\n",
        "\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MSE: %.4f\" % (float(val_mse)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 161.4005\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 147.6059\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 137.6904\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 58.1657\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 120.0262\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 117.2203\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 104.5565\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 49.0517\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 95.2784\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 87.5972\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 79.9248\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 37.1034\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 79.6720\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 70.6396\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 64.2727\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 33.4465\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 63.4362\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 62.3900\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 53.8432\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 28.8013\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 57.6579\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 44.4591\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 42.6775\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 20.9858\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 39.9979\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 41.8617\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 36.7153\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 15.3387\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 30.5219\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 30.8408\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 26.2918\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 13.7357\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 30.2422\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 24.4422\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 22.3499\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 10.1965\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 22.9489\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 18.6880\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 17.2945\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 7.4732\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss (for one batch) at step 0: 15.9625\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 16.5612\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 12.4640\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 6.6211\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss (for one batch) at step 0: 14.5706\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 12.1288\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 12.9339\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 5.6841\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss (for one batch) at step 0: 14.6339\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 11.8489\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 10.4062\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 5.9855\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss (for one batch) at step 0: 10.0062\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 11.2406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 9.8656\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 5.5439\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss (for one batch) at step 0: 11.7557\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 9.1354\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 10.0619\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.3575\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss (for one batch) at step 0: 10.9192\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 10.0775\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 8.3272\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 7.4379\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss (for one batch) at step 0: 9.2606\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.1842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 10.3609\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.8895\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss (for one batch) at step 0: 8.9730\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 8.9031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 7.1273\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.1364\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss (for one batch) at step 0: 6.7487\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 8.2961\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 7.5687\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.3266\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss (for one batch) at step 0: 7.9723\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.7419\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 8.4609\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.9592\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss (for one batch) at step 0: 6.9390\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 8.5832\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.9196\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 4.2151\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss (for one batch) at step 0: 8.3061\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.8629\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.9617\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.6981\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss (for one batch) at step 0: 8.4706\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 6.1370\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 7.2550\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.0965\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss (for one batch) at step 0: 6.9504\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 6.8260\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.3372\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.9809\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss (for one batch) at step 0: 5.9817\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 7.1455\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.8666\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.9382\n",
            "\n",
            "Start of epoch 25\n",
            "Training loss (for one batch) at step 0: 6.4239\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 6.1291\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.5271\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.9455\n",
            "\n",
            "Start of epoch 26\n",
            "Training loss (for one batch) at step 0: 6.6714\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.4135\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.0965\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.1333\n",
            "\n",
            "Start of epoch 27\n",
            "Training loss (for one batch) at step 0: 8.5832\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.0511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.8042\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.9451\n",
            "\n",
            "Start of epoch 28\n",
            "Training loss (for one batch) at step 0: 5.8039\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.3209\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.7682\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.9315\n",
            "\n",
            "Start of epoch 29\n",
            "Training loss (for one batch) at step 0: 5.9864\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.2103\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.2727\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.0019\n",
            "\n",
            "Start of epoch 30\n",
            "Training loss (for one batch) at step 0: 5.0548\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.5250\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.6478\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8881\n",
            "\n",
            "Start of epoch 31\n",
            "Training loss (for one batch) at step 0: 6.3607\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.4465\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.3335\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.3976\n",
            "\n",
            "Start of epoch 32\n",
            "Training loss (for one batch) at step 0: 5.4797\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.1126\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.6231\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0937\n",
            "\n",
            "Start of epoch 33\n",
            "Training loss (for one batch) at step 0: 5.4853\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.8223\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 5.8291\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.3813\n",
            "\n",
            "Start of epoch 34\n",
            "Training loss (for one batch) at step 0: 4.8349\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 5.6266\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.5216\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.3329\n",
            "\n",
            "Start of epoch 35\n",
            "Training loss (for one batch) at step 0: 5.9206\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3111\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.3586\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.1555\n",
            "\n",
            "Start of epoch 36\n",
            "Training loss (for one batch) at step 0: 6.1300\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.3937\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.3623\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 3.1308\n",
            "\n",
            "Start of epoch 37\n",
            "Training loss (for one batch) at step 0: 5.6769\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3645\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.6614\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.3299\n",
            "\n",
            "Start of epoch 38\n",
            "Training loss (for one batch) at step 0: 4.5351\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.8096\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.9980\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0442\n",
            "\n",
            "Start of epoch 39\n",
            "Training loss (for one batch) at step 0: 5.0252\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6148\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 6.4934\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.6997\n",
            "\n",
            "Start of epoch 40\n",
            "Training loss (for one batch) at step 0: 4.5830\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3361\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.0236\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.4017\n",
            "\n",
            "Start of epoch 41\n",
            "Training loss (for one batch) at step 0: 5.4540\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.1556\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.7203\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5791\n",
            "\n",
            "Start of epoch 42\n",
            "Training loss (for one batch) at step 0: 4.7401\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.3058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4485\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1128\n",
            "\n",
            "Start of epoch 43\n",
            "Training loss (for one batch) at step 0: 3.5598\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.9240\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.9539\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8893\n",
            "\n",
            "Start of epoch 44\n",
            "Training loss (for one batch) at step 0: 4.3813\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.9342\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.9947\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1818\n",
            "\n",
            "Start of epoch 45\n",
            "Training loss (for one batch) at step 0: 4.5287\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.6597\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4426\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8483\n",
            "\n",
            "Start of epoch 46\n",
            "Training loss (for one batch) at step 0: 5.1917\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.4089\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.2259\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1631\n",
            "\n",
            "Start of epoch 47\n",
            "Training loss (for one batch) at step 0: 3.7250\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.6367\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.0000\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.5545\n",
            "\n",
            "Start of epoch 48\n",
            "Training loss (for one batch) at step 0: 4.7059\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.9614\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.2236\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.6832\n",
            "\n",
            "Start of epoch 49\n",
            "Training loss (for one batch) at step 0: 3.0964\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1740\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.5082\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.6696\n",
            "\n",
            "Start of epoch 50\n",
            "Training loss (for one batch) at step 0: 4.0435\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.6406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4092\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4593\n",
            "\n",
            "Start of epoch 51\n",
            "Training loss (for one batch) at step 0: 3.1931\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.2903\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.7256\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6036\n",
            "\n",
            "Start of epoch 52\n",
            "Training loss (for one batch) at step 0: 4.6940\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6163\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.6057\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.9271\n",
            "\n",
            "Start of epoch 53\n",
            "Training loss (for one batch) at step 0: 3.2313\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8138\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 4.0431\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6836\n",
            "\n",
            "Start of epoch 54\n",
            "Training loss (for one batch) at step 0: 2.8758\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.1996\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.0268\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4621\n",
            "\n",
            "Start of epoch 55\n",
            "Training loss (for one batch) at step 0: 3.6751\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.8736\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4258\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2221\n",
            "\n",
            "Start of epoch 56\n",
            "Training loss (for one batch) at step 0: 3.0680\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9169\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.3309\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5673\n",
            "\n",
            "Start of epoch 57\n",
            "Training loss (for one batch) at step 0: 2.7438\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.3628\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4544\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4664\n",
            "\n",
            "Start of epoch 58\n",
            "Training loss (for one batch) at step 0: 3.4573\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6859\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9818\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0119\n",
            "\n",
            "Start of epoch 59\n",
            "Training loss (for one batch) at step 0: 3.9461\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6269\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4193\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8424\n",
            "\n",
            "Start of epoch 60\n",
            "Training loss (for one batch) at step 0: 2.8683\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1723\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.3538\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.2304\n",
            "\n",
            "Start of epoch 61\n",
            "Training loss (for one batch) at step 0: 3.5622\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.7039\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0905\n",
            "\n",
            "Start of epoch 62\n",
            "Training loss (for one batch) at step 0: 2.9276\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.0941\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.1288\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.9646\n",
            "\n",
            "Start of epoch 63\n",
            "Training loss (for one batch) at step 0: 2.6209\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.7903\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.4926\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.2800\n",
            "\n",
            "Start of epoch 64\n",
            "Training loss (for one batch) at step 0: 3.8605\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.5895\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4741\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.7785\n",
            "\n",
            "Start of epoch 65\n",
            "Training loss (for one batch) at step 0: 3.0242\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.6027\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9253\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2571\n",
            "\n",
            "Start of epoch 66\n",
            "Training loss (for one batch) at step 0: 3.1141\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3944\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.1803\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.0197\n",
            "\n",
            "Start of epoch 67\n",
            "Training loss (for one batch) at step 0: 2.8410\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9430\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9759\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5403\n",
            "\n",
            "Start of epoch 68\n",
            "Training loss (for one batch) at step 0: 3.0726\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.7035\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.5890\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.3464\n",
            "\n",
            "Start of epoch 69\n",
            "Training loss (for one batch) at step 0: 3.3589\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1289\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7489\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0643\n",
            "\n",
            "Start of epoch 70\n",
            "Training loss (for one batch) at step 0: 2.8664\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 4.2929\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7890\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1638\n",
            "\n",
            "Start of epoch 71\n",
            "Training loss (for one batch) at step 0: 2.5620\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.7403\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.3094\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2305\n",
            "\n",
            "Start of epoch 72\n",
            "Training loss (for one batch) at step 0: 2.5207\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6283\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9305\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.7219\n",
            "\n",
            "Start of epoch 73\n",
            "Training loss (for one batch) at step 0: 3.2698\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5613\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.0439\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.7442\n",
            "\n",
            "Start of epoch 74\n",
            "Training loss (for one batch) at step 0: 2.2441\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6619\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0443\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1322\n",
            "\n",
            "Start of epoch 75\n",
            "Training loss (for one batch) at step 0: 2.4507\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.4444\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3147\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.8361\n",
            "\n",
            "Start of epoch 76\n",
            "Training loss (for one batch) at step 0: 2.3897\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.3674\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.5445\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6227\n",
            "\n",
            "Start of epoch 77\n",
            "Training loss (for one batch) at step 0: 2.1708\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8098\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1817\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 2.1695\n",
            "\n",
            "Start of epoch 78\n",
            "Training loss (for one batch) at step 0: 3.0982\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.7786\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3401\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1979\n",
            "\n",
            "Start of epoch 79\n",
            "Training loss (for one batch) at step 0: 2.9169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9654\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.9181\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5812\n",
            "\n",
            "Start of epoch 80\n",
            "Training loss (for one batch) at step 0: 2.5255\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3228\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.2280\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0088\n",
            "\n",
            "Start of epoch 81\n",
            "Training loss (for one batch) at step 0: 2.6575\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9927\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7589\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0442\n",
            "\n",
            "Start of epoch 82\n",
            "Training loss (for one batch) at step 0: 3.5510\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4712\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3000\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9910\n",
            "\n",
            "Start of epoch 83\n",
            "Training loss (for one batch) at step 0: 2.2460\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6399\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5630\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5014\n",
            "\n",
            "Start of epoch 84\n",
            "Training loss (for one batch) at step 0: 2.6309\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6540\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3569\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0785\n",
            "\n",
            "Start of epoch 85\n",
            "Training loss (for one batch) at step 0: 2.0259\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4197\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0923\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8822\n",
            "\n",
            "Start of epoch 86\n",
            "Training loss (for one batch) at step 0: 2.2104\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3580\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9857\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5740\n",
            "\n",
            "Start of epoch 87\n",
            "Training loss (for one batch) at step 0: 2.1709\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1688\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6821\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4141\n",
            "\n",
            "Start of epoch 88\n",
            "Training loss (for one batch) at step 0: 1.9257\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.1702\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6543\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0796\n",
            "\n",
            "Start of epoch 89\n",
            "Training loss (for one batch) at step 0: 2.1333\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3025\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.8367\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1648\n",
            "\n",
            "Start of epoch 90\n",
            "Training loss (for one batch) at step 0: 2.0139\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5765\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7031\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4796\n",
            "\n",
            "Start of epoch 91\n",
            "Training loss (for one batch) at step 0: 2.8871\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5762\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7302\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9554\n",
            "\n",
            "Start of epoch 92\n",
            "Training loss (for one batch) at step 0: 1.9396\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2297\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6002\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1053\n",
            "\n",
            "Start of epoch 93\n",
            "Training loss (for one batch) at step 0: 1.8817\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1344\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.2133\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.9579\n",
            "\n",
            "Start of epoch 94\n",
            "Training loss (for one batch) at step 0: 2.2038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2833\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0275\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8422\n",
            "\n",
            "Start of epoch 95\n",
            "Training loss (for one batch) at step 0: 1.9234\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8422\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7894\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1764\n",
            "\n",
            "Start of epoch 96\n",
            "Training loss (for one batch) at step 0: 1.9614\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6975\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4036\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0166\n",
            "\n",
            "Start of epoch 97\n",
            "Training loss (for one batch) at step 0: 3.0127\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6297\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.4336\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0470\n",
            "\n",
            "Start of epoch 98\n",
            "Training loss (for one batch) at step 0: 1.9880\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3899\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9978\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8056\n",
            "\n",
            "Start of epoch 99\n",
            "Training loss (for one batch) at step 0: 2.8859\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.2984\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6247\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7205\n",
            "\n",
            "Start of epoch 100\n",
            "Training loss (for one batch) at step 0: 2.0890\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2611\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8366\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7854\n",
            "\n",
            "Start of epoch 101\n",
            "Training loss (for one batch) at step 0: 2.1319\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9132\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3305\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4953\n",
            "\n",
            "Start of epoch 102\n",
            "Training loss (for one batch) at step 0: 2.0046\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2316\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.5878\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9412\n",
            "\n",
            "Start of epoch 103\n",
            "Training loss (for one batch) at step 0: 2.2152\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6724\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7925\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0498\n",
            "\n",
            "Start of epoch 104\n",
            "Training loss (for one batch) at step 0: 1.8064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7180\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9688\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5222\n",
            "\n",
            "Start of epoch 105\n",
            "Training loss (for one batch) at step 0: 2.0685\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.5401\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5102\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9693\n",
            "\n",
            "Start of epoch 106\n",
            "Training loss (for one batch) at step 0: 1.9050\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4961\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9055\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6831\n",
            "\n",
            "Start of epoch 107\n",
            "Training loss (for one batch) at step 0: 2.4128\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4879\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.8362\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7396\n",
            "\n",
            "Start of epoch 108\n",
            "Training loss (for one batch) at step 0: 1.7929\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4333\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1480\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2103\n",
            "\n",
            "Start of epoch 109\n",
            "Training loss (for one batch) at step 0: 1.7326\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0308\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3413\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8071\n",
            "\n",
            "Start of epoch 110\n",
            "Training loss (for one batch) at step 0: 1.3639\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2224\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1343\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5592\n",
            "\n",
            "Start of epoch 111\n",
            "Training loss (for one batch) at step 0: 1.9410\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6514\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.4109\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8256\n",
            "\n",
            "Start of epoch 112\n",
            "Training loss (for one batch) at step 0: 2.3453\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6280\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.8573\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1556\n",
            "\n",
            "Start of epoch 113\n",
            "Training loss (for one batch) at step 0: 1.8310\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6247\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.7694\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4016\n",
            "\n",
            "Start of epoch 114\n",
            "Training loss (for one batch) at step 0: 2.1006\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2877\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9786\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1502\n",
            "\n",
            "Start of epoch 115\n",
            "Training loss (for one batch) at step 0: 1.6588\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8841\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.4592\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2881\n",
            "\n",
            "Start of epoch 116\n",
            "Training loss (for one batch) at step 0: 3.4464\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.9004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8177\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0800\n",
            "\n",
            "Start of epoch 117\n",
            "Training loss (for one batch) at step 0: 2.1663\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7166\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9593\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7827\n",
            "\n",
            "Start of epoch 118\n",
            "Training loss (for one batch) at step 0: 2.1488\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.8269\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5254\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3995\n",
            "\n",
            "Start of epoch 119\n",
            "Training loss (for one batch) at step 0: 1.5045\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0306\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3495\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0403\n",
            "\n",
            "Start of epoch 120\n",
            "Training loss (for one batch) at step 0: 1.6905\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8131\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.6350\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8351\n",
            "\n",
            "Start of epoch 121\n",
            "Training loss (for one batch) at step 0: 1.9286\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.6336\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.2042\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6534\n",
            "\n",
            "Start of epoch 122\n",
            "Training loss (for one batch) at step 0: 1.9398\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6130\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0226\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8153\n",
            "\n",
            "Start of epoch 123\n",
            "Training loss (for one batch) at step 0: 1.8303\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4915\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5235\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7792\n",
            "\n",
            "Start of epoch 124\n",
            "Training loss (for one batch) at step 0: 2.2011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5909\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5401\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6716\n",
            "\n",
            "Start of epoch 125\n",
            "Training loss (for one batch) at step 0: 1.8069\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7832\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9743\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7946\n",
            "\n",
            "Start of epoch 126\n",
            "Training loss (for one batch) at step 0: 1.9476\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8533\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0338\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0166\n",
            "\n",
            "Start of epoch 127\n",
            "Training loss (for one batch) at step 0: 1.5204\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2114\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8328\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8898\n",
            "\n",
            "Start of epoch 128\n",
            "Training loss (for one batch) at step 0: 1.6748\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5077\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3706\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7607\n",
            "\n",
            "Start of epoch 129\n",
            "Training loss (for one batch) at step 0: 1.5120\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9832\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9976\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7708\n",
            "\n",
            "Start of epoch 130\n",
            "Training loss (for one batch) at step 0: 1.7985\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4874\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5076\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7573\n",
            "\n",
            "Start of epoch 131\n",
            "Training loss (for one batch) at step 0: 1.8226\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6452\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1190\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0087\n",
            "\n",
            "Start of epoch 132\n",
            "Training loss (for one batch) at step 0: 2.0092\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4777\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6616\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1827\n",
            "\n",
            "Start of epoch 133\n",
            "Training loss (for one batch) at step 0: 2.2702\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9737\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5372\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0941\n",
            "\n",
            "Start of epoch 134\n",
            "Training loss (for one batch) at step 0: 1.9282\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6285\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 3.2200\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8169\n",
            "\n",
            "Start of epoch 135\n",
            "Training loss (for one batch) at step 0: 2.0751\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4268\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3448\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7912\n",
            "\n",
            "Start of epoch 136\n",
            "Training loss (for one batch) at step 0: 1.4561\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4030\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7289\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6456\n",
            "\n",
            "Start of epoch 137\n",
            "Training loss (for one batch) at step 0: 1.4184\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6066\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7268\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.5513\n",
            "\n",
            "Start of epoch 138\n",
            "Training loss (for one batch) at step 0: 1.6622\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6152\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0724\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1067\n",
            "\n",
            "Start of epoch 139\n",
            "Training loss (for one batch) at step 0: 1.5167\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7115\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7065\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7297\n",
            "\n",
            "Start of epoch 140\n",
            "Training loss (for one batch) at step 0: 1.7559\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1615\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7153\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9518\n",
            "\n",
            "Start of epoch 141\n",
            "Training loss (for one batch) at step 0: 1.4967\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.4839\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9030\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7691\n",
            "\n",
            "Start of epoch 142\n",
            "Training loss (for one batch) at step 0: 1.2859\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8825\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8854\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9788\n",
            "\n",
            "Start of epoch 143\n",
            "Training loss (for one batch) at step 0: 2.0166\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6967\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8762\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0649\n",
            "\n",
            "Start of epoch 144\n",
            "Training loss (for one batch) at step 0: 1.4295\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9903\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2370\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8335\n",
            "\n",
            "Start of epoch 145\n",
            "Training loss (for one batch) at step 0: 1.2711\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9570\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6061\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.2820\n",
            "\n",
            "Start of epoch 146\n",
            "Training loss (for one batch) at step 0: 1.8889\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8636\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9652\n",
            "\n",
            "Start of epoch 147\n",
            "Training loss (for one batch) at step 0: 1.6557\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7957\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7241\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5703\n",
            "\n",
            "Start of epoch 148\n",
            "Training loss (for one batch) at step 0: 1.5615\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5012\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5215\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8248\n",
            "\n",
            "Start of epoch 149\n",
            "Training loss (for one batch) at step 0: 1.0664\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8147\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5908\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8394\n",
            "\n",
            "Start of epoch 150\n",
            "Training loss (for one batch) at step 0: 2.0575\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5771\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6822\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7167\n",
            "\n",
            "Start of epoch 151\n",
            "Training loss (for one batch) at step 0: 1.6212\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6224\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6632\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3384\n",
            "\n",
            "Start of epoch 152\n",
            "Training loss (for one batch) at step 0: 1.9661\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2233\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5020\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8555\n",
            "\n",
            "Start of epoch 153\n",
            "Training loss (for one batch) at step 0: 1.2540\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9951\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1981\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8765\n",
            "\n",
            "Start of epoch 154\n",
            "Training loss (for one batch) at step 0: 1.7372\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5641\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9478\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7560\n",
            "\n",
            "Start of epoch 155\n",
            "Training loss (for one batch) at step 0: 1.7874\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1889\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8942\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.4667\n",
            "\n",
            "Start of epoch 156\n",
            "Training loss (for one batch) at step 0: 1.3985\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3562\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7692\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7454\n",
            "\n",
            "Start of epoch 157\n",
            "Training loss (for one batch) at step 0: 1.6663\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5330\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8290\n",
            "\n",
            "Start of epoch 158\n",
            "Training loss (for one batch) at step 0: 1.7429\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 3.5135\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8410\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7023\n",
            "\n",
            "Start of epoch 159\n",
            "Training loss (for one batch) at step 0: 1.6197\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3396\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3250\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5261\n",
            "\n",
            "Start of epoch 160\n",
            "Training loss (for one batch) at step 0: 1.3233\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4585\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3111\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1113\n",
            "\n",
            "Start of epoch 161\n",
            "Training loss (for one batch) at step 0: 2.2184\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5321\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.1886\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7215\n",
            "\n",
            "Start of epoch 162\n",
            "Training loss (for one batch) at step 0: 1.2845\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7087\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0373\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7035\n",
            "\n",
            "Start of epoch 163\n",
            "Training loss (for one batch) at step 0: 2.0563\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3524\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6467\n",
            "\n",
            "Start of epoch 164\n",
            "Training loss (for one batch) at step 0: 1.8244\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6424\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4358\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6013\n",
            "\n",
            "Start of epoch 165\n",
            "Training loss (for one batch) at step 0: 1.8339\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5021\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1949\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3530\n",
            "\n",
            "Start of epoch 166\n",
            "Training loss (for one batch) at step 0: 1.3296\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2179\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5686\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1807\n",
            "\n",
            "Start of epoch 167\n",
            "Training loss (for one batch) at step 0: 1.6424\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2869\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4166\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5090\n",
            "\n",
            "Start of epoch 168\n",
            "Training loss (for one batch) at step 0: 1.4427\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3698\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6111\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8447\n",
            "\n",
            "Start of epoch 169\n",
            "Training loss (for one batch) at step 0: 1.0169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4185\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4977\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8508\n",
            "\n",
            "Start of epoch 170\n",
            "Training loss (for one batch) at step 0: 2.1959\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0691\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2602\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1318\n",
            "\n",
            "Start of epoch 171\n",
            "Training loss (for one batch) at step 0: 1.3798\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0636\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6888\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6035\n",
            "\n",
            "Start of epoch 172\n",
            "Training loss (for one batch) at step 0: 1.5042\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2092\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9954\n",
            "\n",
            "Start of epoch 173\n",
            "Training loss (for one batch) at step 0: 1.5788\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5042\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4104\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7363\n",
            "\n",
            "Start of epoch 174\n",
            "Training loss (for one batch) at step 0: 2.0137\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1340\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2505\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7182\n",
            "\n",
            "Start of epoch 175\n",
            "Training loss (for one batch) at step 0: 1.2315\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5596\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5528\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6750\n",
            "\n",
            "Start of epoch 176\n",
            "Training loss (for one batch) at step 0: 1.5906\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.9306\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5360\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6940\n",
            "\n",
            "Start of epoch 177\n",
            "Training loss (for one batch) at step 0: 1.6272\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1976\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5550\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.6778\n",
            "\n",
            "Start of epoch 178\n",
            "Training loss (for one batch) at step 0: 1.3726\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0945\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2806\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6962\n",
            "\n",
            "Start of epoch 179\n",
            "Training loss (for one batch) at step 0: 1.4089\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0315\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5347\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6744\n",
            "\n",
            "Start of epoch 180\n",
            "Training loss (for one batch) at step 0: 1.3851\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.1897\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0630\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8689\n",
            "\n",
            "Start of epoch 181\n",
            "Training loss (for one batch) at step 0: 1.5214\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3043\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3025\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6125\n",
            "\n",
            "Start of epoch 182\n",
            "Training loss (for one batch) at step 0: 1.4768\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4008\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3739\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9420\n",
            "\n",
            "Start of epoch 183\n",
            "Training loss (for one batch) at step 0: 1.2138\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6245\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8150\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1057\n",
            "\n",
            "Start of epoch 184\n",
            "Training loss (for one batch) at step 0: 1.2016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3498\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3476\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5774\n",
            "\n",
            "Start of epoch 185\n",
            "Training loss (for one batch) at step 0: 1.2960\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1087\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.0028\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4963\n",
            "\n",
            "Start of epoch 186\n",
            "Training loss (for one batch) at step 0: 1.2774\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1320\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3615\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8613\n",
            "\n",
            "Start of epoch 187\n",
            "Training loss (for one batch) at step 0: 0.9367\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2936\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4828\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5987\n",
            "\n",
            "Start of epoch 188\n",
            "Training loss (for one batch) at step 0: 1.3236\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1800\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2885\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6027\n",
            "\n",
            "Start of epoch 189\n",
            "Training loss (for one batch) at step 0: 1.1305\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1318\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8201\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7457\n",
            "\n",
            "Start of epoch 190\n",
            "Training loss (for one batch) at step 0: 1.2434\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3824\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8254\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9233\n",
            "\n",
            "Start of epoch 191\n",
            "Training loss (for one batch) at step 0: 1.1165\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1808\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5960\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1288\n",
            "\n",
            "Start of epoch 192\n",
            "Training loss (for one batch) at step 0: 1.3035\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5769\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0071\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7276\n",
            "\n",
            "Start of epoch 193\n",
            "Training loss (for one batch) at step 0: 1.7639\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4086\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3086\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5929\n",
            "\n",
            "Start of epoch 194\n",
            "Training loss (for one batch) at step 0: 1.1319\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4617\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7866\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7660\n",
            "\n",
            "Start of epoch 195\n",
            "Training loss (for one batch) at step 0: 1.4162\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4542\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0947\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8517\n",
            "\n",
            "Start of epoch 196\n",
            "Training loss (for one batch) at step 0: 1.3369\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0146\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2387\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7022\n",
            "\n",
            "Start of epoch 197\n",
            "Training loss (for one batch) at step 0: 1.0274\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3517\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2017\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9107\n",
            "\n",
            "Start of epoch 198\n",
            "Training loss (for one batch) at step 0: 1.3191\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3078\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0963\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0777\n",
            "\n",
            "Start of epoch 199\n",
            "Training loss (for one batch) at step 0: 1.0223\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9854\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0735\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.1431\n",
            "\n",
            "Start of epoch 200\n",
            "Training loss (for one batch) at step 0: 1.3475\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2679\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9629\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7054\n",
            "\n",
            "Start of epoch 201\n",
            "Training loss (for one batch) at step 0: 0.9543\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9215\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5543\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8276\n",
            "\n",
            "Start of epoch 202\n",
            "Training loss (for one batch) at step 0: 2.0011\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3024\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7176\n",
            "\n",
            "Start of epoch 203\n",
            "Training loss (for one batch) at step 0: 1.4340\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4414\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8063\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6263\n",
            "\n",
            "Start of epoch 204\n",
            "Training loss (for one batch) at step 0: 1.1422\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.2667\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1566\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5211\n",
            "\n",
            "Start of epoch 205\n",
            "Training loss (for one batch) at step 0: 1.5170\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4404\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1847\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5416\n",
            "\n",
            "Start of epoch 206\n",
            "Training loss (for one batch) at step 0: 1.2105\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3288\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9107\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8553\n",
            "\n",
            "Start of epoch 207\n",
            "Training loss (for one batch) at step 0: 1.6613\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6323\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0210\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5792\n",
            "\n",
            "Start of epoch 208\n",
            "Training loss (for one batch) at step 0: 0.9294\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3578\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0760\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5148\n",
            "\n",
            "Start of epoch 209\n",
            "Training loss (for one batch) at step 0: 1.5028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1491\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2019\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3574\n",
            "\n",
            "Start of epoch 210\n",
            "Training loss (for one batch) at step 0: 1.1687\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3549\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5469\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5403\n",
            "\n",
            "Start of epoch 211\n",
            "Training loss (for one batch) at step 0: 1.0325\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4267\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2230\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3587\n",
            "\n",
            "Start of epoch 212\n",
            "Training loss (for one batch) at step 0: 1.3030\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4950\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7204\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4373\n",
            "\n",
            "Start of epoch 213\n",
            "Training loss (for one batch) at step 0: 1.5634\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4063\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0454\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4242\n",
            "\n",
            "Start of epoch 214\n",
            "Training loss (for one batch) at step 0: 1.5786\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2435\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3258\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5253\n",
            "\n",
            "Start of epoch 215\n",
            "Training loss (for one batch) at step 0: 1.1049\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0236\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9475\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6495\n",
            "\n",
            "Start of epoch 216\n",
            "Training loss (for one batch) at step 0: 0.8357\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0799\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7638\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4296\n",
            "\n",
            "Start of epoch 217\n",
            "Training loss (for one batch) at step 0: 1.4345\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0152\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0091\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6206\n",
            "\n",
            "Start of epoch 218\n",
            "Training loss (for one batch) at step 0: 1.0344\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0491\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3907\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5475\n",
            "\n",
            "Start of epoch 219\n",
            "Training loss (for one batch) at step 0: 1.0097\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2767\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2771\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6528\n",
            "\n",
            "Start of epoch 220\n",
            "Training loss (for one batch) at step 0: 1.3207\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3426\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4927\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6146\n",
            "\n",
            "Start of epoch 221\n",
            "Training loss (for one batch) at step 0: 0.9842\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.3908\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5757\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8169\n",
            "\n",
            "Start of epoch 222\n",
            "Training loss (for one batch) at step 0: 1.7290\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1218\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8459\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3991\n",
            "\n",
            "Start of epoch 223\n",
            "Training loss (for one batch) at step 0: 1.2797\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1147\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0497\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5589\n",
            "\n",
            "Start of epoch 224\n",
            "Training loss (for one batch) at step 0: 1.0019\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8875\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9340\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7198\n",
            "\n",
            "Start of epoch 225\n",
            "Training loss (for one batch) at step 0: 0.9107\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4711\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7002\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7253\n",
            "\n",
            "Start of epoch 226\n",
            "Training loss (for one batch) at step 0: 1.0515\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4920\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1373\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7863\n",
            "\n",
            "Start of epoch 227\n",
            "Training loss (for one batch) at step 0: 1.0510\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1264\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9344\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9712\n",
            "\n",
            "Start of epoch 228\n",
            "Training loss (for one batch) at step 0: 1.1199\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0114\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2220\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9542\n",
            "\n",
            "Start of epoch 229\n",
            "Training loss (for one batch) at step 0: 1.6525\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8525\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0275\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7483\n",
            "\n",
            "Start of epoch 230\n",
            "Training loss (for one batch) at step 0: 0.9888\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6547\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4380\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4454\n",
            "\n",
            "Start of epoch 231\n",
            "Training loss (for one batch) at step 0: 0.9056\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6808\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1616\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6282\n",
            "\n",
            "Start of epoch 232\n",
            "Training loss (for one batch) at step 0: 1.0997\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0004\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9219\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6711\n",
            "\n",
            "Start of epoch 233\n",
            "Training loss (for one batch) at step 0: 1.0589\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9538\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9537\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7611\n",
            "\n",
            "Start of epoch 234\n",
            "Training loss (for one batch) at step 0: 1.2122\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1246\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2679\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4717\n",
            "\n",
            "Start of epoch 235\n",
            "Training loss (for one batch) at step 0: 1.4702\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9943\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7852\n",
            "\n",
            "Start of epoch 236\n",
            "Training loss (for one batch) at step 0: 1.3407\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1390\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9060\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6230\n",
            "\n",
            "Start of epoch 237\n",
            "Training loss (for one batch) at step 0: 0.9494\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2196\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 2.3370\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4571\n",
            "\n",
            "Start of epoch 238\n",
            "Training loss (for one batch) at step 0: 0.9680\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2797\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2105\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6602\n",
            "\n",
            "Start of epoch 239\n",
            "Training loss (for one batch) at step 0: 0.8601\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7031\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8929\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3703\n",
            "\n",
            "Start of epoch 240\n",
            "Training loss (for one batch) at step 0: 1.0997\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4334\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0409\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8825\n",
            "\n",
            "Start of epoch 241\n",
            "Training loss (for one batch) at step 0: 0.9282\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8625\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2936\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5224\n",
            "\n",
            "Start of epoch 242\n",
            "Training loss (for one batch) at step 0: 1.0381\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7220\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6892\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8266\n",
            "\n",
            "Start of epoch 243\n",
            "Training loss (for one batch) at step 0: 0.8926\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0618\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1565\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6043\n",
            "\n",
            "Start of epoch 244\n",
            "Training loss (for one batch) at step 0: 1.1226\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9448\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4611\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4196\n",
            "\n",
            "Start of epoch 245\n",
            "Training loss (for one batch) at step 0: 0.9801\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9789\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3825\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6576\n",
            "\n",
            "Start of epoch 246\n",
            "Training loss (for one batch) at step 0: 1.3340\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8572\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2866\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3714\n",
            "\n",
            "Start of epoch 247\n",
            "Training loss (for one batch) at step 0: 1.3524\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0496\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1088\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5224\n",
            "\n",
            "Start of epoch 248\n",
            "Training loss (for one batch) at step 0: 1.8186\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8515\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6250\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3997\n",
            "\n",
            "Start of epoch 249\n",
            "Training loss (for one batch) at step 0: 0.9415\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1436\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9409\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5939\n",
            "\n",
            "Start of epoch 250\n",
            "Training loss (for one batch) at step 0: 0.8614\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3792\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9236\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4733\n",
            "\n",
            "Start of epoch 251\n",
            "Training loss (for one batch) at step 0: 0.7997\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.6365\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2239\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6241\n",
            "\n",
            "Start of epoch 252\n",
            "Training loss (for one batch) at step 0: 1.0610\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2582\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9681\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7830\n",
            "\n",
            "Start of epoch 253\n",
            "Training loss (for one batch) at step 0: 1.2835\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9195\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9154\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5007\n",
            "\n",
            "Start of epoch 254\n",
            "Training loss (for one batch) at step 0: 1.2999\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9887\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1841\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4436\n",
            "\n",
            "Start of epoch 255\n",
            "Training loss (for one batch) at step 0: 1.3940\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1285\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4175\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5876\n",
            "\n",
            "Start of epoch 256\n",
            "Training loss (for one batch) at step 0: 0.8168\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9203\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8124\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4730\n",
            "\n",
            "Start of epoch 257\n",
            "Training loss (for one batch) at step 0: 0.9665\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2271\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1423\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6575\n",
            "\n",
            "Start of epoch 258\n",
            "Training loss (for one batch) at step 0: 1.0862\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0820\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1551\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7572\n",
            "\n",
            "Start of epoch 259\n",
            "Training loss (for one batch) at step 0: 1.1958\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3583\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2242\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4802\n",
            "\n",
            "Start of epoch 260\n",
            "Training loss (for one batch) at step 0: 1.1903\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9078\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2115\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4887\n",
            "\n",
            "Start of epoch 261\n",
            "Training loss (for one batch) at step 0: 1.2370\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9788\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4083\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5931\n",
            "\n",
            "Start of epoch 262\n",
            "Training loss (for one batch) at step 0: 1.2373\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4504\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8527\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7393\n",
            "\n",
            "Start of epoch 263\n",
            "Training loss (for one batch) at step 0: 1.3434\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1138\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1036\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5043\n",
            "\n",
            "Start of epoch 264\n",
            "Training loss (for one batch) at step 0: 1.2658\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3260\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0697\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5349\n",
            "\n",
            "Start of epoch 265\n",
            "Training loss (for one batch) at step 0: 1.2841\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2880\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1756\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7440\n",
            "\n",
            "Start of epoch 266\n",
            "Training loss (for one batch) at step 0: 0.9769\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4111\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0435\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6345\n",
            "\n",
            "Start of epoch 267\n",
            "Training loss (for one batch) at step 0: 0.8164\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8099\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8710\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5727\n",
            "\n",
            "Start of epoch 268\n",
            "Training loss (for one batch) at step 0: 0.8539\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0980\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9584\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8287\n",
            "\n",
            "Start of epoch 269\n",
            "Training loss (for one batch) at step 0: 1.0766\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9350\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3066\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5970\n",
            "\n",
            "Start of epoch 270\n",
            "Training loss (for one batch) at step 0: 1.1627\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0695\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8166\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6682\n",
            "\n",
            "Start of epoch 271\n",
            "Training loss (for one batch) at step 0: 1.0479\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8660\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8704\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0693\n",
            "\n",
            "Start of epoch 272\n",
            "Training loss (for one batch) at step 0: 1.2467\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8951\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3322\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4709\n",
            "\n",
            "Start of epoch 273\n",
            "Training loss (for one batch) at step 0: 0.9013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3255\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7778\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6108\n",
            "\n",
            "Start of epoch 274\n",
            "Training loss (for one batch) at step 0: 1.1881\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9075\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5965\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6289\n",
            "\n",
            "Start of epoch 275\n",
            "Training loss (for one batch) at step 0: 1.0105\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1860\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1243\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5310\n",
            "\n",
            "Start of epoch 276\n",
            "Training loss (for one batch) at step 0: 1.0142\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4554\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0441\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4829\n",
            "\n",
            "Start of epoch 277\n",
            "Training loss (for one batch) at step 0: 0.9591\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3069\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2597\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5368\n",
            "\n",
            "Start of epoch 278\n",
            "Training loss (for one batch) at step 0: 1.0778\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3405\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8824\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6000\n",
            "\n",
            "Start of epoch 279\n",
            "Training loss (for one batch) at step 0: 1.1291\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9554\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4493\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8546\n",
            "\n",
            "Start of epoch 280\n",
            "Training loss (for one batch) at step 0: 1.0452\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2879\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8688\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6745\n",
            "\n",
            "Start of epoch 281\n",
            "Training loss (for one batch) at step 0: 1.0369\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4642\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1099\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6622\n",
            "\n",
            "Start of epoch 282\n",
            "Training loss (for one batch) at step 0: 1.5473\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1283\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8723\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6313\n",
            "\n",
            "Start of epoch 283\n",
            "Training loss (for one batch) at step 0: 0.7447\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0439\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4655\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7091\n",
            "\n",
            "Start of epoch 284\n",
            "Training loss (for one batch) at step 0: 0.8629\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9861\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0337\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4586\n",
            "\n",
            "Start of epoch 285\n",
            "Training loss (for one batch) at step 0: 0.7962\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2227\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0086\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4689\n",
            "\n",
            "Start of epoch 286\n",
            "Training loss (for one batch) at step 0: 0.8951\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9750\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6528\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5474\n",
            "\n",
            "Start of epoch 287\n",
            "Training loss (for one batch) at step 0: 1.2891\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0186\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5186\n",
            "\n",
            "Start of epoch 288\n",
            "Training loss (for one batch) at step 0: 1.3760\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9006\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0695\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0304\n",
            "\n",
            "Start of epoch 289\n",
            "Training loss (for one batch) at step 0: 1.3774\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1439\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1384\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9102\n",
            "\n",
            "Start of epoch 290\n",
            "Training loss (for one batch) at step 0: 1.4171\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3563\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0499\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8552\n",
            "\n",
            "Start of epoch 291\n",
            "Training loss (for one batch) at step 0: 1.2409\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3866\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8621\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6541\n",
            "\n",
            "Start of epoch 292\n",
            "Training loss (for one batch) at step 0: 1.0199\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8917\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8867\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6389\n",
            "\n",
            "Start of epoch 293\n",
            "Training loss (for one batch) at step 0: 1.0955\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0518\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5076\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4765\n",
            "\n",
            "Start of epoch 294\n",
            "Training loss (for one batch) at step 0: 0.8155\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8432\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6905\n",
            "\n",
            "Start of epoch 295\n",
            "Training loss (for one batch) at step 0: 0.7704\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5212\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0299\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7925\n",
            "\n",
            "Start of epoch 296\n",
            "Training loss (for one batch) at step 0: 0.9681\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9618\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0843\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4550\n",
            "\n",
            "Start of epoch 297\n",
            "Training loss (for one batch) at step 0: 0.8869\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3038\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7770\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5114\n",
            "\n",
            "Start of epoch 298\n",
            "Training loss (for one batch) at step 0: 0.9067\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4254\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5335\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4277\n",
            "\n",
            "Start of epoch 299\n",
            "Training loss (for one batch) at step 0: 0.9428\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1916\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9460\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5791\n",
            "\n",
            "Start of epoch 300\n",
            "Training loss (for one batch) at step 0: 1.5521\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3343\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4284\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4454\n",
            "\n",
            "Start of epoch 301\n",
            "Training loss (for one batch) at step 0: 1.0881\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8739\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2516\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4668\n",
            "\n",
            "Start of epoch 302\n",
            "Training loss (for one batch) at step 0: 1.3197\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8836\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7942\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4965\n",
            "\n",
            "Start of epoch 303\n",
            "Training loss (for one batch) at step 0: 0.9757\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1625\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8868\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6096\n",
            "\n",
            "Start of epoch 304\n",
            "Training loss (for one batch) at step 0: 0.8351\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8430\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1951\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4576\n",
            "\n",
            "Start of epoch 305\n",
            "Training loss (for one batch) at step 0: 1.2817\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9413\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8025\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4171\n",
            "\n",
            "Start of epoch 306\n",
            "Training loss (for one batch) at step 0: 0.9760\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0120\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0648\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3313\n",
            "\n",
            "Start of epoch 307\n",
            "Training loss (for one batch) at step 0: 0.9356\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9158\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2342\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6247\n",
            "\n",
            "Start of epoch 308\n",
            "Training loss (for one batch) at step 0: 1.1765\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0384\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0176\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9655\n",
            "\n",
            "Start of epoch 309\n",
            "Training loss (for one batch) at step 0: 1.3599\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4445\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9764\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7479\n",
            "\n",
            "Start of epoch 310\n",
            "Training loss (for one batch) at step 0: 0.6935\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2352\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8139\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5659\n",
            "\n",
            "Start of epoch 311\n",
            "Training loss (for one batch) at step 0: 0.7931\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7716\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8169\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4627\n",
            "\n",
            "Start of epoch 312\n",
            "Training loss (for one batch) at step 0: 1.0619\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9588\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5353\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4383\n",
            "\n",
            "Start of epoch 313\n",
            "Training loss (for one batch) at step 0: 0.7763\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8391\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8154\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8288\n",
            "\n",
            "Start of epoch 314\n",
            "Training loss (for one batch) at step 0: 0.8318\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0453\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4865\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5399\n",
            "\n",
            "Start of epoch 315\n",
            "Training loss (for one batch) at step 0: 1.7077\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0475\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8028\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4093\n",
            "\n",
            "Start of epoch 316\n",
            "Training loss (for one batch) at step 0: 1.2486\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8174\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0168\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6303\n",
            "\n",
            "Start of epoch 317\n",
            "Training loss (for one batch) at step 0: 1.7850\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3494\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9346\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4762\n",
            "\n",
            "Start of epoch 318\n",
            "Training loss (for one batch) at step 0: 1.3170\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9406\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5797\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4925\n",
            "\n",
            "Start of epoch 319\n",
            "Training loss (for one batch) at step 0: 1.6668\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8755\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1448\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.3361\n",
            "\n",
            "Start of epoch 320\n",
            "Training loss (for one batch) at step 0: 1.1571\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9401\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0056\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9565\n",
            "\n",
            "Start of epoch 321\n",
            "Training loss (for one batch) at step 0: 1.1607\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7914\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7775\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5986\n",
            "\n",
            "Start of epoch 322\n",
            "Training loss (for one batch) at step 0: 1.0609\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8385\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1803\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7245\n",
            "\n",
            "Start of epoch 323\n",
            "Training loss (for one batch) at step 0: 1.2662\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7188\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3070\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0826\n",
            "\n",
            "Start of epoch 324\n",
            "Training loss (for one batch) at step 0: 1.4263\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9014\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3187\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4532\n",
            "\n",
            "Start of epoch 325\n",
            "Training loss (for one batch) at step 0: 0.7483\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8368\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8261\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5449\n",
            "\n",
            "Start of epoch 326\n",
            "Training loss (for one batch) at step 0: 0.9004\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4105\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3080\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5508\n",
            "\n",
            "Start of epoch 327\n",
            "Training loss (for one batch) at step 0: 1.0627\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8237\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7883\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4223\n",
            "\n",
            "Start of epoch 328\n",
            "Training loss (for one batch) at step 0: 2.1038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0931\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4034\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5292\n",
            "\n",
            "Start of epoch 329\n",
            "Training loss (for one batch) at step 0: 0.9344\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6138\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4773\n",
            "\n",
            "Start of epoch 330\n",
            "Training loss (for one batch) at step 0: 0.8514\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8680\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2024\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4111\n",
            "\n",
            "Start of epoch 331\n",
            "Training loss (for one batch) at step 0: 0.9573\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7377\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8108\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7607\n",
            "\n",
            "Start of epoch 332\n",
            "Training loss (for one batch) at step 0: 0.8440\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8493\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8292\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4394\n",
            "\n",
            "Start of epoch 333\n",
            "Training loss (for one batch) at step 0: 1.2998\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1795\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1689\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5139\n",
            "\n",
            "Start of epoch 334\n",
            "Training loss (for one batch) at step 0: 0.8038\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8819\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2021\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4184\n",
            "\n",
            "Start of epoch 335\n",
            "Training loss (for one batch) at step 0: 0.9033\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7033\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9771\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4590\n",
            "\n",
            "Start of epoch 336\n",
            "Training loss (for one batch) at step 0: 1.2489\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2171\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9628\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6212\n",
            "\n",
            "Start of epoch 337\n",
            "Training loss (for one batch) at step 0: 1.3117\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9846\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0022\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5848\n",
            "\n",
            "Start of epoch 338\n",
            "Training loss (for one batch) at step 0: 1.0309\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0896\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8321\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7030\n",
            "\n",
            "Start of epoch 339\n",
            "Training loss (for one batch) at step 0: 1.2604\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7714\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8822\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5666\n",
            "\n",
            "Start of epoch 340\n",
            "Training loss (for one batch) at step 0: 1.3555\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9142\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3204\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4780\n",
            "\n",
            "Start of epoch 341\n",
            "Training loss (for one batch) at step 0: 0.9452\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1857\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9106\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7933\n",
            "\n",
            "Start of epoch 342\n",
            "Training loss (for one batch) at step 0: 0.9321\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2814\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0495\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3947\n",
            "\n",
            "Start of epoch 343\n",
            "Training loss (for one batch) at step 0: 0.6947\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8854\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0404\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5915\n",
            "\n",
            "Start of epoch 344\n",
            "Training loss (for one batch) at step 0: 1.3008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9751\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8316\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7120\n",
            "\n",
            "Start of epoch 345\n",
            "Training loss (for one batch) at step 0: 1.5694\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0681\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3726\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4685\n",
            "\n",
            "Start of epoch 346\n",
            "Training loss (for one batch) at step 0: 0.8617\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8994\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2093\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6318\n",
            "\n",
            "Start of epoch 347\n",
            "Training loss (for one batch) at step 0: 0.8020\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9108\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9649\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5892\n",
            "\n",
            "Start of epoch 348\n",
            "Training loss (for one batch) at step 0: 0.8858\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9915\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8473\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3195\n",
            "\n",
            "Start of epoch 349\n",
            "Training loss (for one batch) at step 0: 1.1793\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7725\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0568\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3995\n",
            "\n",
            "Start of epoch 350\n",
            "Training loss (for one batch) at step 0: 0.9260\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9126\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0058\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5271\n",
            "\n",
            "Start of epoch 351\n",
            "Training loss (for one batch) at step 0: 1.0200\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9608\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9838\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4208\n",
            "\n",
            "Start of epoch 352\n",
            "Training loss (for one batch) at step 0: 1.3469\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2370\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7058\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4322\n",
            "\n",
            "Start of epoch 353\n",
            "Training loss (for one batch) at step 0: 1.0286\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1533\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2369\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6393\n",
            "\n",
            "Start of epoch 354\n",
            "Training loss (for one batch) at step 0: 0.9958\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.8746\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9379\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4391\n",
            "\n",
            "Start of epoch 355\n",
            "Training loss (for one batch) at step 0: 0.9443\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8411\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6248\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7986\n",
            "\n",
            "Start of epoch 356\n",
            "Training loss (for one batch) at step 0: 1.0141\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5853\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8191\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5432\n",
            "\n",
            "Start of epoch 357\n",
            "Training loss (for one batch) at step 0: 1.0352\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9511\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9986\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3922\n",
            "\n",
            "Start of epoch 358\n",
            "Training loss (for one batch) at step 0: 1.1105\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8341\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0025\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7122\n",
            "\n",
            "Start of epoch 359\n",
            "Training loss (for one batch) at step 0: 0.7676\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2661\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4243\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5824\n",
            "\n",
            "Start of epoch 360\n",
            "Training loss (for one batch) at step 0: 1.6082\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9650\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4839\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4303\n",
            "\n",
            "Start of epoch 361\n",
            "Training loss (for one batch) at step 0: 0.7553\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7984\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9431\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8527\n",
            "\n",
            "Start of epoch 362\n",
            "Training loss (for one batch) at step 0: 0.7064\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8572\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1419\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8705\n",
            "\n",
            "Start of epoch 363\n",
            "Training loss (for one batch) at step 0: 0.8843\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9499\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9149\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4940\n",
            "\n",
            "Start of epoch 364\n",
            "Training loss (for one batch) at step 0: 0.8524\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9554\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1976\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5822\n",
            "\n",
            "Start of epoch 365\n",
            "Training loss (for one batch) at step 0: 0.7740\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0169\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8673\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5601\n",
            "\n",
            "Start of epoch 366\n",
            "Training loss (for one batch) at step 0: 0.8863\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3009\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3571\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6609\n",
            "\n",
            "Start of epoch 367\n",
            "Training loss (for one batch) at step 0: 0.9122\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8877\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9339\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7216\n",
            "\n",
            "Start of epoch 368\n",
            "Training loss (for one batch) at step 0: 0.9220\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1770\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4758\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3933\n",
            "\n",
            "Start of epoch 369\n",
            "Training loss (for one batch) at step 0: 0.7540\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3350\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8264\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3792\n",
            "\n",
            "Start of epoch 370\n",
            "Training loss (for one batch) at step 0: 0.8109\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7867\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9466\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5186\n",
            "\n",
            "Start of epoch 371\n",
            "Training loss (for one batch) at step 0: 0.7762\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4308\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3699\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4059\n",
            "\n",
            "Start of epoch 372\n",
            "Training loss (for one batch) at step 0: 0.9574\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2055\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0578\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4902\n",
            "\n",
            "Start of epoch 373\n",
            "Training loss (for one batch) at step 0: 0.8424\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0171\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0934\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4815\n",
            "\n",
            "Start of epoch 374\n",
            "Training loss (for one batch) at step 0: 1.3969\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2826\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2289\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5061\n",
            "\n",
            "Start of epoch 375\n",
            "Training loss (for one batch) at step 0: 0.8135\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3979\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9623\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6053\n",
            "\n",
            "Start of epoch 376\n",
            "Training loss (for one batch) at step 0: 0.8222\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8651\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2250\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3685\n",
            "\n",
            "Start of epoch 377\n",
            "Training loss (for one batch) at step 0: 1.0083\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7517\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7785\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7546\n",
            "\n",
            "Start of epoch 378\n",
            "Training loss (for one batch) at step 0: 0.7709\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8823\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8838\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4388\n",
            "\n",
            "Start of epoch 379\n",
            "Training loss (for one batch) at step 0: 0.8053\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4215\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8151\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5033\n",
            "\n",
            "Start of epoch 380\n",
            "Training loss (for one batch) at step 0: 1.6353\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3070\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1028\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6393\n",
            "\n",
            "Start of epoch 381\n",
            "Training loss (for one batch) at step 0: 1.3161\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3731\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9231\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5377\n",
            "\n",
            "Start of epoch 382\n",
            "Training loss (for one batch) at step 0: 1.2985\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0467\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7617\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4713\n",
            "\n",
            "Start of epoch 383\n",
            "Training loss (for one batch) at step 0: 1.2529\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.6950\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8039\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5734\n",
            "\n",
            "Start of epoch 384\n",
            "Training loss (for one batch) at step 0: 0.6853\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0660\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9474\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4225\n",
            "\n",
            "Start of epoch 385\n",
            "Training loss (for one batch) at step 0: 1.4494\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2833\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8030\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5194\n",
            "\n",
            "Start of epoch 386\n",
            "Training loss (for one batch) at step 0: 0.9844\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3091\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0329\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4117\n",
            "\n",
            "Start of epoch 387\n",
            "Training loss (for one batch) at step 0: 0.9929\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7668\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7766\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5210\n",
            "\n",
            "Start of epoch 388\n",
            "Training loss (for one batch) at step 0: 1.0495\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8951\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4366\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3849\n",
            "\n",
            "Start of epoch 389\n",
            "Training loss (for one batch) at step 0: 0.8150\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.7588\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3384\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5475\n",
            "\n",
            "Start of epoch 390\n",
            "Training loss (for one batch) at step 0: 1.2759\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9244\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.7145\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4518\n",
            "\n",
            "Start of epoch 391\n",
            "Training loss (for one batch) at step 0: 0.9998\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2555\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1481\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4176\n",
            "\n",
            "Start of epoch 392\n",
            "Training loss (for one batch) at step 0: 0.8844\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7738\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8989\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7511\n",
            "\n",
            "Start of epoch 393\n",
            "Training loss (for one batch) at step 0: 1.2058\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8388\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3062\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7308\n",
            "\n",
            "Start of epoch 394\n",
            "Training loss (for one batch) at step 0: 1.0332\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9029\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9590\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3862\n",
            "\n",
            "Start of epoch 395\n",
            "Training loss (for one batch) at step 0: 0.8733\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.6966\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9346\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3214\n",
            "\n",
            "Start of epoch 396\n",
            "Training loss (for one batch) at step 0: 0.9577\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9454\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6474\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6160\n",
            "\n",
            "Start of epoch 397\n",
            "Training loss (for one batch) at step 0: 1.1357\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0481\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7554\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7418\n",
            "\n",
            "Start of epoch 398\n",
            "Training loss (for one batch) at step 0: 0.8028\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3258\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7269\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8842\n",
            "\n",
            "Start of epoch 399\n",
            "Training loss (for one batch) at step 0: 1.0320\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8296\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3113\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4689\n",
            "\n",
            "Start of epoch 400\n",
            "Training loss (for one batch) at step 0: 0.7373\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8648\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3413\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9459\n",
            "\n",
            "Start of epoch 401\n",
            "Training loss (for one batch) at step 0: 0.8293\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7944\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9494\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4774\n",
            "\n",
            "Start of epoch 402\n",
            "Training loss (for one batch) at step 0: 1.0936\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3865\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5519\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6798\n",
            "\n",
            "Start of epoch 403\n",
            "Training loss (for one batch) at step 0: 0.8121\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3165\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.5051\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3766\n",
            "\n",
            "Start of epoch 404\n",
            "Training loss (for one batch) at step 0: 0.9659\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0676\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8527\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4053\n",
            "\n",
            "Start of epoch 405\n",
            "Training loss (for one batch) at step 0: 0.9376\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8959\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9260\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6130\n",
            "\n",
            "Start of epoch 406\n",
            "Training loss (for one batch) at step 0: 0.8256\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1587\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0396\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5510\n",
            "\n",
            "Start of epoch 407\n",
            "Training loss (for one batch) at step 0: 1.1979\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9483\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0493\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4241\n",
            "\n",
            "Start of epoch 408\n",
            "Training loss (for one batch) at step 0: 1.5755\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8110\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0349\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6149\n",
            "\n",
            "Start of epoch 409\n",
            "Training loss (for one batch) at step 0: 1.0673\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2783\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8960\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4591\n",
            "\n",
            "Start of epoch 410\n",
            "Training loss (for one batch) at step 0: 0.8748\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0771\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1486\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4393\n",
            "\n",
            "Start of epoch 411\n",
            "Training loss (for one batch) at step 0: 1.3559\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8787\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0019\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4837\n",
            "\n",
            "Start of epoch 412\n",
            "Training loss (for one batch) at step 0: 1.3765\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1407\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8405\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4633\n",
            "\n",
            "Start of epoch 413\n",
            "Training loss (for one batch) at step 0: 0.8551\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9047\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8440\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4905\n",
            "\n",
            "Start of epoch 414\n",
            "Training loss (for one batch) at step 0: 0.8352\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1007\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8954\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5651\n",
            "\n",
            "Start of epoch 415\n",
            "Training loss (for one batch) at step 0: 1.0767\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7271\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1916\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5490\n",
            "\n",
            "Start of epoch 416\n",
            "Training loss (for one batch) at step 0: 1.1565\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0628\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9941\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6492\n",
            "\n",
            "Start of epoch 417\n",
            "Training loss (for one batch) at step 0: 0.8631\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1664\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3701\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6603\n",
            "\n",
            "Start of epoch 418\n",
            "Training loss (for one batch) at step 0: 0.7962\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9731\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7993\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5402\n",
            "\n",
            "Start of epoch 419\n",
            "Training loss (for one batch) at step 0: 1.2797\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8221\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8968\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5581\n",
            "\n",
            "Start of epoch 420\n",
            "Training loss (for one batch) at step 0: 1.3723\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0015\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0712\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4597\n",
            "\n",
            "Start of epoch 421\n",
            "Training loss (for one batch) at step 0: 0.8681\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8065\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0265\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5028\n",
            "\n",
            "Start of epoch 422\n",
            "Training loss (for one batch) at step 0: 0.7599\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9591\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1039\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5562\n",
            "\n",
            "Start of epoch 423\n",
            "Training loss (for one batch) at step 0: 0.9804\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8483\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0400\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4336\n",
            "\n",
            "Start of epoch 424\n",
            "Training loss (for one batch) at step 0: 0.9186\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2988\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7747\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6552\n",
            "\n",
            "Start of epoch 425\n",
            "Training loss (for one batch) at step 0: 0.8496\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0683\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6814\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3538\n",
            "\n",
            "Start of epoch 426\n",
            "Training loss (for one batch) at step 0: 1.2274\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9068\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7022\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4882\n",
            "\n",
            "Start of epoch 427\n",
            "Training loss (for one batch) at step 0: 0.7448\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8203\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9820\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6805\n",
            "\n",
            "Start of epoch 428\n",
            "Training loss (for one batch) at step 0: 0.7413\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9190\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7201\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3639\n",
            "\n",
            "Start of epoch 429\n",
            "Training loss (for one batch) at step 0: 0.9705\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0095\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.6593\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5751\n",
            "\n",
            "Start of epoch 430\n",
            "Training loss (for one batch) at step 0: 1.4040\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9307\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8987\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5998\n",
            "\n",
            "Start of epoch 431\n",
            "Training loss (for one batch) at step 0: 1.1617\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9247\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9539\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7305\n",
            "\n",
            "Start of epoch 432\n",
            "Training loss (for one batch) at step 0: 1.3169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8782\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7750\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4744\n",
            "\n",
            "Start of epoch 433\n",
            "Training loss (for one batch) at step 0: 0.8325\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2686\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8394\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5023\n",
            "\n",
            "Start of epoch 434\n",
            "Training loss (for one batch) at step 0: 1.3786\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7216\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3066\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5023\n",
            "\n",
            "Start of epoch 435\n",
            "Training loss (for one batch) at step 0: 1.0829\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0270\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8815\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4634\n",
            "\n",
            "Start of epoch 436\n",
            "Training loss (for one batch) at step 0: 1.6227\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9300\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8012\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5240\n",
            "\n",
            "Start of epoch 437\n",
            "Training loss (for one batch) at step 0: 1.0252\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9961\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2345\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3943\n",
            "\n",
            "Start of epoch 438\n",
            "Training loss (for one batch) at step 0: 0.7571\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4645\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2270\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6196\n",
            "\n",
            "Start of epoch 439\n",
            "Training loss (for one batch) at step 0: 0.9756\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8640\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1907\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6472\n",
            "\n",
            "Start of epoch 440\n",
            "Training loss (for one batch) at step 0: 1.6593\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 2.0710\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4356\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4209\n",
            "\n",
            "Start of epoch 441\n",
            "Training loss (for one batch) at step 0: 0.8664\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8881\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5485\n",
            "\n",
            "Start of epoch 442\n",
            "Training loss (for one batch) at step 0: 0.9456\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0884\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7464\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7749\n",
            "\n",
            "Start of epoch 443\n",
            "Training loss (for one batch) at step 0: 1.1163\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8467\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9668\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4706\n",
            "\n",
            "Start of epoch 444\n",
            "Training loss (for one batch) at step 0: 0.9600\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0056\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3602\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4967\n",
            "\n",
            "Start of epoch 445\n",
            "Training loss (for one batch) at step 0: 0.9594\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7805\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2091\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7793\n",
            "\n",
            "Start of epoch 446\n",
            "Training loss (for one batch) at step 0: 0.7919\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8236\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9283\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5700\n",
            "\n",
            "Start of epoch 447\n",
            "Training loss (for one batch) at step 0: 1.2169\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9487\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8067\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5539\n",
            "\n",
            "Start of epoch 448\n",
            "Training loss (for one batch) at step 0: 0.9871\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1798\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4135\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7990\n",
            "\n",
            "Start of epoch 449\n",
            "Training loss (for one batch) at step 0: 1.1013\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7693\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3092\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5223\n",
            "\n",
            "Start of epoch 450\n",
            "Training loss (for one batch) at step 0: 0.7017\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1798\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7513\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4670\n",
            "\n",
            "Start of epoch 451\n",
            "Training loss (for one batch) at step 0: 1.6727\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3647\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1139\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5472\n",
            "\n",
            "Start of epoch 452\n",
            "Training loss (for one batch) at step 0: 0.8779\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8792\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0518\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5206\n",
            "\n",
            "Start of epoch 453\n",
            "Training loss (for one batch) at step 0: 1.1008\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8253\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8709\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5249\n",
            "\n",
            "Start of epoch 454\n",
            "Training loss (for one batch) at step 0: 1.3632\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9076\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7194\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4412\n",
            "\n",
            "Start of epoch 455\n",
            "Training loss (for one batch) at step 0: 0.7931\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8169\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8280\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4599\n",
            "\n",
            "Start of epoch 456\n",
            "Training loss (for one batch) at step 0: 1.1762\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2058\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8063\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4251\n",
            "\n",
            "Start of epoch 457\n",
            "Training loss (for one batch) at step 0: 1.1838\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0163\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4447\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6680\n",
            "\n",
            "Start of epoch 458\n",
            "Training loss (for one batch) at step 0: 0.8646\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0083\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1467\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4090\n",
            "\n",
            "Start of epoch 459\n",
            "Training loss (for one batch) at step 0: 1.1791\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8845\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8083\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7729\n",
            "\n",
            "Start of epoch 460\n",
            "Training loss (for one batch) at step 0: 0.8874\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9333\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7299\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7430\n",
            "\n",
            "Start of epoch 461\n",
            "Training loss (for one batch) at step 0: 1.0278\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2483\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8600\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5126\n",
            "\n",
            "Start of epoch 462\n",
            "Training loss (for one batch) at step 0: 1.2094\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5749\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9662\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4319\n",
            "\n",
            "Start of epoch 463\n",
            "Training loss (for one batch) at step 0: 0.7435\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8140\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1185\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4025\n",
            "\n",
            "Start of epoch 464\n",
            "Training loss (for one batch) at step 0: 1.4946\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8149\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2015\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8090\n",
            "\n",
            "Start of epoch 465\n",
            "Training loss (for one batch) at step 0: 0.9523\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8362\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2080\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5269\n",
            "\n",
            "Start of epoch 466\n",
            "Training loss (for one batch) at step 0: 1.0166\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7889\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8759\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7356\n",
            "\n",
            "Start of epoch 467\n",
            "Training loss (for one batch) at step 0: 0.7324\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0262\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.4866\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7496\n",
            "\n",
            "Start of epoch 468\n",
            "Training loss (for one batch) at step 0: 1.0247\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7137\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2722\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 1.0872\n",
            "\n",
            "Start of epoch 469\n",
            "Training loss (for one batch) at step 0: 0.8885\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8173\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7386\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4750\n",
            "\n",
            "Start of epoch 470\n",
            "Training loss (for one batch) at step 0: 1.0104\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7702\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1307\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5769\n",
            "\n",
            "Start of epoch 471\n",
            "Training loss (for one batch) at step 0: 0.6914\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7885\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7867\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5110\n",
            "\n",
            "Start of epoch 472\n",
            "Training loss (for one batch) at step 0: 0.9141\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7863\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9278\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5305\n",
            "\n",
            "Start of epoch 473\n",
            "Training loss (for one batch) at step 0: 1.4471\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7842\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9948\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5730\n",
            "\n",
            "Start of epoch 474\n",
            "Training loss (for one batch) at step 0: 1.1776\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2164\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9575\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3267\n",
            "\n",
            "Start of epoch 475\n",
            "Training loss (for one batch) at step 0: 0.8928\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9712\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1604\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.7247\n",
            "\n",
            "Start of epoch 476\n",
            "Training loss (for one batch) at step 0: 0.8872\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1349\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9932\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4781\n",
            "\n",
            "Start of epoch 477\n",
            "Training loss (for one batch) at step 0: 0.8591\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7325\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1852\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.9865\n",
            "\n",
            "Start of epoch 478\n",
            "Training loss (for one batch) at step 0: 1.4980\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8211\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1686\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4542\n",
            "\n",
            "Start of epoch 479\n",
            "Training loss (for one batch) at step 0: 1.1310\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7257\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6682\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5316\n",
            "\n",
            "Start of epoch 480\n",
            "Training loss (for one batch) at step 0: 0.8583\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0809\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0329\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4696\n",
            "\n",
            "Start of epoch 481\n",
            "Training loss (for one batch) at step 0: 1.0552\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9542\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3063\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4584\n",
            "\n",
            "Start of epoch 482\n",
            "Training loss (for one batch) at step 0: 0.7814\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0857\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8331\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3449\n",
            "\n",
            "Start of epoch 483\n",
            "Training loss (for one batch) at step 0: 1.1423\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8394\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.3498\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4840\n",
            "\n",
            "Start of epoch 484\n",
            "Training loss (for one batch) at step 0: 0.9551\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4149\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6639\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5223\n",
            "\n",
            "Start of epoch 485\n",
            "Training loss (for one batch) at step 0: 1.0199\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7381\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8554\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6488\n",
            "\n",
            "Start of epoch 486\n",
            "Training loss (for one batch) at step 0: 0.8589\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.4978\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.1224\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5936\n",
            "\n",
            "Start of epoch 487\n",
            "Training loss (for one batch) at step 0: 0.7976\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3701\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7310\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.6519\n",
            "\n",
            "Start of epoch 488\n",
            "Training loss (for one batch) at step 0: 1.0164\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.5527\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8023\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4310\n",
            "\n",
            "Start of epoch 489\n",
            "Training loss (for one batch) at step 0: 0.6715\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.7429\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7559\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4767\n",
            "\n",
            "Start of epoch 490\n",
            "Training loss (for one batch) at step 0: 1.0626\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0284\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9894\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5293\n",
            "\n",
            "Start of epoch 491\n",
            "Training loss (for one batch) at step 0: 0.7467\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.3265\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.7086\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.8584\n",
            "\n",
            "Start of epoch 492\n",
            "Training loss (for one batch) at step 0: 0.7196\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8378\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8649\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3725\n",
            "\n",
            "Start of epoch 493\n",
            "Training loss (for one batch) at step 0: 0.9109\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.9283\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.0492\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5298\n",
            "\n",
            "Start of epoch 494\n",
            "Training loss (for one batch) at step 0: 0.8318\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.1598\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.8873\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5636\n",
            "\n",
            "Start of epoch 495\n",
            "Training loss (for one batch) at step 0: 1.1732\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8670\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.9675\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4712\n",
            "\n",
            "Start of epoch 496\n",
            "Training loss (for one batch) at step 0: 0.9016\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2386\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.9913\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4858\n",
            "\n",
            "Start of epoch 497\n",
            "Training loss (for one batch) at step 0: 0.8975\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.0459\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 0.6495\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.4687\n",
            "\n",
            "Start of epoch 498\n",
            "Training loss (for one batch) at step 0: 1.1626\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 1.2141\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.2634\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.5047\n",
            "\n",
            "Start of epoch 499\n",
            "Training loss (for one batch) at step 0: 0.7638\n",
            "seen so far: 100 samples\n",
            "Training loss (for one batch) at step 10: 0.8932\n",
            "seen so far: 1100 samples\n",
            "Training loss (for one batch) at step 20: 1.8958\n",
            "seen so far: 2100 samples\n",
            "validation MSE: 0.3661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UkV66dgmlyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "f1, f2, f3 = prediction.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(f1, f2, f3, 'o')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZKPk2l1krmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape = (3,))\n",
        "x1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(10, activation='relu')(x1)\n",
        "outputs = layers.Dense(3, activation='linear')(x2)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "epochs = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqPagbGMkovO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ec41d46-ac34-45d2-ca77-cd20b6cfce6a"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # open a GradientTape to record the operations run during the feed forward\n",
        "    # enables aoto-partial-Differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "      # run the feed foward process of the layers.\n",
        "      # the operations that the layers apply to its inpurts and going to bee recorded on the GradientTape\n",
        "      linear = model(x_batch_train, training = True)\n",
        "      # print(linear)\n",
        "      # compute the loss value for this minibatch\n",
        "      # mse = Keras_loss_function(y_batch_train, linear)\n",
        "      mse = tf.keras.losses.MSE(y_batch_train, linear)\n",
        "      reconstruction_error = tf.reduce_mean(mse)\n",
        "      \n",
        "      print(mse)\n",
        "      # mse = keras.losses.mean_squared_error(y_batch_train, linear) # mse gives 3 number\n",
        "    # print(mse)\n",
        "    # use the gradient tap to automatically retrieve the gradients of the \n",
        "    # trainable variables with respect to the loss\n",
        "    grads = tape.gradient(mse, model.trainable_weights)\n",
        "    # print(grads)\n",
        "    # weight + bias for every layer\n",
        "    # run one step of gradient dscent by updating the value of the variables to minize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # print(model.trainable_weights,'\\n')\n",
        "    # print log information every 10 batches.\n",
        "    if step % 10 == 0:\n",
        "      print(\n",
        "          \"Training loss (for one batch) at step %d: %.4f\"\n",
        "          %(step, float(reconstruction_error))\n",
        "      )\n",
        "      print(\"seen so far: %s samples\" % ((step +1) * 100))\n",
        "\n",
        "\n",
        "  # Run a validation loop at the end of each epoch\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_linear = model(x_batch_val, training = False)\n",
        "    val_mse = tf.keras.losses.MSE(y_batch_val, val_linear)\n",
        "    val_reconstruction_error = tf.reduce_mean(val_mse)\n",
        "  # print MSE for validation set\n",
        "  print(\"validation MSE: %.4f\" % (float(val_reconstruction_error)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "tf.Tensor(\n",
            "[0.3842297  0.42797565 0.3472961  0.27937302 0.33427966 0.16493267\n",
            " 0.4086562  0.33521208 0.47289905 1.0540316  0.6916525  0.3622521\n",
            " 0.39382276 0.5528478  0.55679005 0.43935475 0.37818182 0.3618947\n",
            " 0.521801   0.7617163  0.35995445 0.4703331  0.40040436 0.5288081\n",
            " 0.7809743  0.37247488 0.5264825  0.4781715  0.83909756 0.34608588\n",
            " 0.3736016  0.25681335 0.2610033  0.45814824 0.41596866 0.17603369\n",
            " 0.37793037 0.3476676  0.26313162 0.7729811  0.476038   0.6464086\n",
            " 0.41478404 0.8852212  0.48054478 0.6424247  0.18614174 0.36385384\n",
            " 0.41569534 0.16256386 0.61835307 0.3670615  0.56214184 0.4034245\n",
            " 0.47499433 0.4112301  0.54172355 0.1362366  0.420256   0.07681898\n",
            " 0.7463608  1.0133843  0.3815819  0.28662837 0.60999894 0.57684857\n",
            " 0.31372806 0.5327858  0.281065   0.45715943 0.24788512 0.39179942\n",
            " 0.33333334 0.48476732 0.35954127 0.3360711  0.3392143  0.5164711\n",
            " 0.20567058 0.2057393  0.42267242 0.23170583 0.33772638 0.22380936\n",
            " 0.38552478 0.40180376 0.21150778 0.35532656 0.4767847  0.46774587\n",
            " 0.40611672 0.5190676  0.38034818 1.6588454  0.35808685 0.5656989\n",
            " 0.32684463 0.347855   0.5492076  0.5368646 ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 0: 0.4450\n",
            "seen so far: 100 samples\n",
            "tf.Tensor(\n",
            "[0.33860537 0.41040483 0.48373207 0.33541802 0.47145763 0.3492386\n",
            " 0.46276578 0.3469976  0.37813625 0.38515183 0.42063832 0.45781818\n",
            " 0.5603331  0.09026703 0.28138673 0.36046323 0.27292114 0.51712584\n",
            " 0.4488442  0.26163995 0.31794998 0.46203247 0.14701332 0.3603612\n",
            " 0.33832398 0.15801458 0.5733414  0.348163   0.4345752  0.33633122\n",
            " 0.5036835  0.36150765 0.20709808 0.8765311  0.29300377 0.33649907\n",
            " 0.32682645 0.8774343  0.15217887 0.45081353 0.37289107 0.4008452\n",
            " 0.3402873  0.21822572 0.26459107 0.16481005 0.42958677 0.40201747\n",
            " 0.298101   0.36909544 0.6067802  0.33647582 0.38005757 1.0637578\n",
            " 0.42574596 0.86553836 0.34986925 0.33925804 0.21397112 0.76335615\n",
            " 0.4631168  0.48089024 0.45570305 0.2862741  0.4776434  0.21954946\n",
            " 0.44486538 0.40049195 0.4243425  0.48926648 0.48389956 0.37723914\n",
            " 0.35537207 0.3527999  0.4002842  0.4848771  0.7268116  0.38477603\n",
            " 0.210315   0.5238376  0.15521008 0.36547065 0.2937604  0.39938772\n",
            " 0.15684216 0.616399   0.3907454  0.23303841 0.221061   0.38182923\n",
            " 0.34628275 1.0876843  0.50028425 0.20555873 0.66256684 0.38983345\n",
            " 0.47678408 0.40423474 0.24957202 0.3520956 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.29152432 0.3233721  0.13705064 0.7551074  0.34089518 0.38406906\n",
            " 0.32917568 0.58325094 0.6762213  0.15430054 0.3481998  0.2750775\n",
            " 0.3565358  0.6689251  0.1349848  0.31611362 0.36076617 0.21965432\n",
            " 0.4221066  0.3578694  0.52965456 0.44622853 0.36026847 0.1767257\n",
            " 0.33140525 0.32598022 0.6832847  0.4423437  0.44871172 0.33427373\n",
            " 0.50595737 0.34445098 0.31876943 0.22113378 0.41460505 0.427222\n",
            " 0.66325665 0.25512716 0.42291987 0.2891665  0.5147335  0.354877\n",
            " 0.48030153 0.3628141  0.3559177  0.43480864 0.32507205 0.8571439\n",
            " 0.50447065 0.34010923 0.35244584 0.31774402 0.36836326 0.3328918\n",
            " 0.37989524 0.49973473 0.21609624 0.44154337 0.29927945 0.47681364\n",
            " 0.36677876 0.56768316 0.35133287 0.531841   0.56575507 0.33775818\n",
            " 0.4209367  0.21335632 0.36291084 0.3265238  0.43359384 0.33860376\n",
            " 0.43042126 0.34266153 0.08904785 0.54525936 0.47716907 0.19346558\n",
            " 0.5428558  0.42337263 0.5542352  0.36558667 0.49470648 0.3424088\n",
            " 0.33048424 0.40352002 0.40909854 0.54604733 0.28603986 0.4485502\n",
            " 0.33456084 0.34096074 0.38397405 0.43466106 1.5555377  0.40743902\n",
            " 1.4628962  0.3162957  0.45734012 0.3377998 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.28318462 0.401673   0.13908519 0.31875578 0.3217967  0.18928666\n",
            " 0.33611917 0.34430727 0.34520265 0.3192033  0.38713858 0.33224156\n",
            " 0.19869792 0.32757887 0.2679485  0.33568704 0.12757322 0.3467072\n",
            " 0.28354523 0.49371478 0.39409292 0.3078964  0.38514245 0.23425718\n",
            " 0.33313212 0.42288363 0.07868618 0.5823947  0.33471116 0.5458235\n",
            " 0.47892368 0.322184   0.21033865 0.3391601  0.560061   0.37789145\n",
            " 0.42036656 0.3891503  0.3390735  0.5285423  0.38732564 0.19867122\n",
            " 0.3463936  0.44539198 0.33046493 0.3250334  0.37361374 0.42149124\n",
            " 0.36662826 0.43403038 0.3923709  0.34364018 0.39686707 0.3436644\n",
            " 0.2908488  0.56065404 0.18323909 0.4539996  0.40700746 0.25386265\n",
            " 0.547029   0.36603415 0.34087506 0.37716976 0.3509071  0.19781764\n",
            " 0.38281384 0.10220256 0.9254668  0.20705914 0.38527408 0.3434782\n",
            " 0.3754398  0.40569648 0.655157   0.33498192 0.54593617 0.3424355\n",
            " 0.43429923 0.48092857 0.38116562 0.29646543 0.3918167  0.18564002\n",
            " 0.49099353 0.32984874 0.49954614 0.3212475  0.3652924  0.38029277\n",
            " 0.47627833 0.5438964  0.94633454 0.34116924 0.43481383 0.594202\n",
            " 0.5726839  0.34449705 0.42033967 0.34761918], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.36479703 0.7680208  0.39052573 0.5850437  0.24678463 0.2805087\n",
            " 0.6381304  0.7198604  0.3373996  0.3713905  0.27557495 0.34028196\n",
            " 0.34641376 0.4339594  0.24646421 0.34682956 0.3497517  0.3382124\n",
            " 0.41500232 0.21743159 0.34860948 0.30904448 0.28564999 0.3341032\n",
            " 0.3449494  0.2083264  0.5284645  0.34286967 0.33418044 0.32979655\n",
            " 0.24534105 0.6422638  0.33207396 0.19303834 0.8241555  0.47107843\n",
            " 0.51415354 0.21926261 0.3522004  0.14978655 0.34031752 0.8636196\n",
            " 0.3539501  0.7644763  0.44225731 0.37046626 0.40592337 0.3369093\n",
            " 0.24946189 0.31013772 0.20059876 0.2935204  0.16897418 0.5394059\n",
            " 0.35958812 0.4462502  0.21144663 0.44009137 0.3840337  0.41190633\n",
            " 0.38709506 0.39659503 0.38236508 0.48395625 0.2960538  0.47936383\n",
            " 0.32475045 0.5442054  0.29057527 0.37892938 0.42641315 0.58221\n",
            " 0.3497555  0.4765422  0.4534469  0.354197   0.36141133 0.22210272\n",
            " 0.5372376  0.1992395  0.12595765 0.34308004 0.36940417 0.3433002\n",
            " 0.28238046 0.2847316  0.34197536 0.12924929 0.34051016 0.39327565\n",
            " 0.4690628  0.33421257 0.4025921  0.56337184 0.41138878 0.35470316\n",
            " 0.26187548 0.35324803 0.37458658 0.40019867], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.33299348 0.34766078 0.35073122 1.039867   0.34316134 0.14037597\n",
            " 0.4022998  0.4162738  0.28016987 0.3580291  0.10929954 0.3251094\n",
            " 0.3419079  0.39452192 0.41266575 0.33764803 0.3510331  0.3510716\n",
            " 0.34500894 0.26614782 0.3849074  0.27073678 0.4129386  0.35264564\n",
            " 0.35324168 0.3450705  0.2948717  0.35821438 0.36778986 0.22274512\n",
            " 0.32088348 0.34103826 0.3102553  0.329868   0.26212865 0.16584499\n",
            " 0.30236587 0.34565952 0.41835546 0.4439373  0.3354818  0.3235592\n",
            " 0.38401    0.35623923 0.32063818 0.34819278 0.3377924  0.22799258\n",
            " 0.83690447 0.34957287 0.36310136 0.36216065 0.33535758 0.31379086\n",
            " 0.20011495 0.375716   0.07764735 0.13229452 0.34084842 0.34211683\n",
            " 0.3967187  0.3099968  0.16968083 0.24635988 0.4253602  0.23808591\n",
            " 0.33114538 0.558217   0.4247626  0.14247154 0.34569195 0.35400596\n",
            " 0.37147436 0.3523489  0.43949652 0.47408772 0.37984434 0.08132473\n",
            " 0.2179149  0.2977538  0.20160933 0.32961443 0.10687136 0.17839813\n",
            " 0.38436186 0.87266535 0.38569817 0.2961361  0.3579836  0.3785585\n",
            " 0.36578247 0.42422572 0.36369693 0.64489174 0.36969137 0.29083893\n",
            " 0.39458176 0.28632703 0.0956659  0.3756474 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.35162473 0.27470377 0.38727543 0.40015456 0.27356613 0.3442669\n",
            " 0.3322688  0.33290204 0.43995777 0.35583213 0.35837865 0.21290426\n",
            " 0.28384316 0.25786963 0.41519126 0.32032108 0.37899622 0.19962953\n",
            " 0.3211309  0.37739214 0.31819186 0.35486507 0.30338216 0.34549618\n",
            " 0.27450132 0.40810642 0.32473382 0.34061813 0.4753978  0.3415061\n",
            " 0.34177694 0.3580489  0.35367072 0.3349214  0.15647776 0.2903227\n",
            " 0.34560072 0.40749708 0.24014932 0.30344942 0.24531214 0.32055223\n",
            " 0.36529675 0.4013507  0.34342647 0.32537994 0.35906267 0.5894883\n",
            " 0.316673   0.4111867  0.13513616 0.29687557 0.14405863 0.36149868\n",
            " 0.3537192  0.28611448 0.35897052 0.55801976 0.34373593 0.3356603\n",
            " 0.36259887 0.30754527 0.3795717  0.4992133  0.36305287 0.5335761\n",
            " 0.31395096 0.3207828  0.3397441  0.34015557 0.41417518 0.3523886\n",
            " 0.21524926 0.35270187 0.30758214 0.5694886  0.21051015 0.39728966\n",
            " 0.30510342 0.30744165 0.3371625  0.36203137 0.48383275 0.35023427\n",
            " 0.1973873  0.340323   0.34272763 0.6334752  0.3369259  0.37891793\n",
            " 0.346248   0.31795862 0.27040634 0.75140125 0.19667935 0.4908402\n",
            " 0.50517535 0.35266724 0.37160775 0.32099232], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.36673483 0.28527582 0.2980406  0.41611052 0.3696483  0.24737345\n",
            " 0.28314316 0.42966557 0.24397688 0.36961198 0.36368242 0.35794184\n",
            " 0.35775414 0.32390913 0.59346354 0.38881323 0.37482837 0.22639436\n",
            " 0.3400289  0.481513   0.3601997  0.4132885  0.37805584 0.30555838\n",
            " 0.34687057 0.34786272 0.31006023 0.3302621  0.26223573 0.34281144\n",
            " 0.3675406  0.57567257 0.2933918  0.36863947 0.3401974  0.18469654\n",
            " 0.4367089  0.2695022  0.23089288 0.36361805 0.28551435 0.2783998\n",
            " 0.15753521 0.35827184 0.34504136 0.3414662  0.26248544 0.16004036\n",
            " 0.10380874 0.30834666 0.35203317 0.31872293 0.31551185 0.5070054\n",
            " 0.34778515 0.28517127 0.28655684 0.2593713  0.6627396  0.5029884\n",
            " 0.3296826  0.5026198  0.19734687 0.36074212 0.11928969 0.3604797\n",
            " 0.16915856 0.32166    0.36339197 0.49195564 0.34708384 0.34248564\n",
            " 0.33956742 0.3865156  0.33837947 0.28677064 0.29182604 0.18323736\n",
            " 0.32959148 0.30778876 0.3348308  0.20408969 0.32248372 0.35309386\n",
            " 0.36090687 0.38818774 0.3051309  0.20301133 0.33459404 0.33626512\n",
            " 0.3075248  0.29782453 0.30338505 0.3005467  0.28678006 0.21022652\n",
            " 0.40975496 0.32075024 0.45192674 0.338858  ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.2799919  0.3433522  0.35423675 0.33659473 0.06014466 0.3535144\n",
            " 0.30666628 0.36865214 0.35031167 0.36830446 0.17436342 0.31843537\n",
            " 0.37068248 0.3486707  0.31602353 0.35064185 0.33956465 0.3058185\n",
            " 0.09961583 0.5102458  0.31886068 0.31123972 0.36899176 0.3477162\n",
            " 0.32849583 0.15567212 0.3220156  0.29143426 0.27703688 0.36721215\n",
            " 0.2766848  0.28136137 0.36223975 0.3852546  0.27071854 0.2675875\n",
            " 0.37405622 0.23712474 0.3367881  0.1215502  0.36814    0.43975404\n",
            " 0.26930127 0.33729294 0.353676   0.3244782  0.27934995 0.26430824\n",
            " 0.3770313  0.31177324 0.28009066 0.3890896  0.2667968  0.3128083\n",
            " 0.34462574 0.15333669 0.3584713  0.33556136 0.29186454 0.4366397\n",
            " 0.31274807 0.3918287  0.2599719  0.30514428 0.4142312  0.35322425\n",
            " 0.278739   0.353825   0.28266588 0.34256935 0.25851253 0.24076845\n",
            " 0.25094557 0.36666045 0.31899348 0.26694208 0.21589291 0.07612073\n",
            " 0.32031587 0.2331603  0.38829234 0.35331905 0.26742348 0.37241563\n",
            " 0.2971698  0.3498559  0.29411155 0.3342098  0.4159862  0.35912275\n",
            " 0.40698016 0.36188254 0.36080444 0.35791937 0.29378107 0.41792372\n",
            " 0.3503283  0.30921972 0.36850068 0.51692015], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.22131492 0.38821253 0.3604989  0.3175108  0.36168262 0.30705604\n",
            " 0.337715   0.31452498 0.36701488 0.27662802 0.28024146 0.3716818\n",
            " 0.3603491  0.351657   0.2607252  0.3384039  0.31399682 0.2727524\n",
            " 0.135605   0.27479157 0.32083076 0.24283426 0.32635322 0.35371137\n",
            " 0.37152684 0.3847586  0.33451006 0.3078226  0.30451432 0.3497498\n",
            " 0.32923833 0.32635024 0.45178172 0.22755402 0.30769244 0.26453483\n",
            " 0.37273097 0.37662888 0.28134575 0.24684606 0.29841724 0.3603066\n",
            " 0.3108292  0.33653113 0.25915816 0.35195997 0.27680948 0.2973177\n",
            " 0.3356905  0.3452311  0.31607082 0.3383911  0.28466377 0.32576528\n",
            " 0.3564837  0.30770636 0.280396   0.2800002  0.39334455 0.3639116\n",
            " 0.3635541  0.2894334  0.26365522 0.33390418 0.3294582  0.42845544\n",
            " 0.28371307 0.26356885 0.3842573  0.32132652 0.18483482 0.33661696\n",
            " 0.3396337  0.3284659  0.33010694 0.2713847  0.495996   0.29437253\n",
            " 0.26849273 0.27792928 0.37134132 0.26993755 0.1294331  0.3737929\n",
            " 0.37940657 0.29070652 0.34341195 0.28146377 0.36366597 0.16506658\n",
            " 0.2208661  0.3207397  0.27672452 0.3710829  0.3724749  0.30772695\n",
            " 0.40066925 0.29312262 0.3560617  0.29665896], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.23923405 0.43167862 0.26871106 0.16908579 0.3568894  0.36826363\n",
            " 0.28987956 0.36387345 0.33328503 0.31078085 0.35023192 0.28920376\n",
            " 0.21538341 0.27764285 0.38147104 0.15858702 0.3604919  0.30237737\n",
            " 0.29941747 0.323021   0.28151244 0.24814898 0.19540851 0.33324423\n",
            " 0.07399183 0.28819457 0.20476724 0.25104877 0.28109506 0.2967434\n",
            " 0.31538066 0.34878823 0.44300103 0.297784   0.3346604  0.3036486\n",
            " 0.28030252 0.42320383 0.14463076 0.25609845 0.3689867  0.24860741\n",
            " 0.2778025  0.29478118 0.30015048 0.3639984  0.27358702 0.43162528\n",
            " 0.3231435  0.3852464  0.02803682 0.23246926 0.19514853 0.05272064\n",
            " 0.29090592 0.2573625  0.320873   0.38342956 0.25669202 0.33366886\n",
            " 0.37135103 0.28030428 0.32032314 0.1360559  0.21752109 0.29154608\n",
            " 0.30862698 0.2868385  0.27823263 0.29988745 0.30750522 0.24726824\n",
            " 0.20788603 0.197807   0.27760383 0.29728118 0.48043606 0.30844972\n",
            " 0.2876789  0.30425695 0.14233257 0.19092686 0.27597764 0.2751819\n",
            " 0.10031665 0.31047568 0.34494635 0.27960065 0.27863464 0.2680494\n",
            " 0.33747876 0.32948574 0.24073939 0.27468583 0.38045263 0.4051981\n",
            " 0.29064503 0.20718129 0.3444945  0.3051335 ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 10: 0.2860\n",
            "seen so far: 1100 samples\n",
            "tf.Tensor(\n",
            "[0.28896827 0.18377559 0.27005562 0.29697183 0.356186   0.22489923\n",
            " 0.331864   0.37530577 0.29366317 0.22447167 0.58997357 0.29026875\n",
            " 0.24698885 0.2923431  0.3112485  0.33719158 0.34648314 0.3328317\n",
            " 0.24244069 0.2562863  0.3367093  0.34576336 0.21974094 0.33198932\n",
            " 0.36843607 0.31976137 0.30052015 0.28279078 0.11944842 0.25076565\n",
            " 0.33524594 0.33446303 0.342011   0.33336964 0.24224758 0.25098586\n",
            " 0.3461883  0.23080963 0.3085197  0.37730107 0.28225562 0.38059244\n",
            " 0.29004732 0.21401608 0.34589925 0.33092538 0.2566547  0.23428945\n",
            " 0.32929716 0.14023946 0.26974997 0.16437632 0.2713466  0.47651914\n",
            " 0.33686122 0.3843179  0.2653737  0.3220872  0.22170214 0.273645\n",
            " 0.24297774 0.17439348 0.25838488 0.35460064 0.32996485 0.30259857\n",
            " 0.23562343 0.2729177  0.2614543  0.3158305  0.31948084 0.4260838\n",
            " 0.31771302 0.34631887 0.34803352 0.36586782 0.39252463 0.28118125\n",
            " 0.32977614 0.358554   0.37840405 0.3722254  0.24857302 0.25415245\n",
            " 0.32314548 0.2927304  0.19427669 0.31295338 0.361959   0.28466025\n",
            " 0.36825135 0.15722175 0.243756   0.28970245 0.5106651  0.39126644\n",
            " 0.26358736 0.21391742 0.26149392 0.14834173], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.23696004 0.32690254 0.25778756 0.34244177 0.28681016 0.3094726\n",
            " 0.28743148 0.27890146 0.29454497 0.23455207 0.30408713 0.31401375\n",
            " 0.39477614 0.23979662 0.28936514 0.34231713 0.2942169  0.55700886\n",
            " 0.33127224 0.2650075  0.32399806 0.18708141 0.22000362 0.41248903\n",
            " 0.24901567 0.27384853 0.15352559 0.2569198  0.2826409  0.27231586\n",
            " 0.24673323 0.3298781  0.30710098 0.30125004 0.29648018 0.25528288\n",
            " 0.19141363 0.06896492 0.26776418 0.2708753  0.31614026 0.26379368\n",
            " 0.26872256 0.29396182 0.3335271  0.32977697 0.2137994  0.2478928\n",
            " 0.30179983 0.30760923 0.25449643 0.23860908 0.24392302 0.24713093\n",
            " 0.23576547 0.36760232 0.23174386 0.31202355 0.35331497 0.38919365\n",
            " 0.28394642 0.3634454  0.3955301  0.2957924  0.20517941 0.301151\n",
            " 0.26248136 0.27742818 0.31890935 0.26781186 0.2605305  0.32571235\n",
            " 0.30538237 0.24851258 0.30596706 0.35831392 0.11175701 0.3286778\n",
            " 0.17257452 0.3357928  0.30100235 0.17594393 0.2783067  0.17165482\n",
            " 0.27326977 0.34126535 0.27835885 0.2719891  0.28605917 0.20277023\n",
            " 0.3616831  0.31726506 0.58742243 0.27343437 0.30354738 0.16523248\n",
            " 0.34375966 0.3038285  0.33178613 0.16623943], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.2685776  0.32618877 0.3249875  0.11446599 0.32999837 0.26023212\n",
            " 0.25093696 0.3172836  0.30831257 0.2761821  0.28273883 0.06830766\n",
            " 0.04342751 0.05556902 0.2747275  0.33743775 0.33672443 0.17521377\n",
            " 0.2670023  0.07991254 0.15319704 0.28685462 0.08886545 0.3115206\n",
            " 0.19382901 0.24978943 0.41486037 0.38398412 0.33366635 0.28283566\n",
            " 0.12353293 0.09854249 0.2767789  0.37031913 0.31082124 0.31187943\n",
            " 0.23953122 0.12802781 0.2530872  0.27224538 0.30205122 0.29981792\n",
            " 0.14017612 0.34522924 0.28378192 0.30021596 0.25377855 0.32234967\n",
            " 0.2770259  0.33713555 0.16024187 0.18009402 0.25516674 0.30105367\n",
            " 0.37853846 0.33283257 0.2987595  0.3469651  0.19645976 0.28955454\n",
            " 0.2423399  0.2859172  0.32615098 0.36354363 0.23641753 0.37241706\n",
            " 0.28166893 0.23622374 0.18219166 0.26048806 0.2303143  0.12238446\n",
            " 0.30174184 0.34594932 0.3704017  0.26562744 0.32192582 0.30739045\n",
            " 0.3156833  0.28402606 0.29879618 0.37288105 0.33251876 0.03968087\n",
            " 0.2967293  0.20007099 0.27721566 0.34025702 0.32315505 0.27150515\n",
            " 0.33754846 0.29916337 0.37292466 0.2824492  0.3163289  0.37773585\n",
            " 0.21061029 0.33140412 0.26379335 0.32768714], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.37390468 0.31336483 0.35865283 0.15390667 0.15047626 0.32166103\n",
            " 0.11441863 0.11874525 0.1854911  0.23841558 0.23349933 0.25924763\n",
            " 0.2908295  0.05824827 0.34697375 0.32483768 0.1981129  0.25992933\n",
            " 0.38642702 0.3925395  0.21787809 0.25616163 0.26646075 0.23119497\n",
            " 0.30468526 0.26187792 0.27819052 0.23235424 0.1999281  0.3284382\n",
            " 0.27859142 0.30242196 0.36099425 0.3116388  0.22106875 0.2908775\n",
            " 0.24910994 0.28648806 0.35629153 0.33678266 0.22700195 0.21730177\n",
            " 0.421618   0.24822426 0.24700426 0.21967687 0.27492177 0.37852868\n",
            " 0.26453397 0.18562062 0.33828554 0.21870004 0.26627597 0.31894478\n",
            " 0.43073288 0.30633903 0.28878033 0.32853517 0.14598542 0.24843852\n",
            " 0.36494732 0.22304732 0.23963802 0.07504886 0.36421737 0.28550252\n",
            " 0.26832113 0.22344844 0.26643097 0.3354504  0.3599856  0.33864686\n",
            " 0.33466616 0.26069722 0.23412871 0.27737212 0.3708721  0.288843\n",
            " 0.19428425 0.31091502 0.33192822 0.33527362 0.20475511 0.27175245\n",
            " 0.2728189  0.08373078 0.31999567 0.2769784  0.31392163 0.23855948\n",
            " 0.20863748 0.3116913  0.21204315 0.278562   0.2939252  0.1983006\n",
            " 0.26043007 0.29192355 0.23482615 0.29902646], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.27094567 0.15436926 0.16022637 0.31999752 0.17976744 0.37035668\n",
            " 0.27730158 0.33465973 0.22218452 0.29455188 0.23947094 0.25586233\n",
            " 0.21962075 0.26901007 0.38087955 0.34409752 0.27641404 0.27945796\n",
            " 0.3351821  0.32053775 0.26969877 0.31059426 0.31564    0.2219212\n",
            " 0.23584051 0.22507174 0.33121088 0.32017484 0.23841895 0.17949893\n",
            " 0.3489319  0.33720478 0.32176918 0.27839616 0.21491675 0.3330561\n",
            " 0.26996458 0.34585965 0.19277088 0.03252349 0.2594953  0.21170282\n",
            " 0.1861543  0.27810404 0.3305573  0.32914698 0.16892327 0.19528027\n",
            " 0.33174092 0.25788212 0.29457894 0.2816837  0.13545378 0.07181466\n",
            " 0.35160485 0.28022805 0.2869369  0.3138806  0.21260834 0.2918799\n",
            " 0.32496732 0.30713448 0.1939857  0.3272969  0.32036415 0.26196578\n",
            " 0.19564493 0.21834874 0.23408647 0.362456   0.11899447 0.34548226\n",
            " 0.17552121 0.34976754 0.18307775 0.27578217 0.26323292 0.12768847\n",
            " 0.33212885 0.21305571 0.19902967 0.23540562 0.3046259  0.35915157\n",
            " 0.30954447 0.35996374 0.13310944 0.31566313 0.11360919 0.03226601\n",
            " 0.31019264 0.22161289 0.24480325 0.28581688 0.27794358 0.26333413\n",
            " 0.3297069  0.3146987  0.09825637 0.23499888], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.30756798 0.3574636  0.29644406 0.34493235 0.32763603 0.21513219\n",
            " 0.0400536  0.20271952 0.13489424 0.3086586  0.32708308 0.2576398\n",
            " 0.34673557 0.2542626  0.28304183 0.21380186 0.3002254  0.23027025\n",
            " 0.29147097 0.02086367 0.2338685  0.35403347 0.33090547 0.12452856\n",
            " 0.31720915 0.15195623 0.19751544 0.07774805 0.18570499 0.32333153\n",
            " 0.3367362  0.3055738  0.21771717 0.15567166 0.299543   0.2737138\n",
            " 0.2387623  0.31054798 0.26748526 0.2660165  0.13420133 0.22306202\n",
            " 0.23318417 0.24193859 0.28115514 0.37076035 0.28189477 0.31177798\n",
            " 0.2695255  0.33924738 0.3643789  0.35040125 0.28899166 0.23704238\n",
            " 0.3221477  0.22502618 0.3212222  0.39078522 0.27434763 0.33850893\n",
            " 0.24721418 0.20122021 0.31354025 0.34900984 0.29653397 0.17063761\n",
            " 0.2427185  0.33019254 0.30944893 0.36936966 0.25921348 0.17701213\n",
            " 0.3110958  0.31878203 0.2553019  0.08773562 0.28890747 0.15277489\n",
            " 0.02581631 0.36837184 0.15023033 0.32627687 0.27123722 0.246103\n",
            " 0.34209046 0.0354803  0.34712747 0.28424358 0.29712757 0.25556377\n",
            " 0.25718507 0.31996307 0.2476425  0.2832339  0.30341396 0.3118625\n",
            " 0.31040633 0.28764936 0.31381372 0.15637387], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.28136435 0.3386174  0.28419036 0.30592853 0.26639268 0.24431555\n",
            " 0.3393575  0.18869448 0.33914903 0.2872932  0.17257577 0.27113184\n",
            " 0.18765378 0.18037444 0.26633322 0.3059413  0.29717132 0.16093957\n",
            " 0.3721896  0.3245947  0.24404502 0.332702   0.33221343 0.12189436\n",
            " 0.15401661 0.22319333 0.2965741  0.26730174 0.2918048  0.26008633\n",
            " 0.23175013 0.1858974  0.3026388  0.14500506 0.33067858 0.31353155\n",
            " 0.28764343 0.34157193 0.29565182 0.14002974 0.3087976  0.25637087\n",
            " 0.3261511  0.2305532  0.21112289 0.32834873 0.17606018 0.22056925\n",
            " 0.2295125  0.09144437 0.31403795 0.2561613  0.36402562 0.10566606\n",
            " 0.37515554 0.24493933 0.22308397 0.27453902 0.27158806 0.20223223\n",
            " 0.23078616 0.2414719  0.26379752 0.32462344 0.2805464  0.13866757\n",
            " 0.28692096 0.31292066 0.23948902 0.30586413 0.34261146 0.12848131\n",
            " 0.07041368 0.16874176 0.14793703 0.255622   0.31614497 0.19385417\n",
            " 0.32262358 0.30294356 0.29800153 0.21015619 0.27716228 0.23634446\n",
            " 0.31387672 0.30323794 0.28103667 0.2948245  0.07005589 0.2996424\n",
            " 0.02580774 0.29634377 0.16080965 0.24094678 0.3378485  0.23864268\n",
            " 0.15396264 0.19963259 0.15075308 0.11971133], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.27899376 0.20223342 0.32428262 0.2601827  0.25790596 0.23120002\n",
            " 0.15480058 0.14621772 0.03863489 0.24580853 0.2380581  0.2861302\n",
            " 0.31626424 0.17910765 0.2647022  0.34248415 0.23772015 0.16645043\n",
            " 0.24770086 0.3365165  0.33185384 0.19508064 0.25014162 0.11381338\n",
            " 0.3084469  0.21199794 0.22082861 0.16557495 0.2037425  0.25162706\n",
            " 0.3028513  0.25211605 0.3032678  0.24226522 0.12429819 0.1300106\n",
            " 0.2846058  0.02157947 0.24825712 0.01844059 0.09685019 0.3296539\n",
            " 0.1376819  0.2725282  0.23601294 0.31139198 0.04945783 0.23847745\n",
            " 0.08545903 0.09408927 0.0849008  0.27105227 0.30412763 0.34079003\n",
            " 0.26255187 0.3422179  0.12653257 0.15084668 0.26165605 0.28263092\n",
            " 0.29405278 0.23048122 0.27991188 0.2961829  0.11020321 0.2335202\n",
            " 0.32052872 0.29393232 0.2887891  0.27935544 0.03643813 0.20929553\n",
            " 0.3169907  0.3026336  0.18294974 0.33263794 0.25310674 0.08349339\n",
            " 0.28511965 0.37188387 0.15040636 0.1839004  0.22705276 0.29303232\n",
            " 0.19488513 0.31153622 0.33917832 0.24614753 0.2535246  0.1095294\n",
            " 0.12380942 0.2810613  0.19882245 0.22756557 0.32782575 0.19405161\n",
            " 0.3020491  0.16831149 0.11635818 0.20504892], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.24758779 0.32501325 0.2836285  0.23598379 0.2502075  0.1661918\n",
            " 0.26159665 0.21122618 0.14337231 0.34237382 0.31724262 0.21215373\n",
            " 0.31889802 0.18234454 0.08868485 0.22601919 0.35567698 0.27572647\n",
            " 0.2780191  0.00684245 0.24291222 0.2922146  0.28680933 0.30841467\n",
            " 0.20395081 0.29950497 0.348897   0.2704399  0.25970224 0.22062282\n",
            " 0.17899017 0.2760227  0.30989942 0.2786066  0.2102565  0.31867778\n",
            " 0.1913464  0.24920051 0.24049468 0.26769724 0.10720685 0.23156841\n",
            " 0.3302739  0.32172477 0.21634205 0.3410221  0.28036174 0.2935719\n",
            " 0.12738663 0.19478817 0.24594058 0.35374808 0.3200037  0.24476199\n",
            " 0.29225507 0.28935894 0.24050312 0.17560528 0.2888569  0.31032732\n",
            " 0.18335603 0.14973544 0.28072783 0.27131656 0.22910742 0.22226174\n",
            " 0.22302909 0.24118169 0.35821223 0.34155855 0.13636191 0.27453515\n",
            " 0.34278178 0.23067589 0.13038723 0.25895146 0.18740363 0.29457012\n",
            " 0.27581957 0.20327671 0.33077157 0.06344172 0.31783208 0.1975767\n",
            " 0.3314882  0.21058083 0.30736074 0.3521359  0.27697173 0.2307204\n",
            " 0.15285861 0.1351726  0.28948668 0.1494669  0.23520927 0.03789795\n",
            " 0.20448284 0.2177201  0.17183459 0.15633984], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.18041258 0.20900422 0.32073024 0.32724932 0.0278318  0.17862046\n",
            " 0.1706816  0.14145379 0.10026846 0.11940191 0.15938121 0.23851527\n",
            " 0.12853554 0.17460032 0.2950208  0.1646107  0.00533793 0.31643918\n",
            " 0.28081283 0.24241681 0.15693456 0.11520684 0.3142553  0.2060792\n",
            " 0.24041252 0.32594934 0.31317738 0.29918507 0.19183974 0.32283905\n",
            " 0.17659122 0.25170553 0.21909797 0.3320009  0.09381638 0.15966533\n",
            " 0.23108697 0.15000087 0.31861576 0.23287435 0.14111947 0.19602883\n",
            " 0.33418146 0.2512408  0.07719048 0.3732493  0.06216107 0.25909677\n",
            " 0.19615084 0.2952465  0.18556535 0.21958792 0.27443913 0.18809916\n",
            " 0.16690685 0.3391355  0.23980017 0.25559577 0.2018326  0.31721193\n",
            " 0.27673545 0.136243   0.28603724 0.31663343 0.12552182 0.18345343\n",
            " 0.2644833  0.02223705 0.28176025 0.17116104 0.1390916  0.18188679\n",
            " 0.26368317 0.16560407 0.26820415 0.14816895 0.17852394 0.21747947\n",
            " 0.21764386 0.3134917  0.31112382 0.14726205 0.22024828 0.3314125\n",
            " 0.33486918 0.3116422  0.33401987 0.22133379 0.18731217 0.18084814\n",
            " 0.32237262 0.2298444  0.20550056 0.2712831  0.29859674 0.33482158\n",
            " 0.12692825 0.12350497 0.33569315 0.11470162], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 20: 0.2211\n",
            "seen so far: 2100 samples\n",
            "tf.Tensor(\n",
            "[0.23144247 0.33879742 0.14677544 0.32218623 0.24731171 0.31881595\n",
            " 0.0322589  0.19507545 0.21366513 0.15076883 0.3021364  0.30647263\n",
            " 0.29544795 0.2658094  0.33354452 0.14610256 0.2908325  0.2815483\n",
            " 0.19021988 0.26818383 0.30070862 0.16753705 0.06329893 0.2712773\n",
            " 0.26427916 0.17794007 0.3052695  0.2076959  0.30157995 0.28655976\n",
            " 0.19358534 0.25618988 0.15995759 0.33109716 0.07970617 0.15572765\n",
            " 0.14743558 0.30454794 0.14479117 0.08843189 0.1596966  0.33878684\n",
            " 0.25615165 0.32414114 0.07296926 0.0207927  0.2918156  0.22373682\n",
            " 0.12730268 0.19687898 0.284346   0.28914568 0.18910599 0.12800036\n",
            " 0.24882846 0.2810168  0.21063952 0.307502   0.28494924 0.06836697\n",
            " 0.18239415 0.2843445  0.12444752 0.3216487  0.10240545 0.18738376\n",
            " 0.14800587 0.09900433 0.29729846 0.14789574 0.33585715 0.23302056\n",
            " 0.30963352 0.18515496 0.13043202 0.25644466 0.23138146 0.2714771\n",
            " 0.19289248 0.33132383 0.25383505 0.33627796 0.23028065 0.29238337\n",
            " 0.27415597 0.16782725 0.32218271 0.21131353 0.22528736 0.21138924\n",
            " 0.12736486 0.2977529  0.31002215 0.32946342 0.21727347 0.25066265\n",
            " 0.32362744 0.12874682 0.11915678 0.10172594], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.17323786 0.27629113 0.33320794 0.24291764 0.21127702 0.29972777\n",
            " 0.20413923 0.32141444 0.21231286 0.33384085 0.19803673 0.25013736\n",
            " 0.2786018  0.1973233  0.24335773 0.27714372 0.08895933 0.23679233\n",
            " 0.06152049 0.18640642 0.07742587 0.2693504  0.11678014 0.19947481\n",
            " 0.18959485 0.13829201 0.3358426  0.19450538 0.2632726  0.2824761\n",
            " 0.31687337 0.33575296 0.32476422 0.02742511 0.24321629 0.04998103\n",
            " 0.2420458  0.31923416 0.18567984 0.19833611 0.11476003 0.271932\n",
            " 0.24851859 0.30086723 0.31604055 0.1381449  0.19450374 0.26985928\n",
            " 0.21654384 0.09237063], shape=(50,), dtype=float32)\n",
            "validation MSE: 0.1899\n",
            "\n",
            "Start of epoch 1\n",
            "tf.Tensor(\n",
            "[0.3313953  0.20341645 0.3226659  0.14281751 0.3229989  0.16427433\n",
            " 0.3203505  0.24651998 0.16810526 0.10632781 0.2851185  0.31277844\n",
            " 0.30349603 0.1165634  0.11577926 0.19339967 0.3139977  0.10008118\n",
            " 0.29715848 0.27350724 0.1893477  0.05150051 0.30579758 0.2936509\n",
            " 0.25094804 0.25883448 0.3223711  0.19261725 0.25718763 0.18776089\n",
            " 0.3322799  0.2791425  0.17866875 0.2010556  0.18286656 0.14677624\n",
            " 0.10460207 0.12581028 0.29301003 0.13361266 0.29696926 0.21089351\n",
            " 0.30747142 0.30742544 0.32462665 0.13398473 0.13989346 0.275835\n",
            " 0.2574328  0.22609873 0.3057214  0.11069418 0.15286423 0.06490477\n",
            " 0.29733908 0.2003249  0.1292771  0.18319221 0.313012   0.27421117\n",
            " 0.15805368 0.19120967 0.08853001 0.12422273 0.32090035 0.01442142\n",
            " 0.14258109 0.2909715  0.08163998 0.16121013 0.17739117 0.29999918\n",
            " 0.3172763  0.14779983 0.28882733 0.2511148  0.1273496  0.07871453\n",
            " 0.22668286 0.31057915 0.04748614 0.23662925 0.08097426 0.32504904\n",
            " 0.24361496 0.19680683 0.31677738 0.15547365 0.18375619 0.15258591\n",
            " 0.18398635 0.15242405 0.17642485 0.28775528 0.31949314 0.21518667\n",
            " 0.28095192 0.20263462 0.16036656 0.13218848], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 0: 0.2129\n",
            "seen so far: 100 samples\n",
            "tf.Tensor(\n",
            "[0.2788829  0.18989624 0.08001024 0.24994554 0.1456657  0.24430859\n",
            " 0.26567966 0.17554156 0.22771217 0.1765126  0.20354761 0.17382276\n",
            " 0.09584603 0.25039425 0.16765118 0.29152402 0.2910491  0.06667724\n",
            " 0.12692258 0.27668932 0.22527172 0.14284652 0.11548334 0.22627951\n",
            " 0.15631033 0.24844886 0.08287787 0.11829377 0.24832135 0.18327796\n",
            " 0.27194777 0.17663203 0.23498331 0.17696299 0.36211988 0.13013415\n",
            " 0.14949797 0.08495104 0.21319087 0.31390125 0.14923966 0.23806201\n",
            " 0.29283604 0.21185867 0.09874019 0.31063575 0.16976845 0.21519653\n",
            " 0.20350908 0.3273119  0.11049669 0.09145246 0.25009882 0.09810459\n",
            " 0.15220726 0.13704379 0.1808422  0.09688931 0.30187726 0.23310421\n",
            " 0.2729698  0.13793981 0.05695805 0.18748158 0.18755777 0.22968173\n",
            " 0.06264815 0.28234825 0.3207487  0.18893169 0.19847864 0.28431755\n",
            " 0.28189173 0.14300491 0.27265653 0.04578687 0.1878783  0.31495425\n",
            " 0.17504936 0.2753934  0.14368324 0.25582644 0.32679674 0.32541588\n",
            " 0.202821   0.28046933 0.31993666 0.3287439  0.30716532 0.1223318\n",
            " 0.25688788 0.30592614 0.13291478 0.11572102 0.13937859 0.21122883\n",
            " 0.25655255 0.23618282 0.2643884  0.03441466], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.23382054 0.26412222 0.32478055 0.23527783 0.32518917 0.08591021\n",
            " 0.2334603  0.20749974 0.27625367 0.14379261 0.31760183 0.2045756\n",
            " 0.14743038 0.20971425 0.29545942 0.32084396 0.1748801  0.32109764\n",
            " 0.2918567  0.13205783 0.16136716 0.32502046 0.09301946 0.29278505\n",
            " 0.29694256 0.16582131 0.2928339  0.18230157 0.31830665 0.14851873\n",
            " 0.22533709 0.13488574 0.04154313 0.29428372 0.32676595 0.2925236\n",
            " 0.16527884 0.20415883 0.13996412 0.29891357 0.14486535 0.30839032\n",
            " 0.16795178 0.21105047 0.26752406 0.03055946 0.13671844 0.2156711\n",
            " 0.1813258  0.1673826  0.01699818 0.205157   0.07924041 0.13365348\n",
            " 0.29267958 0.07275685 0.16064835 0.10606257 0.02907775 0.23047888\n",
            " 0.09762049 0.10859185 0.31588057 0.20023918 0.20444727 0.25114766\n",
            " 0.24884613 0.26256973 0.2765286  0.2189005  0.2663566  0.29563913\n",
            " 0.20012458 0.29995874 0.16958727 0.23139746 0.2808356  0.13037686\n",
            " 0.29490134 0.18992637 0.0653742  0.3186722  0.30450615 0.16552232\n",
            " 0.3317945  0.23990737 0.12118348 0.27211222 0.30080286 0.24401033\n",
            " 0.18000843 0.10252295 0.02995016 0.21684875 0.27179527 0.07948302\n",
            " 0.18983652 0.2812647  0.16406165 0.23621118], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.25777656 0.19258189 0.09350592 0.11865472 0.261947   0.20781791\n",
            " 0.27571425 0.20527518 0.1745107  0.20631534 0.24930714 0.21303542\n",
            " 0.17808573 0.14637716 0.17292917 0.2874255  0.27077937 0.15680599\n",
            " 0.11172587 0.15173207 0.23702526 0.08934382 0.22575945 0.30873397\n",
            " 0.24325036 0.27373126 0.24403651 0.31195006 0.2590003  0.2090038\n",
            " 0.15201755 0.06677429 0.2630174  0.31558633 0.25079864 0.205728\n",
            " 0.04398829 0.13120395 0.20227586 0.18218638 0.08524146 0.09889463\n",
            " 0.23225562 0.29554668 0.28870788 0.19173147 0.19257934 0.05801718\n",
            " 0.19525816 0.23059683 0.19084918 0.12125436 0.27401957 0.23179471\n",
            " 0.1918056  0.32269374 0.23010898 0.18714209 0.14264916 0.30317497\n",
            " 0.09798816 0.11020934 0.06020801 0.00742118 0.1541168  0.23481934\n",
            " 0.29863587 0.11263785 0.16941871 0.16328286 0.02094063 0.29210186\n",
            " 0.19802266 0.21189862 0.3227519  0.02040861 0.32385972 0.30699098\n",
            " 0.31165555 0.08509827 0.28351203 0.21660955 0.1512655  0.11785677\n",
            " 0.12913649 0.19115113 0.19201805 0.15819317 0.02393563 0.13753453\n",
            " 0.23200661 0.01195907 0.1295662  0.20934938 0.3204493  0.26718244\n",
            " 0.2880462  0.04771848 0.0637312  0.30553135], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.24176419 0.15395159 0.10107813 0.07915679 0.09455231 0.11395892\n",
            " 0.16648242 0.10060275 0.30248645 0.08732174 0.22751564 0.30628654\n",
            " 0.23453806 0.22988985 0.28184754 0.10094791 0.14974551 0.14308715\n",
            " 0.1783074  0.07103221 0.19056354 0.20231158 0.01538976 0.19472957\n",
            " 0.31934887 0.249689   0.2974411  0.13030054 0.31867713 0.29453087\n",
            " 0.11243647 0.12194402 0.14504087 0.08199631 0.0814686  0.15949\n",
            " 0.21134293 0.04170766 0.12214424 0.24793816 0.08691579 0.22583467\n",
            " 0.16444866 0.29131904 0.24941425 0.2733371  0.32039875 0.117227\n",
            " 0.1595685  0.18873541 0.2629689  0.03375624 0.1546823  0.3105217\n",
            " 0.14540392 0.08097187 0.22709428 0.32405052 0.27096468 0.12953253\n",
            " 0.20297861 0.12717758 0.30639163 0.2587811  0.1134525  0.28572783\n",
            " 0.27879605 0.29970804 0.12646033 0.20292325 0.21022885 0.14355953\n",
            " 0.13957426 0.16904218 0.03287143 0.00553238 0.20508485 0.19054823\n",
            " 0.2775592  0.2923611  0.11979278 0.23253751 0.180833   0.08237056\n",
            " 0.21006697 0.20261078 0.20936157 0.11655737 0.13615455 0.2126496\n",
            " 0.21180642 0.24107705 0.18940915 0.32013616 0.16109699 0.30747858\n",
            " 0.2135507  0.1585475  0.06976875 0.22095335], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.10808279 0.08467861 0.2053147  0.11481135 0.22706966 0.20310257\n",
            " 0.11944052 0.03757229 0.08356666 0.15132287 0.2997266  0.29284048\n",
            " 0.14838919 0.27945238 0.22159712 0.11525929 0.16864012 0.2462186\n",
            " 0.15815657 0.11081598 0.11307871 0.21907233 0.03029384 0.22141439\n",
            " 0.2573119  0.1600431  0.25691766 0.05875902 0.24072935 0.19963603\n",
            " 0.29424694 0.27158132 0.16607143 0.06981918 0.05880145 0.04413529\n",
            " 0.06896394 0.13578407 0.30996686 0.20073898 0.2883004  0.12956637\n",
            " 0.21887983 0.2908533  0.12426078 0.10628065 0.13160563 0.06734829\n",
            " 0.01318046 0.03401805 0.15118767 0.07238529 0.2366674  0.20448701\n",
            " 0.30831027 0.30520198 0.19507422 0.10997212 0.19478963 0.18636943\n",
            " 0.19456922 0.14095251 0.1815954  0.20805667 0.22543706 0.22713257\n",
            " 0.15183382 0.1296193  0.11469332 0.10940394 0.14916225 0.2575174\n",
            " 0.30824015 0.30462593 0.26408297 0.08357042 0.1373743  0.2801158\n",
            " 0.1146253  0.12655218 0.18928385 0.26794913 0.11849455 0.11150906\n",
            " 0.21467906 0.30613717 0.28330156 0.0821962  0.21624947 0.17595439\n",
            " 0.24400727 0.11960107 0.28836164 0.18988484 0.1401368  0.13560538\n",
            " 0.31017885 0.30795178 0.25429606 0.00885126], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.2833903  0.11906537 0.2398556  0.14452495 0.17510335 0.18075477\n",
            " 0.19298561 0.20353629 0.2805951  0.132019   0.17417221 0.24838935\n",
            " 0.2013905  0.26349542 0.1773631  0.18467422 0.17094822 0.12424898\n",
            " 0.01109754 0.19640915 0.21006052 0.16049905 0.14358427 0.04109391\n",
            " 0.17343377 0.24327224 0.30194676 0.15062751 0.1649606  0.11678944\n",
            " 0.23685528 0.1050255  0.26029783 0.05214125 0.18497442 0.1879149\n",
            " 0.2322791  0.28940287 0.2926679  0.19355221 0.18896472 0.16719596\n",
            " 0.05301886 0.12729648 0.17172974 0.22291349 0.08500279 0.13882177\n",
            " 0.12552805 0.07168573 0.21041466 0.2719321  0.09890514 0.21525224\n",
            " 0.24403656 0.14617603 0.20517741 0.2958845  0.2290433  0.23676427\n",
            " 0.11846944 0.2038079  0.21812694 0.12416595 0.23659201 0.13891186\n",
            " 0.06895126 0.23316033 0.16281836 0.17482954 0.27763215 0.15559936\n",
            " 0.14354345 0.15733492 0.13375428 0.13288797 0.25254092 0.27618992\n",
            " 0.13028185 0.20436256 0.17637777 0.24150987 0.06592782 0.15410018\n",
            " 0.09624738 0.15081997 0.25764778 0.15248781 0.12649801 0.2879231\n",
            " 0.09676769 0.17019409 0.30579525 0.10583802 0.18411016 0.11356912\n",
            " 0.19271298 0.24970885 0.19538014 0.06411775], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.26547208 0.25851935 0.13506167 0.28881618 0.22837436 0.21720259\n",
            " 0.1499598  0.2796192  0.10640413 0.04001543 0.18007217 0.09304645\n",
            " 0.07575177 0.17589466 0.25908792 0.25931662 0.23953317 0.17585254\n",
            " 0.09371779 0.12616439 0.04872142 0.09263306 0.25340536 0.13818626\n",
            " 0.04957402 0.07953423 0.2777522  0.11270535 0.12422174 0.1937598\n",
            " 0.28973052 0.1611611  0.28942356 0.16480383 0.20420945 0.0373817\n",
            " 0.2965888  0.1936114  0.18834965 0.26814255 0.15678303 0.23658569\n",
            " 0.09126753 0.11351353 0.12925953 0.18979527 0.12017009 0.14403449\n",
            " 0.2780716  0.24152978 0.28654456 0.02285955 0.2042392  0.23600845\n",
            " 0.29230773 0.22677077 0.14803135 0.23934585 0.15065464 0.25578108\n",
            " 0.25756708 0.2467653  0.12382834 0.11936883 0.10377958 0.31840512\n",
            " 0.15660739 0.23510091 0.07326602 0.28124094 0.1041405  0.04626689\n",
            " 0.18408263 0.16281055 0.14645107 0.16774781 0.11970035 0.08210955\n",
            " 0.23552942 0.14966242 0.01502047 0.14640321 0.24196923 0.10576215\n",
            " 0.298892   0.26043501 0.25929382 0.06747166 0.22890238 0.10294145\n",
            " 0.18546982 0.24992937 0.19361895 0.23819076 0.29234952 0.12302601\n",
            " 0.1645227  0.11837403 0.22902232 0.2294359 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.04420858 0.2913695  0.14243488 0.10387576 0.26131594 0.314191\n",
            " 0.2652658  0.07875418 0.18675669 0.3006613  0.25002474 0.17988646\n",
            " 0.09310385 0.18119334 0.26555178 0.15077564 0.04892422 0.00729764\n",
            " 0.19305022 0.25781387 0.26333833 0.06021602 0.1800173  0.03138311\n",
            " 0.26477212 0.1173544  0.17283277 0.25630453 0.06228356 0.14919697\n",
            " 0.14281885 0.21224815 0.12378845 0.04838824 0.28475595 0.18991984\n",
            " 0.09389645 0.15339664 0.10880164 0.2449042  0.14837027 0.2786493\n",
            " 0.22353064 0.262185   0.14799474 0.15369727 0.20864773 0.11829624\n",
            " 0.21376853 0.24789931 0.25118265 0.11004138 0.24682038 0.28272137\n",
            " 0.05610858 0.1933698  0.04628487 0.1356666  0.06712297 0.18603027\n",
            " 0.24072511 0.09516314 0.21536304 0.0722265  0.3050194  0.06537366\n",
            " 0.17803095 0.27976832 0.06828941 0.27257782 0.09888818 0.15581934\n",
            " 0.13601297 0.20481974 0.03315905 0.2355145  0.29122302 0.20720385\n",
            " 0.29383722 0.10426769 0.17110647 0.2062149  0.1403868  0.10765412\n",
            " 0.28968787 0.24691625 0.04686219 0.15165326 0.04483102 0.15210508\n",
            " 0.08622509 0.08338066 0.19111367 0.14656202 0.23445316 0.19676991\n",
            " 0.08365368 0.12696035 0.2580609  0.17812298], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.26045635 0.15327276 0.01539098 0.24641962 0.14452685 0.08915076\n",
            " 0.21056795 0.10874116 0.02844579 0.07614483 0.17296845 0.26639244\n",
            " 0.167053   0.1448146  0.04047858 0.12621875 0.12192056 0.00764188\n",
            " 0.23762338 0.16098972 0.11319095 0.2658752  0.06300145 0.18733938\n",
            " 0.11539847 0.31742537 0.08012674 0.0612006  0.19920518 0.12805209\n",
            " 0.21387197 0.02587563 0.18482178 0.07984403 0.09492779 0.17268544\n",
            " 0.10543258 0.20099725 0.08931512 0.13447575 0.19565822 0.21330464\n",
            " 0.04824265 0.23795444 0.01470725 0.13043575 0.18129982 0.03108983\n",
            " 0.186702   0.02483695 0.08217073 0.283977   0.08240966 0.22491719\n",
            " 0.2654357  0.23342936 0.14894374 0.12183896 0.26281825 0.14253758\n",
            " 0.10974758 0.19071166 0.30548236 0.18685369 0.26784474 0.19926049\n",
            " 0.2730545  0.06560602 0.04665999 0.269327   0.17437778 0.01130496\n",
            " 0.20657624 0.09722283 0.0134156  0.24983655 0.24610992 0.13088052\n",
            " 0.10871349 0.29155365 0.14447168 0.24937582 0.28970936 0.27718925\n",
            " 0.26912168 0.26967505 0.04324751 0.19673796 0.09158254 0.3010825\n",
            " 0.12632622 0.23402952 0.28950095 0.20034854 0.14344494 0.1761989\n",
            " 0.21682204 0.14642213 0.18691675 0.30819124], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.12410807 0.05208427 0.14556025 0.02093737 0.15389432 0.06661571\n",
            " 0.2191974  0.13224936 0.12924305 0.2248437  0.10368226 0.1862521\n",
            " 0.19353844 0.09809218 0.11331906 0.22856086 0.26246822 0.15623422\n",
            " 0.15572742 0.40999314 0.10704377 0.28590137 0.22200452 0.02851581\n",
            " 0.13682584 0.08043086 0.11726256 0.14564215 0.1481979  0.18396969\n",
            " 0.24032551 0.11464509 0.12884918 0.08970768 0.07565497 0.03269265\n",
            " 0.03310516 0.19944708 0.19615169 0.12890957 0.10860822 0.17435122\n",
            " 0.17555694 0.09759447 0.1988805  0.07385551 0.09136317 0.26648438\n",
            " 0.13009097 0.09667057 0.20717597 0.1909548  0.10547329 0.10892638\n",
            " 0.16671453 0.06518807 0.14331429 0.0453807  0.23731928 0.21972235\n",
            " 0.10784885 0.05934403 0.23395054 0.0274915  0.16704561 0.24307692\n",
            " 0.15063216 0.12838657 0.11345362 0.11854541 0.1809669  0.22293842\n",
            " 0.2183178  0.12654588 0.27702126 0.12819894 0.22163148 0.06921074\n",
            " 0.02209129 0.26827514 0.17434959 0.14413275 0.07447702 0.16184945\n",
            " 0.22672015 0.26769438 0.26409346 0.18128274 0.11790267 0.13749875\n",
            " 0.13966049 0.05268531 0.14000547 0.14706971 0.0634261  0.15979989\n",
            " 0.24166997 0.12334517 0.16448043 0.176912  ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 10: 0.1502\n",
            "seen so far: 1100 samples\n",
            "tf.Tensor(\n",
            "[0.10193688 0.13752733 0.2500178  0.28169325 0.14554004 0.06444327\n",
            " 0.2588317  0.27178928 0.2175519  0.1982824  0.20155233 0.14569065\n",
            " 0.08875519 0.25812498 0.1640477  0.13498554 0.07088331 0.09836316\n",
            " 0.04431274 0.08182571 0.07417408 0.05011586 0.12199392 0.2133333\n",
            " 0.10940671 0.26860142 0.16549669 0.04342842 0.05742529 0.2786446\n",
            " 0.04299926 0.06906904 0.20951839 0.18241505 0.09901267 0.10552877\n",
            " 0.16451265 0.15763082 0.2785724  0.15796222 0.24215871 0.20496964\n",
            " 0.22040819 0.07960764 0.19122578 0.07591227 0.27940506 0.27791888\n",
            " 0.25443888 0.2719756  0.0534835  0.0878765  0.2697234  0.23311399\n",
            " 0.12705287 0.13120268 0.08490017 0.03506174 0.28790775 0.07206952\n",
            " 0.18373568 0.19630434 0.09021029 0.10242017 0.11096592 0.1066375\n",
            " 0.2668903  0.25284484 0.27387086 0.11182011 0.12734668 0.21228136\n",
            " 0.2495151  0.03634226 0.09453442 0.13172714 0.06560087 0.23762982\n",
            " 0.09684993 0.19114292 0.1607026  0.178644   0.17518075 0.10341177\n",
            " 0.1406771  0.1675518  0.1354963  0.02949275 0.06581574 0.2557539\n",
            " 0.14471903 0.04705715 0.23193    0.04438749 0.11430982 0.09106702\n",
            " 0.02158467 0.11011291 0.08255161 0.1229178 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.12263087 0.15819131 0.1264762  0.09428064 0.07756124 0.19156283\n",
            " 0.13367005 0.17567162 0.03906538 0.24548267 0.26808676 0.23629409\n",
            " 0.2409416  0.11413249 0.23837732 0.25103733 0.0260797  0.12522735\n",
            " 0.12374201 0.09006721 0.13402233 0.18104877 0.21211743 0.12133647\n",
            " 0.20227836 0.25554606 0.05235805 0.1827149  0.14490058 0.15398651\n",
            " 0.04680275 0.16713195 0.1088216  0.14094388 0.1370283  0.12428699\n",
            " 0.18245052 0.00908803 0.06787956 0.21529907 0.18287206 0.15942048\n",
            " 0.25577423 0.26078388 0.059889   0.17601466 0.11589896 0.12492438\n",
            " 0.2502815  0.14105503 0.14180237 0.154258   0.14574862 0.28507894\n",
            " 0.19712843 0.07841186 0.19229482 0.16607413 0.0396497  0.14113778\n",
            " 0.19145511 0.22975107 0.03675797 0.21337833 0.20000857 0.15991305\n",
            " 0.06769232 0.27057555 0.26182392 0.27706614 0.28999332 0.04534684\n",
            " 0.14892338 0.05490479 0.23189496 0.16727132 0.1376016  0.18323223\n",
            " 0.18887703 0.12736556 0.18728836 0.27066648 0.11166295 0.04423225\n",
            " 0.08982661 0.15218602 0.2016175  0.10141981 0.28028932 0.16392462\n",
            " 0.14540023 0.23720731 0.14117992 0.18091662 0.13084812 0.04090146\n",
            " 0.17971604 0.10081128 0.26437053 0.17237611], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.06986244 0.23210768 0.18001388 0.19334896 0.126628   0.23975998\n",
            " 0.13670371 0.14042611 0.07071336 0.04115032 0.1631348  0.20977867\n",
            " 0.06010218 0.22357212 0.18242244 0.05253332 0.18022488 0.11675598\n",
            " 0.21399991 0.15151818 0.16998808 0.19637902 0.09278346 0.26389173\n",
            " 0.1505049  0.10149459 0.20015478 0.25496837 0.0491778  0.09608825\n",
            " 0.23154055 0.188172   0.15880188 0.08394134 0.2046792  0.15302357\n",
            " 0.04659495 0.26556015 0.13417885 0.20441245 0.10472637 0.1352204\n",
            " 0.20322831 0.24807774 0.16828746 0.04870791 0.0185981  0.14403225\n",
            " 0.04929725 0.07278287 0.07565777 0.02798989 0.19812371 0.07296753\n",
            " 0.18536176 0.21457894 0.1191267  0.17529859 0.25673997 0.16849948\n",
            " 0.1371172  0.27826512 0.2675583  0.07410121 0.02571028 0.11136758\n",
            " 0.12989601 0.21017647 0.19681962 0.249651   0.07693257 0.06836507\n",
            " 0.17418014 0.09148435 0.01370937 0.01897451 0.0498349  0.23093069\n",
            " 0.09037719 0.05983033 0.28224525 0.11740986 0.04083401 0.18680634\n",
            " 0.11765312 0.09425464 0.23832424 0.21609025 0.02707865 0.18577003\n",
            " 0.07483929 0.10717642 0.0491177  0.15150388 0.06471597 0.04749532\n",
            " 0.20899479 0.11630967 0.1366879  0.18725409], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.14599636 0.08103971 0.26799756 0.2276497  0.04262462 0.11804583\n",
            " 0.02888056 0.13848065 0.04698271 0.15056613 0.26895878 0.04587873\n",
            " 0.2997924  0.11209124 0.05165921 0.06893717 0.11306106 0.03014869\n",
            " 0.05236301 0.10867856 0.15645073 0.02942633 0.0743917  0.01610467\n",
            " 0.12438434 0.18696062 0.04683352 0.10407972 0.202326   0.06344382\n",
            " 0.09190285 0.19906111 0.11527038 0.19701044 0.23356795 0.09825391\n",
            " 0.1680303  0.27650663 0.0344458  0.17522132 0.26250854 0.17589027\n",
            " 0.16215241 0.21618931 0.0974142  0.25844064 0.16188276 0.04619065\n",
            " 0.16034125 0.09876215 0.13447057 0.2612776  0.01589709 0.26902738\n",
            " 0.18074481 0.08353034 0.01797817 0.26982465 0.16192982 0.01989704\n",
            " 0.08258785 0.08886277 0.18926334 0.03932021 0.0596639  0.04787505\n",
            " 0.00636313 0.10386601 0.11756579 0.24934272 0.22384666 0.14424053\n",
            " 0.12406728 0.22357613 0.12370976 0.13734771 0.10878127 0.20985727\n",
            " 0.23643614 0.02385699 0.04613677 0.26817873 0.17109054 0.01865274\n",
            " 0.20815702 0.11499358 0.12031154 0.19165455 0.15601212 0.10294095\n",
            " 0.0364855  0.18639451 0.16854304 0.17412029 0.07945522 0.13921909\n",
            " 0.07026184 0.13500255 0.05190493 0.02482861], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.03230233 0.13582101 0.23432994 0.214323   0.13828184 0.03219241\n",
            " 0.11903588 0.13984899 0.04803591 0.03683135 0.13447566 0.08972775\n",
            " 0.06795735 0.06137952 0.12787057 0.10990623 0.0880748  0.19722676\n",
            " 0.05936221 0.17186701 0.13111295 0.2553284  0.15652774 0.1410688\n",
            " 0.01094082 0.12109118 0.22992945 0.24134414 0.15825112 0.17948334\n",
            " 0.18626992 0.1807533  0.1166429  0.01571116 0.14126806 0.1259136\n",
            " 0.14270356 0.09236327 0.01396399 0.15508057 0.1863967  0.15549983\n",
            " 0.09377076 0.14336006 0.23344682 0.09375653 0.07655151 0.16983171\n",
            " 0.05719886 0.0884397  0.05397779 0.0821311  0.10213501 0.06176896\n",
            " 0.10614029 0.4837118  0.13552074 0.15007813 0.06612125 0.0339708\n",
            " 0.03226788 0.02608937 0.23435225 0.24821734 0.08202129 0.04341219\n",
            " 0.12997998 0.1017684  0.13438535 0.09526998 0.2007748  0.1161802\n",
            " 0.16648541 0.06123284 0.06581632 0.2403623  0.04661013 0.04771423\n",
            " 0.02446193 0.09628228 0.1391688  0.06553481 0.10471871 0.03660424\n",
            " 0.24597983 0.11083361 0.16252166 0.1613143  0.15925848 0.22442119\n",
            " 0.07705371 0.0257499  0.13834496 0.03808688 0.19229583 0.14863099\n",
            " 0.05350865 0.1896811  0.09681927 0.09066186], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.1089628  0.12029624 0.09550003 0.03546701 0.12914188 0.23354214\n",
            " 0.09027305 0.03558332 0.11505304 0.13772045 0.08395231 0.04363019\n",
            " 0.10736159 0.20193924 0.08458712 0.00474567 0.03624218 0.06210193\n",
            " 0.06229116 0.0821721  0.10994717 0.21075709 0.1827231  0.12989728\n",
            " 0.08827438 0.10070169 0.04941064 0.05525376 0.24662031 0.14372267\n",
            " 0.07077007 0.08453688 0.17643453 0.1032193  0.22139233 0.03525971\n",
            " 0.2509924  0.25975767 0.02023165 0.09695514 0.06417167 0.09534431\n",
            " 0.05589123 0.17676438 0.05777389 0.23948689 0.0489942  0.12700213\n",
            " 0.14980148 0.14833917 0.2215565  0.06181975 0.08602902 0.26733664\n",
            " 0.0786768  0.1345316  0.21386041 0.10125869 0.1937655  0.02480895\n",
            " 0.19636166 0.23697084 0.1367063  0.14819469 0.08908603 0.17380762\n",
            " 0.27463612 0.02665603 0.04647295 0.10024218 0.05336409 0.14478317\n",
            " 0.23780452 0.05183309 0.23702244 0.05360791 0.11742207 0.1864336\n",
            " 0.1350312  0.03204067 0.11572615 0.2421878  0.26396316 0.00556698\n",
            " 0.2489848  0.04313284 0.20000316 0.0304858  0.23044379 0.1252584\n",
            " 0.0889244  0.21549125 0.25728378 0.05229089 0.12324497 0.08981815\n",
            " 0.20134597 0.12226004 0.2434839  0.24949302], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.08765975 0.07620061 0.0850262  0.08803643 0.03634645 0.21275832\n",
            " 0.13168792 0.15710686 0.17185111 0.25552177 0.14860122 0.14577533\n",
            " 0.08830243 0.11350176 0.0846206  0.20962979 0.19109541 0.0949759\n",
            " 0.10298551 0.03210601 0.20576082 0.09124479 0.24953611 0.19642448\n",
            " 0.06949715 0.12854964 0.25072572 0.22026189 0.18769945 0.03058992\n",
            " 0.0105003  0.1607428  0.16771066 0.19098286 0.0972493  0.03958925\n",
            " 0.16104703 0.24113615 0.08085681 0.11583847 0.15974174 0.11335101\n",
            " 0.18634753 0.0821901  0.08041751 0.08076917 0.05944761 0.01243898\n",
            " 0.18669151 0.2081626  0.08014136 0.23507889 0.1134127  0.13990013\n",
            " 0.16954796 0.12894721 0.21425998 0.13258845 0.04145507 0.10346341\n",
            " 0.06784064 0.13021307 0.15667869 0.23450442 0.0837618  0.18533099\n",
            " 0.30704907 0.24359901 0.11633131 0.19103312 0.12308455 0.21243359\n",
            " 0.02748768 0.274961   0.13229789 0.2149735  0.10465785 0.1769465\n",
            " 0.08994403 0.2051583  0.20659393 0.23817639 0.11277003 0.25350797\n",
            " 0.1570156  0.03796132 0.11474511 0.17093416 0.13200572 0.03645314\n",
            " 0.00560581 0.16973215 0.12821428 0.1413454  0.19589722 0.14735785\n",
            " 0.02512195 0.01323919 0.15695782 0.23645194], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.25104275 0.19303952 0.15892662 0.05431307 0.21518023 0.2378562\n",
            " 0.02484047 0.03101399 0.19080327 0.09078149 0.11827464 0.06860423\n",
            " 0.15148465 0.03988506 0.22251375 0.11216287 0.02604563 0.19452225\n",
            " 0.07973634 0.05512876 0.26471016 0.20277834 0.22506906 0.19885474\n",
            " 0.09622663 0.02919078 0.18249728 0.03701686 0.06194508 0.18097298\n",
            " 0.12837678 0.22135134 0.0584483  0.15398891 0.16784243 0.21120195\n",
            " 0.04122596 0.16592123 0.22724676 0.1726305  0.11991512 0.0759292\n",
            " 0.1086954  0.04508567 0.2566929  0.11100164 0.13159479 0.11762936\n",
            " 0.227314   0.13547663 0.0269574  0.1348285  0.14515309 0.05874744\n",
            " 0.16917707 0.1121305  0.29057094 0.01199482 0.02757549 0.07870553\n",
            " 0.20997912 0.21738426 0.2200462  0.00383417 0.14005907 0.00969123\n",
            " 0.16850938 0.16606711 0.07364786 0.005911   0.04178892 0.22584605\n",
            " 0.05460248 0.17399687 0.01332506 0.22538424 0.12510349 0.00379624\n",
            " 0.1039606  0.09493876 0.07712826 0.17001094 0.04877347 0.10863902\n",
            " 0.1741995  0.09807107 0.10779553 0.21505828 0.23803526 0.16791515\n",
            " 0.03592096 0.37429938 0.2324347  0.01578609 0.10103711 0.07613974\n",
            " 0.05881281 0.10145602 0.06787358 0.24767487], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.16242649 0.14310594 0.06262971 0.17963035 0.07618713 0.08503956\n",
            " 0.10534763 0.14081173 0.05061361 0.20590788 0.06242211 0.02197341\n",
            " 0.09426633 0.10850238 0.10694828 0.14053734 0.0960874  0.14112663\n",
            " 0.13008296 0.07727929 0.09386349 0.24041271 0.16243169 0.05513683\n",
            " 0.10867194 0.21682602 0.06603389 0.1502327  0.183421   0.05712813\n",
            " 0.07292796 0.10682539 0.17836972 0.03459315 0.11094484 0.1606629\n",
            " 0.0553393  0.04359951 0.09826308 0.23461665 0.19388159 0.07321572\n",
            " 0.17489295 0.08638299 0.09045648 0.16176282 0.03060067 0.24886452\n",
            " 0.05774263 0.12813668 0.08358058 0.12932786 0.07991301 0.09156394\n",
            " 0.1076531  0.16446856 0.08025948 0.04840925 0.16489454 0.15552181\n",
            " 0.23669182 0.22773717 0.22040516 0.20436883 0.02185234 0.12282624\n",
            " 0.01294256 0.09010989 0.16598912 0.17126818 0.03631352 0.09607851\n",
            " 0.22138418 0.09811398 0.10092161 0.09340908 0.16064896 0.19700097\n",
            " 0.11649726 0.13520212 0.15290359 0.20168702 0.21968943 0.1231159\n",
            " 0.02280951 0.07893016 0.20134048 0.07046714 0.24024057 0.08492061\n",
            " 0.05502505 0.09475944 0.07624229 0.20436282 0.19771107 0.12506737\n",
            " 0.07195619 0.05132234 0.00375306 0.027816  ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.21771018 0.14314835 0.04634861 0.04627129 0.07912228 0.22461195\n",
            " 0.09828281 0.00308021 0.16635126 0.06714094 0.06682156 0.15999375\n",
            " 0.08183388 0.04618227 0.12675567 0.03652835 0.1633607  0.10201939\n",
            " 0.02231883 0.18224259 0.02559385 0.22321205 0.22147562 0.187021\n",
            " 0.05740846 0.06138228 0.19725384 0.08089484 0.00812993 0.046482\n",
            " 0.02034249 0.01661388 0.06051704 0.19018506 0.17116334 0.21126741\n",
            " 0.08192962 0.10087845 0.04733881 0.09866009 0.05116795 0.09655934\n",
            " 0.19340704 0.14460991 0.22714306 0.10314375 0.1269039  0.05388863\n",
            " 0.14320935 0.15072174 0.04262312 0.14031765 0.19350672 0.21074337\n",
            " 0.14661898 0.17207204 0.1813305  0.06953308 0.17790438 0.17172958\n",
            " 0.22907765 0.20491469 0.20006217 0.04529919 0.24425775 0.2020415\n",
            " 0.18327643 0.1390913  0.18584096 0.19703168 0.10616666 0.18669222\n",
            " 0.06570604 0.19259696 0.17856686 0.13488774 0.15459935 0.04619363\n",
            " 0.11061566 0.05557472 0.03836817 0.13630904 0.17487483 0.06385063\n",
            " 0.13162291 0.10231179 0.06106044 0.08753437 0.32482204 0.19629367\n",
            " 0.1387827  0.10636421 0.07050654 0.00557718 0.10413629 0.17075133\n",
            " 0.07087282 0.13516815 0.03489714 0.0241413 ], shape=(100,), dtype=float32)\n",
            "Training loss (for one batch) at step 20: 0.1223\n",
            "seen so far: 2100 samples\n",
            "tf.Tensor(\n",
            "[0.05305724 0.15940745 0.08436912 0.07338754 0.05973347 0.08356518\n",
            " 0.08188417 0.10101346 0.02643691 0.11073181 0.05024974 0.14759515\n",
            " 0.0111099  0.13022201 0.00847946 0.16625465 0.06271806 0.19934648\n",
            " 0.02151326 0.1205103  0.06979799 0.08462852 0.05721307 0.12604146\n",
            " 0.11738924 0.23108931 0.11571088 0.13370347 0.07822075 0.09298623\n",
            " 0.13256902 0.04293966 0.22237831 0.15616684 0.13913226 0.05066796\n",
            " 0.1218005  0.1330053  0.08331897 0.24001952 0.11761171 0.03175687\n",
            " 0.21034777 0.23278745 0.07224399 0.24660404 0.10997534 0.07638108\n",
            " 0.22180903 0.20852877 0.09371275 0.09026625 0.06483634 0.12840885\n",
            " 0.13412587 0.20158333 0.09366571 0.08679885 0.22851507 0.13860208\n",
            " 0.11167402 0.1480806  0.04894427 0.16485138 0.09294283 0.08400401\n",
            " 0.10057796 0.22996242 0.10146173 0.09136364 0.00856537 0.22634842\n",
            " 0.06113872 0.09697485 0.00605936 0.05211107 0.10838688 0.03118072\n",
            " 0.14994599 0.22117989 0.21196313 0.17997533 0.12963386 0.0546221\n",
            " 0.01349061 0.01227262 0.14918138 0.04906755 0.00191588 0.0895802\n",
            " 0.03278978 0.07688961 0.07082786 0.05265345 0.01851128 0.20758905\n",
            " 0.2065655  0.06055497 0.08199655 0.1656986 ], shape=(100,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[0.11764292 0.10331783 0.08423856 0.08583949 0.1050837  0.02376318\n",
            " 0.01376242 0.05455611 0.02640394 0.23626041 0.0675372  0.13477957\n",
            " 0.06608228 0.17150836 0.20930547 0.11049739 0.14595348 0.17330019\n",
            " 0.20812123 0.02106757 0.07712548 0.20196168 0.01564169 0.11641435\n",
            " 0.03333508 0.14913861 0.22561258 0.29245448 0.23102708 0.22531027\n",
            " 0.23592341 0.02941599 0.15962237 0.22754438 0.09065429 0.06748082\n",
            " 0.00730838 0.05509123 0.10081244 0.2353303  0.04436046 0.12448078\n",
            " 0.23104258 0.03962379 0.18872851 0.09647026 0.20408778 0.06952699\n",
            " 0.09108593 0.1712933 ], shape=(50,), dtype=float32)\n",
            "validation MSE: 0.1149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYc1EE-yrmVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a684cd89-8fe8-4d5e-c687-becdc9a5ecd0"
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "f1, f2, f3 = prediction.T\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.plot3D(f1, f2, f3, 'o')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7fdaa4e729b0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZAks17s1Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use Lecture's"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}